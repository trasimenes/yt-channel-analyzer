#!/usr/bin/env python3
"""
Graph Data Extractor - Outil pour extraire des donn√©es de graphiques
Permet de simuler une souris qui suit une courbe ou d'extraire des donn√©es par OCR
"""

import time
import cv2
import numpy as np
import pyautogui
import pytesseract
from PIL import Image, ImageDraw
import json
import os
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import threading
import queue
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc

class GraphDataExtractor:
    """
    Extracteur de donn√©es de graphiques avec plusieurs m√©thodes :
    1. Balayage horizontal avec captures p√©riodiques
    2. Suivi automatique de courbe par d√©tection de couleur
    3. Extraction OCR des valeurs
    4. Consolidation des donn√©es
    """
    
    def __init__(self, output_dir: str = "graph_extractions"):
        self.output_dir = output_dir
        self.screenshots = []
        self.data_points = []
        self.is_scanning = False
        self.scan_thread = None
        
        # Configuration par d√©faut
        self.screenshot_interval = 0.25  # 1/4 de seconde
        self.curve_color = (255, 0, 0)  # Rouge par d√©faut
        self.tolerance = 30  # Tol√©rance de couleur
        
        # Cr√©er le dossier de sortie
        os.makedirs(self.output_dir, exist_ok=True)
        
        # D√©sactiver la protection contre les mouvements de souris
        pyautogui.FAILSAFE = False
    
    def horizontal_scan_with_screenshots(self, 
                                       start_x: int, 
                                       start_y: int, 
                                       end_x: int, 
                                       end_y: int,
                                       duration: float = 10.0,
                                       extract_ocr: bool = True) -> List[Dict]:
        """
        Balaye horizontalement de gauche √† droite en prenant des captures d'√©cran p√©riodiques.
        
        Args:
            start_x, start_y: Position de d√©part
            end_x, end_y: Position de fin
            duration: Dur√©e totale du balayage en secondes
            extract_ocr: Si True, extrait les valeurs par OCR
            
        Returns:
            Liste des points de donn√©es extraits
        """
        print(f"üéØ D√©but du balayage horizontal de ({start_x}, {start_y}) √† ({end_x}, {end_y})")
        print(f"üì∏ Capture toutes les {self.screenshot_interval}s pendant {duration}s")
        
        # Calculer le nombre de captures
        num_captures = int(duration / self.screenshot_interval)
        step_x = (end_x - start_x) / num_captures
        step_y = (end_y - start_y) / num_captures
        
        # R√©initialiser les donn√©es
        self.screenshots = []
        self.data_points = []
        
        # D√©marrer le balayage
        start_time = time.time()
        
        for i in range(num_captures):
            # Calculer la position actuelle
            current_x = start_x + (i * step_x)
            current_y = start_y + (i * step_y)
            
            # D√©placer la souris
            pyautogui.moveTo(current_x, current_y, duration=0.1)
            
            # Prendre une capture d'√©cran
            screenshot = pyautogui.screenshot()
            timestamp = time.time() - start_time
            
            # Sauvegarder la capture
            filename = f"scan_{i:04d}_{timestamp:.2f}s.png"
            filepath = os.path.join(self.output_dir, filename)
            screenshot.save(filepath)
            
            # Stocker les infos
            capture_data = {
                'index': i,
                'timestamp': timestamp,
                'x': current_x,
                'y': current_y,
                'filename': filename,
                'filepath': filepath
            }
            
            # Extraction OCR si demand√©e
            if extract_ocr:
                ocr_data = self._extract_ocr_from_screenshot(screenshot, current_x, current_y)
                capture_data.update(ocr_data)
            
            self.screenshots.append(capture_data)
            
            # Attendre jusqu'√† la prochaine capture
            time.sleep(self.screenshot_interval)
            
            print(f"üì∏ Capture {i+1}/{num_captures} - Position: ({current_x:.0f}, {current_y:.0f})")
        
        print(f"‚úÖ Balayage termin√© ! {len(self.screenshots)} captures sauvegard√©es")
        
        # Consolidation finale
        self._consolidate_data()
        
        return self.data_points
    
    def detect_and_follow_curve(self, 
                              region: Tuple[int, int, int, int],
                              curve_color: Tuple[int, int, int] = (255, 0, 0),
                              tolerance: int = 30) -> List[Dict]:
        """
        D√©tecte automatiquement une courbe et la suit de gauche √† droite.
        
        Args:
            region: (x, y, width, height) r√©gion √† analyser
            curve_color: Couleur de la courbe √† suivre (RGB)
            tolerance: Tol√©rance pour la d√©tection de couleur
            
        Returns:
            Liste des points de donn√©es de la courbe
        """
        print(f"üéØ D√©tection de courbe couleur {curve_color} dans la r√©gion {region}")
        
        # Prendre une capture de la r√©gion
        screenshot = pyautogui.screenshot(region=region)
        img_array = np.array(screenshot)
        
        # Conversion BGR pour OpenCV
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        # Cr√©er un masque pour la couleur de la courbe
        lower_color = np.array([max(0, c - tolerance) for c in curve_color[::-1]])  # BGR
        upper_color = np.array([min(255, c + tolerance) for c in curve_color[::-1]])  # BGR
        
        mask = cv2.inRange(img_bgr, lower_color, upper_color)
        
        # Trouver les contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            print("‚ùå Aucune courbe d√©tect√©e avec cette couleur")
            return []
        
        # Prendre le plus grand contour (probablement la courbe principale)
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Extraire les points et les trier par x
        points = []
        for point in largest_contour:
            x, y = point[0]
            # Convertir en coordonn√©es √©cran
            screen_x = region[0] + x
            screen_y = region[1] + y
            points.append((screen_x, screen_y))
        
        # Trier par x (de gauche √† droite)
        points.sort(key=lambda p: p[0])
        
        print(f"üìä {len(points)} points d√©tect√©s sur la courbe")
        
        # Extraire les donn√©es √† intervalles r√©guliers
        self.data_points = []
        interval = max(1, len(points) // 50)  # Maximum 50 points
        
        for i in range(0, len(points), interval):
            x, y = points[i]
            
            # Prendre une capture centr√©e sur ce point
            capture_region = (x - 100, y - 100, 200, 200)
            screenshot = pyautogui.screenshot(region=capture_region)
            
            # Extraction OCR
            ocr_data = self._extract_ocr_from_screenshot(screenshot, x, y)
            
            data_point = {
                'x': x,
                'y': y,
                'timestamp': time.time(),
                **ocr_data
            }
            
            self.data_points.append(data_point)
            
            print(f"üìç Point {len(self.data_points)}: ({x}, {y}) - Valeur: {ocr_data.get('value', 'N/A')}")
        
        return self.data_points
    
    def _extract_ocr_from_screenshot(self, screenshot: Image.Image, x: int, y: int) -> Dict:
        """
        Extrait les valeurs num√©riques d'une capture d'√©cran par OCR.
        
        Args:
            screenshot: Image PIL
            x, y: Position du curseur
            
        Returns:
            Dictionnaire avec les valeurs extraites
        """
        try:
            # Prendre une r√©gion autour du curseur pour l'OCR
            region_size = 150
            left = max(0, x - region_size)
            top = max(0, y - region_size)
            right = min(screenshot.width, x + region_size)
            bottom = min(screenshot.height, y + region_size)
            
            # Recadrer la r√©gion d'int√©r√™t
            ocr_region = screenshot.crop((left, top, right, bottom))
            
            # Am√©liorer l'image pour l'OCR
            ocr_region = ocr_region.convert('L')  # Niveaux de gris
            ocr_region = ocr_region.point(lambda x: 0 if x < 128 else 255, '1')  # Binarisation
            
            # Extraction OCR
            text = pytesseract.image_to_string(ocr_region, config='--psm 8 -c tessedit_char_whitelist=0123456789.,KkMm%')
            
            # Chercher des valeurs num√©riques
            values = self._extract_numbers_from_text(text)
            
            return {
                'ocr_text': text.strip(),
                'value': values[0] if values else None,
                'all_values': values,
                'ocr_region': (left, top, right, bottom)
            }
            
        except Exception as e:
            print(f"‚ùå Erreur OCR: {e}")
            return {
                'ocr_text': '',
                'value': None,
                'all_values': [],
                'error': str(e)
            }
    
    def _extract_numbers_from_text(self, text: str) -> List[float]:
        """
        Extrait les nombres d'un texte OCR.
        
        Args:
            text: Texte brut de l'OCR
            
        Returns:
            Liste des nombres trouv√©s
        """
        values = []
        
        # Patterns pour diff√©rents formats de nombres
        patterns = [
            r'(\d+\.?\d*[KkMm]?)',  # Nombres avec K, M
            r'(\d+[,\.]\d+)',        # D√©cimaux
            r'(\d+)',                # Entiers
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    # Convertir les suffixes K, M
                    if match.lower().endswith('k'):
                        value = float(match[:-1]) * 1000
                    elif match.lower().endswith('m'):
                        value = float(match[:-1]) * 1000000
                    else:
                        value = float(match.replace(',', '.'))
                    
                    values.append(value)
                except ValueError:
                    continue
        
        return values
    
    def _consolidate_data(self):
        """
        Consolide toutes les donn√©es extraites dans un format structur√©.
        """
        print("üìä Consolidation des donn√©es...")
        
        # Trier par timestamp
        self.screenshots.sort(key=lambda x: x['timestamp'])
        
        # Extraire les valeurs uniques
        values = []
        timestamps = []
        
        for capture in self.screenshots:
            if capture.get('value') is not None:
                values.append(capture['value'])
                timestamps.append(capture['timestamp'])
        
        # Cr√©er les points de donn√©es consolid√©s
        self.data_points = []
        for i, (value, timestamp) in enumerate(zip(values, timestamps)):
            self.data_points.append({
                'index': i,
                'timestamp': timestamp,
                'value': value,
                'source': 'horizontal_scan'
            })
        
        print(f"‚úÖ {len(self.data_points)} points de donn√©es consolid√©s")
        
        # Sauvegarder en JSON
        output_file = os.path.join(self.output_dir, f"consolidated_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'total_screenshots': len(self.screenshots),
                    'total_data_points': len(self.data_points),
                    'extraction_date': datetime.now().isoformat(),
                    'screenshot_interval': self.screenshot_interval
                },
                'screenshots': self.screenshots,
                'data_points': self.data_points
            }, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Donn√©es sauvegard√©es dans {output_file}")
    
    def interactive_setup(self):
        """
        Interface interactive pour configurer l'extraction.
        """
        print("üéØ Configuration interactive de l'extraction de graphique")
        print("=" * 50)
        
        # Demander le type d'extraction
        print("\nTypes d'extraction disponibles :")
        print("1. Balayage horizontal avec captures p√©riodiques")
        print("2. D√©tection automatique de courbe")
        print("3. Extraction depuis une vid√©o")
        
        choice = input("\nChoisissez une option (1-3): ").strip()
        
        if choice == "1":
            self._setup_horizontal_scan()
        elif choice == "2":
            self._setup_curve_detection()
        elif choice == "3":
            self._setup_video_extraction()
        else:
            print("‚ùå Option invalide")
    
    def _setup_horizontal_scan(self):
        """
        Configuration du balayage horizontal.
        """
        print("\nüéØ Configuration du balayage horizontal")
        print("Cliquez sur le point de d√©part, puis sur le point d'arriv√©e")
        
        input("Appuyez sur Entr√©e quand vous √™tes pr√™t √† cliquer le point de d√©part...")
        start_pos = self._wait_for_click()
        print(f"Point de d√©part: {start_pos}")
        
        input("Appuyez sur Entr√©e quand vous √™tes pr√™t √† cliquer le point d'arriv√©e...")
        end_pos = self._wait_for_click()
        print(f"Point d'arriv√©e: {end_pos}")
        
        # Demander la dur√©e
        duration = float(input("Dur√©e du balayage en secondes (d√©faut: 10): ") or "10")
        
        # Lancer l'extraction
        print(f"\nüöÄ Lancement du balayage dans 3 secondes...")
        time.sleep(3)
        
        self.horizontal_scan_with_screenshots(
            start_pos[0], start_pos[1],
            end_pos[0], end_pos[1],
            duration=duration
        )
    
    def _setup_curve_detection(self):
        """
        Configuration de la d√©tection de courbe.
        """
        print("\nüéØ Configuration de la d√©tection de courbe")
        print("Cliquez sur deux coins oppos√©s de la r√©gion √† analyser")
        
        input("Appuyez sur Entr√©e pour cliquer le coin haut-gauche...")
        top_left = self._wait_for_click()
        
        input("Appuyez sur Entr√©e pour cliquer le coin bas-droite...")
        bottom_right = self._wait_for_click()
        
        # Calculer la r√©gion
        region = (
            top_left[0], top_left[1],
            bottom_right[0] - top_left[0],
            bottom_right[1] - top_left[1]
        )
        
        print(f"R√©gion s√©lectionn√©e: {region}")
        
        # Demander la couleur de la courbe
        print("\nCouleur de la courbe :")
        print("1. Rouge (d√©faut)")
        print("2. Bleu")
        print("3. Vert")
        print("4. Personnalis√©e")
        
        color_choice = input("Choisissez (1-4): ").strip() or "1"
        
        color_map = {
            "1": (255, 0, 0),
            "2": (0, 0, 255),
            "3": (0, 255, 0),
        }
        
        if color_choice in color_map:
            curve_color = color_map[color_choice]
        else:
            # Couleur personnalis√©e
            r = int(input("Rouge (0-255): ") or "255")
            g = int(input("Vert (0-255): ") or "0")
            b = int(input("Bleu (0-255): ") or "0")
            curve_color = (r, g, b)
        
        print(f"\nüöÄ D√©tection de la courbe...")
        self.detect_and_follow_curve(region, curve_color)
    
    def _wait_for_click(self) -> Tuple[int, int]:
        """
        Attend que l'utilisateur clique et retourne la position.
        """
        print("Cliquez sur la position souhait√©e...")
        
        # Attendre un clic
        while True:
            try:
                # V√©rifier si la souris est cliqu√©e
                if pyautogui.click is not None:
                    time.sleep(0.1)
                    pos = pyautogui.position()
                    # Attendre que l'utilisateur clique r√©ellement
                    import tkinter as tk
                    root = tk.Tk()
                    root.withdraw()
                    root.update()
                    
                    # M√©thode simple: attendre un changement de position
                    initial_pos = pyautogui.position()
                    while pyautogui.position() == initial_pos:
                        time.sleep(0.1)
                    
                    return pyautogui.position()
            except:
                time.sleep(0.1)
    
    def extract_from_video(self, video_path: str, frame_interval: int = 30) -> List[Dict]:
        """
        Extrait des donn√©es d'une vid√©o de graphique.
        
        Args:
            video_path: Chemin vers la vid√©o
            frame_interval: Intervalle entre les frames analys√©es
            
        Returns:
            Liste des donn√©es extraites
        """
        print(f"üé¨ Extraction depuis la vid√©o: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        data_points = []
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                # Convertir en PIL pour OCR
                frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                
                # Extraction OCR sur toute la frame
                ocr_data = self._extract_ocr_from_screenshot(frame_pil, frame.shape[1]//2, frame.shape[0]//2)
                
                if ocr_data['value'] is not None:
                    data_points.append({
                        'frame': frame_count,
                        'timestamp': frame_count / cap.get(cv2.CAP_PROP_FPS),
                        'value': ocr_data['value'],
                        'source': 'video'
                    })
                    
                    print(f"üìç Frame {frame_count}: {ocr_data['value']}")
            
            frame_count += 1
        
        cap.release()
        
        print(f"‚úÖ Extraction termin√©e: {len(data_points)} points extraits")
        return data_points

# Fonction utilitaire pour utilisation simple
def quick_horizontal_scan(duration: float = 10.0) -> List[Dict]:
    """
    Fonction rapide pour un balayage horizontal simple.
    
    Args:
        duration: Dur√©e du balayage en secondes
        
    Returns:
        Liste des points de donn√©es extraits
    """
    extractor = GraphDataExtractor()
    
    print("üéØ Balayage horizontal rapide")
    print("1. Positionnez votre souris au point de d√©part")
    input("Appuyez sur Entr√©e quand vous √™tes pr√™t...")
    start_pos = pyautogui.position()
    
    print("2. Positionnez votre souris au point d'arriv√©e")
    input("Appuyez sur Entr√©e quand vous √™tes pr√™t...")
    end_pos = pyautogui.position()
    
    print("üöÄ D√©marrage dans 3 secondes...")
    time.sleep(3)
    
    return extractor.horizontal_scan_with_screenshots(
        start_pos[0], start_pos[1],
        end_pos[0], end_pos[1],
        duration=duration
    )

if __name__ == "__main__":
    # Exemple d'utilisation
    extractor = GraphDataExtractor()
    extractor.interactive_setup() 