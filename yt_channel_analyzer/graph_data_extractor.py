#!/usr/bin/env python3
"""
Graph Data Extractor - Outil pour extraire des donnÃ©es de graphiques
Permet de simuler une souris qui suit une courbe ou d'extraire des donnÃ©es par OCR
"""

import time
import cv2
import numpy as np
import pyautogui
import pytesseract
from PIL import Image, ImageDraw
import json
import os
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import threading
import queue
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc

class GraphDataExtractor:
    """
    Extracteur de donnÃ©es de graphiques avec plusieurs mÃ©thodes :
    1. Balayage horizontal avec captures pÃ©riodiques
    2. Suivi automatique de courbe par dÃ©tection de couleur
    3. Extraction OCR des valeurs
    4. Consolidation des donnÃ©es
    """
    
    def __init__(self, output_dir: str = "graph_extractions"):
        self.output_dir = output_dir
        self.screenshots = []
        self.data_points = []
        self.is_scanning = False
        self.scan_thread = None
        
        # Configuration par dÃ©faut
        self.screenshot_interval = 0.25  # 1/4 de seconde
        self.curve_color = (255, 0, 0)  # Rouge par dÃ©faut
        self.tolerance = 30  # TolÃ©rance de couleur
        
        # CrÃ©er le dossier de sortie
        os.makedirs(self.output_dir, exist_ok=True)
        
        # DÃ©sactiver la protection contre les mouvements de souris
        pyautogui.FAILSAFE = False
    
    def horizontal_scan_with_screenshots(self, 
                                       start_x: int, 
                                       start_y: int, 
                                       end_x: int, 
                                       end_y: int,
                                       duration: float = 10.0,
                                       extract_ocr: bool = True) -> List[Dict]:
        """
        Balaye horizontalement de gauche Ã  droite en prenant des captures d'Ã©cran pÃ©riodiques.
        
        Args:
            start_x, start_y: Position de dÃ©part
            end_x, end_y: Position de fin
            duration: DurÃ©e totale du balayage en secondes
            extract_ocr: Si True, extrait les valeurs par OCR
            
        Returns:
            Liste des points de donnÃ©es extraits
        """
        print(f"ðŸŽ¯ DÃ©but du balayage horizontal de ({start_x}, {start_y}) Ã  ({end_x}, {end_y})")
        print(f"ðŸ“¸ Capture toutes les {self.screenshot_interval}s pendant {duration}s")
        
        # Calculer le nombre de captures
        num_captures = int(duration / self.screenshot_interval)
        step_x = (end_x - start_x) / num_captures
        step_y = (end_y - start_y) / num_captures
        
        # RÃ©initialiser les donnÃ©es
        self.screenshots = []
        self.data_points = []
        
        # DÃ©marrer le balayage
        start_time = time.time()
        
        for i in range(num_captures):
            # Calculer la position actuelle
            current_x = start_x + (i * step_x)
            current_y = start_y + (i * step_y)
            
            # DÃ©placer la souris
            pyautogui.moveTo(current_x, current_y, duration=0.1)
            
            # Prendre une capture d'Ã©cran
            screenshot = pyautogui.screenshot()
            timestamp = time.time() - start_time
            
            # Sauvegarder la capture
            filename = f"scan_{i:04d}_{timestamp:.2f}s.png"
            filepath = os.path.join(self.output_dir, filename)
            screenshot.save(filepath)
            
            # Stocker les infos
            capture_data = {
                'index': i,
                'timestamp': timestamp,
                'x': current_x,
                'y': current_y,
                'filename': filename,
                'filepath': filepath
            }
            
            # Extraction OCR si demandÃ©e
            if extract_ocr:
                ocr_data = self._extract_ocr_from_screenshot(screenshot, current_x, current_y)
                capture_data.update(ocr_data)
            
            self.screenshots.append(capture_data)
            
            # Attendre jusqu'Ã  la prochaine capture
            time.sleep(self.screenshot_interval)
            
            print(f"ðŸ“¸ Capture {i+1}/{num_captures} - Position: ({current_x:.0f}, {current_y:.0f})")
        
        print(f"âœ… Balayage terminÃ© ! {len(self.screenshots)} captures sauvegardÃ©es")
        
        # Consolidation finale
        self._consolidate_data()
        
        return self.data_points
    
    def detect_and_follow_curve(self, 
                              region: Tuple[int, int, int, int],
                              curve_color: Tuple[int, int, int] = (255, 0, 0),
                              tolerance: int = 30) -> List[Dict]:
        """
        DÃ©tecte automatiquement une courbe et la suit de gauche Ã  droite.
        
        Args:
            region: (x, y, width, height) rÃ©gion Ã  analyser
            curve_color: Couleur de la courbe Ã  suivre (RGB)
            tolerance: TolÃ©rance pour la dÃ©tection de couleur
            
        Returns:
            Liste des points de donnÃ©es de la courbe
        """
        print(f"ðŸŽ¯ DÃ©tection de courbe couleur {curve_color} dans la rÃ©gion {region}")
        
        # Prendre une capture de la rÃ©gion
        screenshot = pyautogui.screenshot(region=region)
        img_array = np.array(screenshot)
        
        # Conversion BGR pour OpenCV
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        # CrÃ©er un masque pour la couleur de la courbe
        lower_color = np.array([max(0, c - tolerance) for c in curve_color[::-1]])  # BGR
        upper_color = np.array([min(255, c + tolerance) for c in curve_color[::-1]])  # BGR
        
        mask = cv2.inRange(img_bgr, lower_color, upper_color)
        
        # Trouver les contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            print("âŒ Aucune courbe dÃ©tectÃ©e avec cette couleur")
            return []
        
        # Prendre le plus grand contour (probablement la courbe principale)
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Extraire les points et les trier par x
        points = []
        for point in largest_contour:
            x, y = point[0]
            # Convertir en coordonnÃ©es Ã©cran
            screen_x = region[0] + x
            screen_y = region[1] + y
            points.append((screen_x, screen_y))
        
        # Trier par x (de gauche Ã  droite)
        points.sort(key=lambda p: p[0])
        
        print(f"ðŸ“Š {len(points)} points dÃ©tectÃ©s sur la courbe")
        
        # Extraire les donnÃ©es Ã  intervalles rÃ©guliers
        self.data_points = []
        interval = max(1, len(points) // 50)  # Maximum 50 points
        
        for i in range(0, len(points), interval):
            x, y = points[i]
            
            # Prendre une capture centrÃ©e sur ce point
            capture_region = (x - 100, y - 100, 200, 200)
            screenshot = pyautogui.screenshot(region=capture_region)
            
            # Extraction OCR
            ocr_data = self._extract_ocr_from_screenshot(screenshot, x, y)
            
            data_point = {
                'x': x,
                'y': y,
                'timestamp': time.time(),
                **ocr_data
            }
            
            self.data_points.append(data_point)
            
            print(f"ðŸ“ Point {len(self.data_points)}: ({x}, {y}) - Valeur: {ocr_data.get('value', 'N/A')}")
        
        return self.data_points
    
    def _extract_ocr_from_screenshot(self, screenshot: Image.Image, x: int, y: int) -> Dict:
        """
        Extrait les valeurs numÃ©riques d'une capture d'Ã©cran par OCR.
        
        Args:
            screenshot: Image PIL
            x, y: Position du curseur
            
        Returns:
            Dictionnaire avec les valeurs extraites
        """
        try:
            # Prendre une rÃ©gion autour du curseur pour l'OCR
            region_size = 150
            left = max(0, x - region_size)
            top = max(0, y - region_size)
            right = min(screenshot.width, x + region_size)
            bottom = min(screenshot.height, y + region_size)
            
            # Recadrer la rÃ©gion d'intÃ©rÃªt
            ocr_region = screenshot.crop((left, top, right, bottom))
            
            # AmÃ©liorer l'image pour l'OCR
            ocr_region = ocr_region.convert('L')  # Niveaux de gris
            ocr_region = ocr_region.point(lambda x: 0 if x < 128 else 255, '1')  # Binarisation
            
            # Extraction OCR
            text = pytesseract.image_to_string(ocr_region, config='--psm 8 -c tessedit_char_whitelist=0123456789.,KkMm%')
            
            # Chercher des valeurs numÃ©riques
            values = self._extract_numbers_from_text(text)
            
            return {
                'ocr_text': text.strip(),
                'value': values[0] if values else None,
                'all_values': values,
                'ocr_region': (left, top, right, bottom)
            }
            
        except Exception as e:
            print(f"âŒ Erreur OCR: {e}")
            return {
                'ocr_text': '',
                'value': None,
                'all_values': [],
                'error': str(e)
            }
    
    def _extract_numbers_from_text(self, text: str) -> List[float]:
        """
        Extrait les nombres d'un texte OCR.
        
        Args:
            text: Texte brut de l'OCR
            
        Returns:
            Liste des nombres trouvÃ©s
        """
        values = []
        
        # Patterns pour diffÃ©rents formats de nombres
        patterns = [
            r'(\d+\.?\d*[KkMm]?)',  # Nombres avec K, M
            r'(\d+[,\.]\d+)',        # DÃ©cimaux
            r'(\d+)',                # Entiers
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    # Convertir les suffixes K, M
                    if match.lower().endswith('k'):
                        value = float(match[:-1]) * 1000
                    elif match.lower().endswith('m'):
                        value = float(match[:-1]) * 1000000
                    else:
                        value = float(match.replace(',', '.'))
                    
                    values.append(value)
                except ValueError:
                    continue
        
        return values
    
    def _consolidate_data(self):
        """
        Consolide toutes les donnÃ©es extraites dans un format structurÃ©.
        """
        print("ðŸ“Š Consolidation des donnÃ©es...")
        
        # Trier par timestamp
        self.screenshots.sort(key=lambda x: x['timestamp'])
        
        # Extraire les valeurs uniques
        values = []
        timestamps = []
        
        for capture in self.screenshots:
            if capture.get('value') is not None:
                values.append(capture['value'])
                timestamps.append(capture['timestamp'])
        
        # CrÃ©er les points de donnÃ©es consolidÃ©s
        self.data_points = []
        for i, (value, timestamp) in enumerate(zip(values, timestamps)):
            self.data_points.append({
                'index': i,
                'timestamp': timestamp,
                'value': value,
                'source': 'horizontal_scan'
            })
        
        print(f"âœ… {len(self.data_points)} points de donnÃ©es consolidÃ©s")
        
        # Sauvegarder en JSON
        output_file = os.path.join(self.output_dir, f"consolidated_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'total_screenshots': len(self.screenshots),
                    'total_data_points': len(self.data_points),
                    'extraction_date': datetime.now().isoformat(),
                    'screenshot_interval': self.screenshot_interval
                },
                'screenshots': self.screenshots,
                'data_points': self.data_points
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ DonnÃ©es sauvegardÃ©es dans {output_file}")
    
    def interactive_setup(self):
        """
        Interface interactive pour configurer l'extraction.
        """
        print("ðŸŽ¯ Configuration interactive de l'extraction de graphique")
        print("=" * 50)
        
        # Demander le type d'extraction
        print("\nTypes d'extraction disponibles :")
        print("1. Balayage horizontal avec captures pÃ©riodiques")
        print("2. DÃ©tection automatique de courbe")
        print("3. Extraction depuis une vidÃ©o")
        
        choice = input("\nChoisissez une option (1-3): ").strip()
        
        if choice == "1":
            self._setup_horizontal_scan()
        elif choice == "2":
            self._setup_curve_detection()
        elif choice == "3":
            self._setup_video_extraction()
        else:
            print("âŒ Option invalide")
    
    def _setup_horizontal_scan(self):
        """
        Configuration du balayage horizontal.
        """
        print("\nðŸŽ¯ Configuration du balayage horizontal")
        print("Cliquez sur le point de dÃ©part, puis sur le point d'arrivÃ©e")
        
        input("Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt Ã  cliquer le point de dÃ©part...")
        start_pos = self._wait_for_click()
        print(f"Point de dÃ©part: {start_pos}")
        
        input("Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt Ã  cliquer le point d'arrivÃ©e...")
        end_pos = self._wait_for_click()
        print(f"Point d'arrivÃ©e: {end_pos}")
        
        # Demander la durÃ©e
        duration = float(input("DurÃ©e du balayage en secondes (dÃ©faut: 10): ") or "10")
        
        # Lancer l'extraction
        print(f"\nðŸš€ Lancement du balayage dans 3 secondes...")
        time.sleep(3)
        
        self.horizontal_scan_with_screenshots(
            start_pos[0], start_pos[1],
            end_pos[0], end_pos[1],
            duration=duration
        )
    
    def _setup_curve_detection(self):
        """
        Configuration de la dÃ©tection de courbe.
        """
        print("\nðŸŽ¯ Configuration de la dÃ©tection de courbe")
        print("Cliquez sur deux coins opposÃ©s de la rÃ©gion Ã  analyser")
        
        input("Appuyez sur EntrÃ©e pour cliquer le coin haut-gauche...")
        top_left = self._wait_for_click()
        
        input("Appuyez sur EntrÃ©e pour cliquer le coin bas-droite...")
        bottom_right = self._wait_for_click()
        
        # Calculer la rÃ©gion
        region = (
            top_left[0], top_left[1],
            bottom_right[0] - top_left[0],
            bottom_right[1] - top_left[1]
        )
        
        print(f"RÃ©gion sÃ©lectionnÃ©e: {region}")
        
        # Demander la couleur de la courbe
        print("\nCouleur de la courbe :")
        print("1. Rouge (dÃ©faut)")
        print("2. Bleu")
        print("3. Vert")
        print("4. PersonnalisÃ©e")
        
        color_choice = input("Choisissez (1-4): ").strip() or "1"
        
        color_map = {
            "1": (255, 0, 0),
            "2": (0, 0, 255),
            "3": (0, 255, 0),
        }
        
        if color_choice in color_map:
            curve_color = color_map[color_choice]
        else:
            # Couleur personnalisÃ©e
            r = int(input("Rouge (0-255): ") or "255")
            g = int(input("Vert (0-255): ") or "0")
            b = int(input("Bleu (0-255): ") or "0")
            curve_color = (r, g, b)
        
        print(f"\nðŸš€ DÃ©tection de la courbe...")
        self.detect_and_follow_curve(region, curve_color)
    
    def _wait_for_click(self) -> Tuple[int, int]:
        """
        Attend que l'utilisateur clique et retourne la position.
        """
        print("Cliquez sur la position souhaitÃ©e...")
        
        # Attendre un clic
        while True:
            try:
                # VÃ©rifier si la souris est cliquÃ©e
                if pyautogui.click is not None:
                    time.sleep(0.1)
                    pos = pyautogui.position()
                    # Attendre que l'utilisateur clique rÃ©ellement
                    import tkinter as tk
                    root = tk.Tk()
                    root.withdraw()
                    root.update()
                    
                    # MÃ©thode simple: attendre un changement de position
                    initial_pos = pyautogui.position()
                    while pyautogui.position() == initial_pos:
                        time.sleep(0.1)
                    
                    return pyautogui.position()
            except:
                time.sleep(0.1)
    
    def extract_from_video(self, video_path: str, frame_interval: int = 30) -> List[Dict]:
        """
        Extrait des donnÃ©es d'une vidÃ©o de graphique.
        
        Args:
            video_path: Chemin vers la vidÃ©o
            frame_interval: Intervalle entre les frames analysÃ©es
            
        Returns:
            Liste des donnÃ©es extraites
        """
        print(f"ðŸŽ¬ Extraction depuis la vidÃ©o: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        data_points = []
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                # Convertir en PIL pour OCR
                frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                
                # Extraction OCR sur toute la frame
                ocr_data = self._extract_ocr_from_screenshot(frame_pil, frame.shape[1]//2, frame.shape[0]//2)
                
                if ocr_data['value'] is not None:
                    data_points.append({
                        'frame': frame_count,
                        'timestamp': frame_count / cap.get(cv2.CAP_PROP_FPS),
                        'value': ocr_data['value'],
                        'source': 'video'
                    })
                    
                    print(f"ðŸ“ Frame {frame_count}: {ocr_data['value']}")
            
            frame_count += 1
        
        cap.release()
        
        print(f"âœ… Extraction terminÃ©e: {len(data_points)} points extraits")
        return data_points

# Fonction utilitaire pour utilisation simple
def quick_horizontal_scan(duration: float = 10.0) -> List[Dict]:
    """
    Fonction rapide pour un balayage horizontal simple.
    
    Args:
        duration: DurÃ©e du balayage en secondes
        
    Returns:
        Liste des points de donnÃ©es extraits
    """
    extractor = GraphDataExtractor()
    
    print("ðŸŽ¯ Balayage horizontal rapide")
    print("1. Positionnez votre souris au point de dÃ©part")
    input("Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt...")
    start_pos = pyautogui.position()
    
    print("2. Positionnez votre souris au point d'arrivÃ©e")
    input("Appuyez sur EntrÃ©e quand vous Ãªtes prÃªt...")
    end_pos = pyautogui.position()
    
    print("ðŸš€ DÃ©marrage dans 3 secondes...")
    time.sleep(3)
    
    return extractor.horizontal_scan_with_screenshots(
        start_pos[0], start_pos[1],
        end_pos[0], end_pos[1],
        duration=duration
    )

if __name__ == "__main__":
    # Exemple d'utilisation
    extractor = GraphDataExtractor()
    extractor.interactive_setup() 