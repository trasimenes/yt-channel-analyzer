"""
YouTube Date Correction Agent - Production Ready
================================================

Agent sp√©cialis√© pour d√©tecter et corriger les dates de publication YouTube incorrectes
qui ont √©t√© d√©finies comme dates d'import au lieu des vraies dates de publication YouTube.

‚ö†Ô∏è PROBL√àME CRITIQUE : DATES D'IMPORT vs DATES YOUTUBE R√âELLES
- Sympt√¥mes : Toutes les vid√©os avec la m√™me date (ex: 2025-07-30)
- Impact : Calculs de fr√©quence erron√©s (1000+ vid√©os/semaine impossible)
- Solution : R√©cup√©ration des vraies dates via YouTube API v3

üõ°Ô∏è PROTOCOLES DE S√âCURIT√â :
- Backup obligatoire avant toute modification
- Dry-run par d√©faut (--confirm requis pour appliquer)
- Validation par √©chantillonnage
- Rollback complet disponible
- Logging d√©taill√© de toutes les op√©rations

Auteur: Claude Code Agent
Date: 2025-07-30
"""

import sqlite3
import logging
import json
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import urllib.request
import urllib.parse
import urllib.error

try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    YOUTUBE_API_AVAILABLE = True
except ImportError:
    YOUTUBE_API_AVAILABLE = False

from .base import get_db_connection, DB_PATH


@dataclass
class DateAnomalyReport:
    """Rapport d'anomalie de dates"""
    competitor_id: int
    competitor_name: str
    total_videos: int
    suspicious_dates: int
    most_common_date: str
    date_frequency: int
    confidence_score: float
    anomaly_type: str


@dataclass
class DateCorrectionResult:
    """R√©sultat d'une correction de date"""
    video_id: str
    old_date: str
    new_date: str
    success: bool
    error_message: Optional[str] = None


class YouTubeDateCorrectionAgent:
    """
    Agent de Correction de Dates YouTube
    
    Agent d√©di√© pour corriger toutes les dates d'import erron√©es
    et restaurer les vraies dates de publication YouTube via l'API YouTube v3.
    
    üéØ FONCTIONNALIT√âS PRINCIPALES :
    - D√©tection d'anomalies de dates (dates identiques massives)
    - Backup automatique des donn√©es avant correction
    - R√©cup√©ration des vraies dates via YouTube API v3
    - Application s√©curis√©e des corrections (dry-run par d√©faut)
    - Validation par √©chantillonnage des corrections
    - Rollback complet en cas de probl√®me
    - Rapport d√©taill√© des op√©rations
    
    üö® SEUILS DE D√âTECTION :
    - >50 vid√©os avec la m√™me date = suspect
    - Date connue d'import (2025-07-05, 2025-07-30) = critique
    - published_at > youtube_published_at = incoh√©rent
    """
    
    def __init__(self, youtube_api_key: Optional[str] = None, dry_run: bool = True):
        """
        Initialiser l'agent de correction de dates
        
        Args:
            youtube_api_key: Cl√© API YouTube v3 (si disponible)
            dry_run: Mode dry-run par d√©faut (s√©curit√©)
        """
        self.youtube_api_key = youtube_api_key
        self.dry_run = dry_run
        self.backup_table_name = f"video_dates_backup_{int(time.time())}"
        
        # Configuration des logs
        self.setup_logging()
        
        # Seuils de d√©tection d'anomalies
        self.MASS_DATE_THRESHOLD = 50  # >50 vid√©os avec m√™me date = suspect
        self.KNOWN_IMPORT_DATES = [
            '2025-07-05',  # Date d'import connue Center Parcs
            '2025-07-30',  # Date d'import Pierre et Vacances
            '2025-07-21'   # Autre date d'import potentielle
        ]
        
        self.logger.info("ü§ñ YouTubeDateCorrectionAgent initialis√©")
        self.logger.info(f"üìä Mode: {'DRY-RUN' if dry_run else 'PRODUCTION'}")
    
    def setup_logging(self):
        """Configuration du syst√®me de logging"""
        log_file = Path("youtube_date_correction.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def detect_suspicious_dates(self, competitor_id: Optional[int] = None) -> List[DateAnomalyReport]:
        """
        D√©tecter toutes les anomalies de dates dans la base de donn√©es
        
        üö® TYPES D'ANOMALIES D√âTECT√âES :
        1. Dates identiques massives (>50 vid√©os m√™me date)
        2. Dates d'import connues (2025-07-05, 2025-07-30)
        3. Incoh√©rences temporelles (published_at > youtube_published_at)
        4. Uniformit√© suspecte par concurrent
        
        Args:
            competitor_id: ID concurrent sp√©cifique (None = tous)
            
        Returns:
            List[DateAnomalyReport]: Liste des anomalies d√©tect√©es
        """
        self.logger.info("üîç PHASE 1 : D√©tection des anomalies de dates")
        
        anomalies = []
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # Requ√™te pour analyser les dates par concurrent
            where_clause = "WHERE c.id = ?" if competitor_id else ""
            params = [competitor_id] if competitor_id else []
            
            query = f"""
            SELECT 
                c.id as competitor_id,
                c.name as competitor_name,
                COUNT(v.id) as total_videos,
                DATE(v.published_at) as most_common_date,
                COUNT(DATE(v.published_at)) as date_frequency,
                COUNT(DISTINCT DATE(v.published_at)) as distinct_dates
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            {where_clause}
            GROUP BY c.id, c.name, DATE(v.published_at)
            HAVING total_videos > 0
            ORDER BY date_frequency DESC
            """
            
            cursor.execute(query, params)
            results = cursor.fetchall()
            
            # Analyser chaque r√©sultat pour d√©tecter les anomalies
            for row in results:
                competitor_id = row['competitor_id']
                competitor_name = row['competitor_name']
                total_videos = row['total_videos']
                most_common_date = row['most_common_date']
                date_frequency = row['date_frequency']
                distinct_dates = row['distinct_dates'] if 'distinct_dates' in row.keys() else 1
                
                # Calculer le score de confiance d'anomalie
                confidence_score = self._calculate_anomaly_confidence(
                    total_videos, date_frequency, distinct_dates, most_common_date
                )
                
                # D√©terminer le type d'anomalie
                anomaly_type = self._determine_anomaly_type(
                    date_frequency, most_common_date, distinct_dates
                )
                
                # Cr√©er un rapport d'anomalie si suspect
                if confidence_score > 0.7:  # Seuil de confiance 70%
                    anomaly = DateAnomalyReport(
                        competitor_id=competitor_id,
                        competitor_name=competitor_name,
                        total_videos=total_videos,
                        suspicious_dates=date_frequency,
                        most_common_date=most_common_date,
                        date_frequency=date_frequency,
                        confidence_score=confidence_score,
                        anomaly_type=anomaly_type
                    )
                    anomalies.append(anomaly)
                    
                    self.logger.warning(
                        f"üö® ANOMALIE: {competitor_name} - {date_frequency}/{total_videos} "
                        f"vid√©os avec date {most_common_date} (confiance: {confidence_score:.1%})"
                    )
        
        self.logger.info(f"‚úÖ D√©tection termin√©e : {len(anomalies)} anomalies trouv√©es")
        return anomalies
    
    def _calculate_anomaly_confidence(self, total_videos: int, date_frequency: int, 
                                    distinct_dates: int, most_common_date: str) -> float:
        """Calculer le score de confiance qu'il s'agit d'une anomalie"""
        confidence = 0.0
        
        # Facteur 1: Pourcentage de vid√©os avec la m√™me date
        same_date_ratio = date_frequency / max(total_videos, 1)
        if same_date_ratio > 0.8:  # >80% m√™me date
            confidence += 0.4
        elif same_date_ratio > 0.5:  # >50% m√™me date
            confidence += 0.2
        
        # Facteur 2: Date d'import connue
        if any(known_date in most_common_date for known_date in self.KNOWN_IMPORT_DATES):
            confidence += 0.4
        
        # Facteur 3: Masse critique de vid√©os
        if date_frequency > self.MASS_DATE_THRESHOLD:
            confidence += 0.3
        elif date_frequency > 20:
            confidence += 0.1
        
        # Facteur 4: Diversit√© des dates (moins = plus suspect)
        if distinct_dates == 1 and total_videos > 10:
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def _determine_anomaly_type(self, date_frequency: int, most_common_date: str, 
                              distinct_dates: int) -> str:
        """D√©terminer le type d'anomalie d√©tect√©e"""
        if any(known_date in most_common_date for known_date in self.KNOWN_IMPORT_DATES):
            return "IMPORT_DATE_KNOWN"
        elif date_frequency > self.MASS_DATE_THRESHOLD:
            return "MASS_IDENTICAL_DATES"
        elif distinct_dates == 1:
            return "ALL_SAME_DATE"
        else:
            return "SUSPICIOUS_PATTERN"
    
    def backup_current_dates(self) -> bool:
        """
        Cr√©er une sauvegarde compl√®te des dates actuelles
        
        üõ°Ô∏è S√âCURIT√â : Backup obligatoire avant toute modification
        
        Returns:
            bool: Succ√®s de la sauvegarde
        """
        self.logger.info("üíæ PHASE 2 : Cr√©ation du backup des dates")
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # Cr√©er la table de backup
                cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS {self.backup_table_name} (
                    id INTEGER,
                    video_id TEXT,
                    concurrent_id INTEGER,
                    title TEXT,
                    published_at DATETIME,
                    youtube_published_at DATETIME,
                    backup_created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (id)
                )
                """)
                
                # Copier toutes les donn√©es de dates
                cursor.execute(f"""
                INSERT INTO {self.backup_table_name} 
                (id, video_id, concurrent_id, title, published_at, youtube_published_at)
                SELECT id, video_id, concurrent_id, title, published_at, youtube_published_at
                FROM video
                """)
                
                backup_count = cursor.rowcount
                conn.commit()
                
                self.logger.info(f"‚úÖ Backup cr√©√© : table '{self.backup_table_name}' avec {backup_count} entr√©es")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du backup : {e}")
            return False
    
    def fetch_youtube_dates(self, video_ids: List[str]) -> Dict[str, Optional[str]]:
        """
        R√©cup√©rer les vraies dates de publication depuis YouTube API v3
        
        üåê STRAT√âGIES DE R√âCUP√âRATION :
        1. YouTube API v3 (si cl√© disponible)
        2. Scraping l√©ger de la page YouTube (fallback)
        3. Estimation bas√©e sur l'ID vid√©o (dernier recours)
        
        Args:
            video_ids: Liste des IDs vid√©o YouTube
            
        Returns:
            Dict[str, Optional[str]]: Mapping video_id -> date de publication
        """
        self.logger.info(f"üåê PHASE 3 : R√©cup√©ration des vraies dates YouTube pour {len(video_ids)} vid√©os")
        
        dates_map = {}
        
        if self.youtube_api_key:
            dates_map = self._fetch_dates_via_api(video_ids)
        else:
            self.logger.warning("‚ö†Ô∏è Pas de cl√© API YouTube - utilisation du scraping leger")
            dates_map = self._fetch_dates_via_scraping(video_ids)
        
        success_count = sum(1 for date in dates_map.values() if date is not None)
        self.logger.info(f"‚úÖ Dates r√©cup√©r√©es : {success_count}/{len(video_ids)} vid√©os")
        
        return dates_map
    
    def _fetch_dates_via_api(self, video_ids: List[str]) -> Dict[str, Optional[str]]:
        """R√©cup√©rer les dates via YouTube API v3 (m√©thode pr√©f√©r√©e)"""
        dates_map = {}
        
        if not YOUTUBE_API_AVAILABLE:
            self.logger.error("‚ùå Google API client non disponible - fallback vers scraping")
            return self._fetch_dates_via_scraping(video_ids)
        
        try:
            # Initialiser le client YouTube API v3
            youtube = build('youtube', 'v3', developerKey=self.youtube_api_key)
            
            # Traiter par batches de 50 (limite API YouTube)
            batch_size = 50
            for i in range(0, len(video_ids), batch_size):
                batch = video_ids[i:i + batch_size]
                
                try:
                    # Faire la requ√™te API
                    request = youtube.videos().list(
                        part='snippet',
                        id=','.join(batch)
                    )
                    response = request.execute()
                    
                    # Parser les r√©sultats
                    for item in response.get('items', []):
                        video_id = item['id']
                        published_at = item['snippet']['publishedAt']
                        # Convertir de ISO format vers datetime
                        date_obj = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                        dates_map[video_id] = date_obj.strftime('%Y-%m-%d %H:%M:%S')
                    
                    # Marquer les vid√©os non trouv√©es
                    found_ids = {item['id'] for item in response.get('items', [])}
                    for video_id in batch:
                        if video_id not in found_ids:
                            dates_map[video_id] = None
                    
                    # Respecter les limites de taux
                    time.sleep(0.1)  # 100ms entre les requ√™tes
                    
                    self.logger.info(f"‚úÖ Batch {i//batch_size + 1}: {len(response.get('items', []))}/{len(batch)} vid√©os r√©cup√©r√©es")
                    
                except HttpError as e:
                    self.logger.error(f"‚ùå Erreur API YouTube pour batch {i}-{i+batch_size}: {e}")
                    # Marquer les vid√©os de ce batch comme non r√©cup√©r√©es
                    for video_id in batch:
                        if video_id not in dates_map:
                            dates_map[video_id] = None
                            
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur inattendue pour batch {i}-{i+batch_size}: {e}")
                    # Fallback vers scraping pour ce batch
                    fallback_dates = self._fetch_dates_via_scraping(batch)
                    dates_map.update(fallback_dates)
        
        except Exception as e:
            self.logger.error(f"‚ùå Erreur d'initialisation YouTube API: {e}")
            # Fallback complet vers scraping
            return self._fetch_dates_via_scraping(video_ids)
        
        return dates_map
    
    def _fetch_dates_via_scraping(self, video_ids: List[str]) -> Dict[str, Optional[str]]:
        """
        R√©cup√©ration l√©g√®re par scraping (fallback sans API)
        
        üö® ATTENTION : M√©thode de fallback uniquement
        Plus lente et moins fiable que l'API officielle
        """
        dates_map = {}
        
        for video_id in video_ids[:10]:  # Limiter √† 10 pour √©viter le rate limiting
            try:
                # URL de la vid√©o YouTube
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                
                # Headers pour √©viter la d√©tection de bot
                req = urllib.request.Request(
                    video_url,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                    }
                )
                
                with urllib.request.urlopen(req, timeout=10) as response:
                    html = response.read().decode('utf-8')
                
                # Chercher la date de publication dans le HTML
                date_match = re.search(r'"publishDate":"([^"]*)"', html)
                if date_match:
                    iso_date = date_match.group(1)
                    date_obj = datetime.fromisoformat(iso_date.replace('Z', '+00:00'))
                    dates_map[video_id] = date_obj.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    dates_map[video_id] = None
                
                # D√©lai pour √©viter le rate limiting
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur scraping pour {video_id}: {e}")
                dates_map[video_id] = None
        
        # Pour les vid√©os restantes, retourner None
        for video_id in video_ids[10:]:
            dates_map[video_id] = None
        
        return dates_map
    
    def apply_corrections(self, competitor_id: int, confirm: bool = False) -> List[DateCorrectionResult]:
        """
        Appliquer les corrections de dates pour un concurrent
        
        üõ°Ô∏è S√âCURIT√â :
        - Dry-run par d√©faut (confirm=False)
        - Backup obligatoire avant application
        - Validation des donn√©es avant √©criture
        - Logging de chaque modification
        
        Args:
            competitor_id: ID du concurrent √† corriger
            confirm: True pour appliquer r√©ellement (False = dry-run)
            
        Returns:
            List[DateCorrectionResult]: R√©sultats des corrections
        """
        self.logger.info(f"üîß PHASE 4 : Application des corrections pour concurrent {competitor_id}")
        self.logger.info(f"üìä Mode: {'PRODUCTION' if confirm else 'DRY-RUN'}")
        
        if confirm and not self.backup_current_dates():
            self.logger.error("‚ùå Impossible de cr√©er le backup - abandon")
            return []
        
        corrections = []
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # R√©cup√©rer les vid√©os √† corriger
                cursor.execute("""
                SELECT id, video_id, title, published_at, youtube_published_at
                FROM video 
                WHERE concurrent_id = ?
                ORDER BY id
                """, (competitor_id,))
                
                videos = cursor.fetchall()
                video_ids = [v['video_id'] for v in videos]
                
                # R√©cup√©rer les vraies dates YouTube
                youtube_dates = self.fetch_youtube_dates(video_ids)
                
                # Appliquer les corrections
                for video in videos:
                    video_id = video['video_id']
                    current_date = video['published_at']
                    youtube_date = youtube_dates.get(video_id)
                    
                    if youtube_date and youtube_date != current_date:
                        if confirm:
                            # Appliquer la correction en base
                            cursor.execute("""
                            UPDATE video 
                            SET published_at = ?, youtube_published_at = ?, last_updated = CURRENT_TIMESTAMP
                            WHERE id = ?
                            """, (youtube_date, youtube_date, video['id']))
                        
                        correction = DateCorrectionResult(
                            video_id=video_id,
                            old_date=current_date,
                            new_date=youtube_date,
                            success=True
                        )
                        corrections.append(correction)
                        
                        self.logger.info(
                            f"{'‚úÖ CORRIG√â' if confirm else 'üîç √Ä CORRIGER'}: {video_id} "
                            f"{current_date} ‚Üí {youtube_date}"
                        )
                    
                    elif not youtube_date:
                        correction = DateCorrectionResult(
                            video_id=video_id,
                            old_date=current_date,
                            new_date=None,
                            success=False,
                            error_message="Date YouTube non r√©cup√©rable"
                        )
                        corrections.append(correction)
                
                if confirm:
                    conn.commit()
                    self.logger.info(f"‚úÖ {len(corrections)} corrections appliqu√©es en base")
                else:
                    self.logger.info(f"üîç {len(corrections)} corrections simul√©es (dry-run)")
        
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors des corrections : {e}")
            if confirm:
                conn.rollback()
        
        return corrections
    
    def generate_report(self, anomalies: List[DateAnomalyReport], 
                       corrections: List[DateCorrectionResult]) -> str:
        """
        G√©n√©rer un rapport d√©taill√© des corrections
        
        üìä CONTENU DU RAPPORT :
        - R√©sum√© des anomalies d√©tect√©es
        - D√©tail des corrections appliqu√©es  
        - Statistiques de succ√®s/√©chec
        - Recommandations d'actions
        
        Args:
            anomalies: Liste des anomalies d√©tect√©es
            corrections: Liste des corrections appliqu√©es
            
        Returns:
            str: Rapport format√© en fran√ßais
        """
        report_lines = [
            "üìä RAPPORT DE CORRECTION DES DATES YOUTUBE",
            "=" * 50,
            f"üïê G√©n√©r√© le : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "üîç ANALYSE DES ANOMALIES",
            "-" * 25
        ]
        
        # Section anomalies
        if anomalies:
            total_videos = sum(a.total_videos for a in anomalies)
            total_suspicious = sum(a.suspicious_dates for a in anomalies)
            
            report_lines.extend([
                f"Concurrents analys√©s : {len(anomalies)}",
                f"Vid√©os totales analys√©es : {total_videos:,}",
                f"Dates suspectes d√©tect√©es : {total_suspicious:,}",
                f"Taux d'anomalie : {total_suspicious/max(total_videos,1):.1%}",
                ""
            ])
            
            # D√©tail par concurrent
            for anomaly in anomalies:
                report_lines.extend([
                    f"üö® {anomaly.competitor_name} (ID: {anomaly.competitor_id})",
                    f"   ‚Ä¢ Vid√©os totales : {anomaly.total_videos}",
                    f"   ‚Ä¢ Date suspecte : {anomaly.most_common_date}",
                    f"   ‚Ä¢ Fr√©quence : {anomaly.date_frequency} vid√©os ({anomaly.date_frequency/anomaly.total_videos:.1%})",
                    f"   ‚Ä¢ Type d'anomalie : {anomaly.anomaly_type}",
                    f"   ‚Ä¢ Confiance : {anomaly.confidence_score:.1%}",
                    ""
                ])
        else:
            report_lines.append("‚úÖ Aucune anomalie d√©tect√©e")
        
        # Section corrections
        report_lines.extend([
            "",
            "üîß CORRECTIONS APPLIQU√âES",
            "-" * 25
        ])
        
        if corrections:
            successful = [c for c in corrections if c.success]
            failed = [c for c in corrections if not c.success]
            
            report_lines.extend([
                f"Corrections tent√©es : {len(corrections)}",
                f"Corrections r√©ussies : {len(successful)}",
                f"Corrections √©chou√©es : {len(failed)}",
                f"Taux de succ√®s : {len(successful)/max(len(corrections),1):.1%}",
                ""
            ])
            
            # √âchantillon de corrections r√©ussies
            if successful:
                report_lines.extend([
                    "üìã √âCHANTILLON DE CORRECTIONS R√âUSSIES :",
                    ""
                ])
                for correction in successful[:5]:
                    report_lines.append(
                        f"   ‚Ä¢ {correction.video_id}: {correction.old_date} ‚Üí {correction.new_date}"
                    )
                if len(successful) > 5:
                    report_lines.append(f"   ... et {len(successful) - 5} autres")
                report_lines.append("")
            
            # √âchantillon d'√©checs
            if failed:
                report_lines.extend([
                    "‚ùå √âCHANTILLON D'√âCHECS :",
                    ""
                ])
                for correction in failed[:3]:
                    report_lines.append(
                        f"   ‚Ä¢ {correction.video_id}: {correction.error_message}"
                    )
                if len(failed) > 3:
                    report_lines.append(f"   ... et {len(failed) - 3} autres")
                report_lines.append("")
        else:
            report_lines.append("‚ÑπÔ∏è Aucune correction appliqu√©e")
        
        # Section recommandations
        report_lines.extend([
            "",
            "üí° RECOMMANDATIONS",
            "-" * 18,
            "",
            "1. üîç V√âRIFICATION :",
            "   ‚Ä¢ Valider manuellement un √©chantillon de corrections",
            "   ‚Ä¢ V√©rifier les m√©triques de fr√©quence apr√®s correction",
            "   ‚Ä¢ Contr√¥ler la coh√©rence des nouvelles dates",
            "",
            "2. üõ°Ô∏è S√âCURIT√â :",
            f"   ‚Ä¢ Backup cr√©√© : table '{self.backup_table_name}'",
            "   ‚Ä¢ Rollback disponible avec la m√©thode rollback()",
            "   ‚Ä¢ Log d√©taill√© dans youtube_date_correction.log",
            "",
            "3. üìà SUIVI :",
            "   ‚Ä¢ Recalculer les m√©triques de fr√©quence temporelles", 
            "   ‚Ä¢ Mettre √† jour les analyses de tendances",
            "   ‚Ä¢ Auditer les autres concurrents pour des anomalies similaires",
            "",
            "4. üîÑ PR√âVENTION :",
            "   ‚Ä¢ Modifier les scripts d'import pour utiliser youtube_published_at",
            "   ‚Ä¢ Ajouter des validations de coh√©rence de dates",
            "   ‚Ä¢ Impl√©menter des alertes pour d√©tecter les futures anomalies"
        ])
        
        return "\n".join(report_lines)
    
    def rollback(self) -> bool:
        """
        Restaurer les dates originales depuis le backup
        
        üö® FONCTION DE S√âCURIT√â CRITIQUE
        Permet de revenir √† l'√©tat initial en cas de probl√®me
        
        Returns:
            bool: Succ√®s du rollback
        """
        self.logger.info("üîÑ ROLLBACK : Restauration des dates originales")
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # V√©rifier que la table de backup existe
                cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                    (self.backup_table_name,)
                )
                
                if not cursor.fetchone():
                    self.logger.error(f"‚ùå Table de backup '{self.backup_table_name}' introuvable")
                    return False
                
                # Restaurer les dates depuis le backup
                cursor.execute(f"""
                UPDATE video 
                SET published_at = b.published_at,
                    youtube_published_at = b.youtube_published_at,
                    last_updated = CURRENT_TIMESTAMP
                FROM {self.backup_table_name} b
                WHERE video.id = b.id
                """)
                
                restored_count = cursor.rowcount
                conn.commit()
                
                self.logger.info(f"‚úÖ Rollback r√©ussi : {restored_count} vid√©os restaur√©es")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du rollback : {e}")
            return False
    
    def validate_corrections(self, competitor_id: int, sample_size: int = 5) -> Dict[str, Any]:
        """
        Valider un √©chantillon de corrections appliqu√©es
        
        üîç CONTR√îLE QUALIT√â :
        - V√©rification manuelle recommand√©e
        - Comparaison avec sources externes
        - D√©tection d'incoh√©rences
        
        Args:
            competitor_id: ID du concurrent √† valider
            sample_size: Nombre de vid√©os √† √©chantillonner
            
        Returns:
            Dict[str, Any]: R√©sultats de validation
        """
        self.logger.info(f"üîç VALIDATION : Contr√¥le qualit√© pour concurrent {competitor_id}")
        
        validation_results = {
            'competitor_id': competitor_id,
            'sample_size': sample_size,
            'samples': [],
            'coherence_score': 0.0,
            'recommendations': []
        }
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # R√©cup√©rer un √©chantillon al√©atoire
                cursor.execute("""
                SELECT video_id, title, published_at, youtube_published_at, 
                       view_count, like_count
                FROM video 
                WHERE concurrent_id = ? 
                ORDER BY RANDOM() 
                LIMIT ?
                """, (competitor_id, sample_size))
                
                samples = cursor.fetchall()
                
                coherent_samples = 0
                for sample in samples:
                    sample_data = {
                        'video_id': sample['video_id'],
                        'title': sample['title'][:50] + '...' if len(sample['title']) > 50 else sample['title'],
                        'published_at': sample['published_at'],
                        'youtube_published_at': sample['youtube_published_at'],
                        'is_coherent': True,
                        'issues': []
                    }
                    
                    # V√©rifications de coh√©rence
                    if sample['published_at'] and sample['youtube_published_at']:
                        pub_date = datetime.fromisoformat(sample['published_at'].replace('Z', ''))
                        yt_date = datetime.fromisoformat(sample['youtube_published_at'].replace('Z', ''))
                        
                        # Les dates doivent √™tre identiques ou tr√®s proches
                        if abs((pub_date - yt_date).total_seconds()) > 86400:  # >24h de diff√©rence
                            sample_data['is_coherent'] = False
                            sample_data['issues'].append("Dates published_at et youtube_published_at diff√©rentes")
                    
                    # V√©rifier que la date n'est pas dans le futur
                    if sample['published_at']:
                        pub_date = datetime.fromisoformat(sample['published_at'].replace('Z', ''))
                        if pub_date > datetime.now():
                            sample_data['is_coherent'] = False
                            sample_data['issues'].append("Date de publication dans le futur")
                    
                    if sample_data['is_coherent']:
                        coherent_samples += 1
                    
                    validation_results['samples'].append(sample_data)
                
                # Calculer le score de coh√©rence
                validation_results['coherence_score'] = coherent_samples / max(len(samples), 1)
                
                # G√©n√©rer des recommandations
                if validation_results['coherence_score'] < 0.8:
                    validation_results['recommendations'].append(
                        "üö® Score de coh√©rence faible - r√©vision manuelle recommand√©e"
                    )
                
                if validation_results['coherence_score'] > 0.95:
                    validation_results['recommendations'].append(
                        "‚úÖ Corrections coh√©rentes - qualit√© excellente"
                    )
                
                self.logger.info(
                    f"‚úÖ Validation termin√©e : score de coh√©rence {validation_results['coherence_score']:.1%}"
                )
        
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la validation : {e}")
            validation_results['error'] = str(e)
        
        return validation_results


def main():
    """
    Script de d√©monstration pour Pierre et Vacances (competitor 504)
    
    Usage:
        python youtube_date_corrector.py --analyze                    # Analyse uniquement
        python youtube_date_corrector.py --fix --confirm             # Correction r√©elle
        python youtube_date_corrector.py --fix --competitor-id 504   # Concurrent sp√©cifique
        python youtube_date_corrector.py --rollback                  # Restauration
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="YouTube Date Correction Agent")
    parser.add_argument('--analyze', action='store_true', help="Analyse uniquement des anomalies")
    parser.add_argument('--fix', action='store_true', help="Appliquer les corrections")
    parser.add_argument('--confirm', action='store_true', help="Confirmer les modifications (sinon dry-run)")
    parser.add_argument('--competitor-id', type=int, help="ID concurrent sp√©cifique")
    parser.add_argument('--rollback', action='store_true', help="Restaurer depuis le backup")
    parser.add_argument('--youtube-api-key', help="Cl√© API YouTube v3")
    
    args = parser.parse_args()
    
    # Initialiser l'agent
    agent = YouTubeDateCorrectionAgent(
        youtube_api_key=args.youtube_api_key,
        dry_run=not args.confirm
    )
    
    try:
        if args.rollback:
            # Rollback
            success = agent.rollback()
            if success:
                print("‚úÖ Rollback r√©ussi")
            else:
                print("‚ùå √âchec du rollback")
                
        elif args.analyze or args.fix:
            # Phase 1: D√©tection des anomalies
            anomalies = agent.detect_suspicious_dates(args.competitor_id)
            
            if not anomalies:
                print("‚úÖ Aucune anomalie d√©tect√©e")
                return
            
            # Affichage des anomalies
            print(f"\nüö® {len(anomalies)} anomalies d√©tect√©es :")
            for anomaly in anomalies:
                print(f"  ‚Ä¢ {anomaly.competitor_name} : {anomaly.suspicious_dates}/{anomaly.total_videos} "
                      f"vid√©os avec date {anomaly.most_common_date} (confiance: {anomaly.confidence_score:.1%})")
            
            if args.fix:
                # Phase 2: Application des corrections
                all_corrections = []
                
                for anomaly in anomalies:
                    print(f"\nüîß Correction de {anomaly.competitor_name}...")
                    corrections = agent.apply_corrections(anomaly.competitor_id, args.confirm)
                    all_corrections.extend(corrections)
                
                # Phase 3: G√©n√©ration du rapport
                report = agent.generate_report(anomalies, all_corrections)
                print("\n" + report)
                
                # Sauvegarder le rapport
                report_file = f"youtube_date_correction_report_{int(time.time())}.txt"
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"\nüìÑ Rapport sauvegard√© : {report_file}")
                
                # Phase 4: Validation (si corrections appliqu√©es)
                if args.confirm and all_corrections:
                    print("\nüîç Validation des corrections...")
                    for anomaly in anomalies:
                        validation = agent.validate_corrections(anomaly.competitor_id)
                        print(f"  ‚Ä¢ {anomaly.competitor_name} : coh√©rence {validation['coherence_score']:.1%}")
        
        else:
            parser.print_help()
    
    except KeyboardInterrupt:
        print("\n\nüõë Op√©ration annul√©e par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur critique : {e}")
        agent.logger.error(f"Erreur critique : {e}")


if __name__ == "__main__":
    main()