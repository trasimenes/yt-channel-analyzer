"""
Module pour les interactions avec la base de donn√©es SQLite.
"""

import sqlite3
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import re
from collections import defaultdict

DB_PATH = 'instance/database.db'

def get_db_connection():
    """Cr√©er une connexion √† la base de donn√©es"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # Pour acc√©der aux colonnes par nom
    return conn

def extract_video_id_from_url(video_url: str) -> str:
    """Extraire l'ID vid√©o depuis l'URL YouTube"""
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
        r'(?:embed\/)([0-9A-Za-z_-]{11})',
        r'(?:v\/)([0-9A-Za-z_-]{11})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, video_url)
        if match:
            return match.group(1)
    
    # Fallback: utiliser l'URL compl√®te comme ID
    return video_url.replace('/', '_').replace(':', '_')[:50]

def extract_channel_id_from_url(channel_url: str) -> str:
    """Extraire l'ID de cha√Æne depuis l'URL YouTube"""
    if '/channel/' in channel_url:
        return channel_url.split('/channel/')[-1].split('/')[0].split('?')[0]
    elif '/@' in channel_url:
        return channel_url.split('/@')[-1].split('/')[0].split('?')[0]
    elif '/c/' in channel_url:
        return channel_url.split('/c/')[-1].split('/')[0].split('?')[0]
    elif '/user/' in channel_url:
        return channel_url.split('/user/')[-1].split('/')[0].split('?')[0]
    else:
        return channel_url.replace('/', '_').replace(':', '_')[:100]

def detect_language(text: str) -> str:
    """
    D√©tection tr√®s l√©g√®re de la langue pour √©viter une d√©pendance externe.
    Analyse quelques marqueurs courants FR / EN / DE / NL.

    Args:
        text: Texte √† analyser

    Returns:
        str: Code langue ('fr', 'en', 'de', 'nl')
    """
    if not text:
        return 'en'

    text_lower = text.lower()
    # Mots tr√®s fr√©quents par langue
    french_markers = ['le ', 'la ', 'les ', 'des ', 'un ', 'une ', 'comment ', 'pour ', 'avec ']
    german_markers = ['der ', 'die ', 'das ', 'und ', 'mit ', 'eine ', 'ein ', 'wie ']
    dutch_markers = ['de ', 'het ', 'een ', 'hoe ', 'met ', 'en ']

    score_fr = sum(text_lower.count(m) for m in french_markers)
    score_de = sum(text_lower.count(m) for m in german_markers)
    score_nl = sum(text_lower.count(m) for m in dutch_markers)

    # Par d√©faut anglais si aucun score dominant
    if max(score_fr, score_de, score_nl) == 0:
        return 'en'

    if score_fr >= score_de and score_fr >= score_nl:
        return 'fr'
    if score_de >= score_fr and score_de >= score_nl:
        return 'de'
    return 'nl'
def save_competitor_and_videos(channel_url: str, videos: List[Dict], channel_info: Dict = None) -> int:
    """
    Sauvegarder un concurrent et ses vid√©os dans la base de donn√©es.
    Cr√©e automatiquement le concurrent s'il n'existe pas.
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si le concurrent existe d√©j√†
        cursor.execute('SELECT id FROM concurrent WHERE channel_url = ?', (channel_url,))
        existing = cursor.fetchone()
        
        if existing:
            competitor_id = existing[0]
            print(f"[DB] Concurrent existant trouv√©: ID {competitor_id}")
        else:
            # Cr√©er un nouveau concurrent
            channel_id = extract_channel_id_from_url(channel_url)
            
            # Utiliser les infos de la cha√Æne si disponibles
            if channel_info:
                name = channel_info.get('title', 'Canal inconnu')
                thumbnail_url = channel_info.get('thumbnail', '')
                banner_url = channel_info.get('banner', '')
                description = channel_info.get('description', '')
                subscriber_count = channel_info.get('subscriber_count')
                view_count = channel_info.get('view_count')
                video_count = len(videos)
                country = channel_info.get('country', '')
                language = channel_info.get('language', '')
            else:
                # Valeurs par d√©faut
                name = channel_url.split('/')[-1] if '/' in channel_url else 'Canal inconnu'
                thumbnail_url = ''
                banner_url = ''
                description = ''
                subscriber_count = None
                view_count = None
                video_count = len(videos)
                country = ''
                language = ''
            
            cursor.execute('''
                INSERT INTO concurrent (
                    name, channel_id, channel_url, thumbnail_url, banner_url,
                    description, subscriber_count, view_count, video_count,
                    country, language, created_at, last_updated
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                name, channel_id, channel_url, thumbnail_url, banner_url,
                description, subscriber_count, view_count, video_count,
                country, language, datetime.now(), datetime.now()
            ))
            
            competitor_id = cursor.lastrowid
            print(f"[DB] Nouveau concurrent cr√©√©: {name} (ID {competitor_id})")
        
        # Sauvegarder les vid√©os
        videos_added = 0
        videos_updated = 0
        
        for video in videos:
            video_id = extract_video_id_from_url(video.get('url', ''))
            
            # V√©rifier si la vid√©o existe d√©j√†
            cursor.execute('SELECT id FROM video WHERE video_id = ?', (video_id,))
            existing_video = cursor.fetchone()
            
            # Pr√©parer les donn√©es de la vid√©o
            title = video.get('title', '')[:200]  # Limiter √† 200 caract√®res
            description = video.get('description', '')
            url = video.get('url', '')
            thumbnail_url = video.get('thumbnail', '')
            
            # Traiter la date de publication
            published_at = None
            if video.get('publication_date'):
                try:
                    # Essayer de parser diff√©rents formats de date
                    date_str = video['publication_date']
                    if 'il y a' in date_str:
                        # Pour les dates relatives, utiliser la date actuelle
                        published_at = datetime.now()
                    else:
                        # Essayer de parser la date directement
                        published_at = datetime.strptime(date_str, '%Y-%m-%d')
                except:
                    published_at = datetime.now()
            
            # Traiter la dur√©e
            duration_seconds = None
            duration_text = video.get('duration', '')
            if duration_text:
                try:
                    # Convertir "2:45" en secondes
                    parts = duration_text.split(':')
                    if len(parts) == 2:
                        duration_seconds = int(parts[0]) * 60 + int(parts[1])
                    elif len(parts) == 3:
                        duration_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
                except:
                    pass
            
            # Traiter les m√©triques
            view_count = video.get('views') or video.get('view_count') or 0
            if isinstance(view_count, str):
                view_count = int(re.sub(r'[^\d]', '', view_count)) if re.sub(r'[^\d]', '', view_count) else 0
            
            like_count = video.get('likes') or video.get('like_count')
            comment_count = video.get('comments') or video.get('comment_count')
            
            if existing_video:
                # Mettre √† jour la vid√©o existante
                cursor.execute('''
                    UPDATE video SET
                        title = ?, description = ?, url = ?, thumbnail_url = ?,
                        published_at = ?, duration_seconds = ?, duration_text = ?,
                        view_count = ?, like_count = ?, comment_count = ?,
                        last_updated = ?
                    WHERE id = ?
                ''', (
                    title, description, url, thumbnail_url,
                    published_at, duration_seconds, duration_text,
                    view_count, like_count, comment_count,
                    datetime.now(), existing_video[0]
                ))
                videos_updated += 1
            else:
                # Ins√©rer une nouvelle vid√©o
                cursor.execute('''
                    INSERT INTO video (
                        concurrent_id, video_id, title, description, url, thumbnail_url,
                        published_at, duration_seconds, duration_text,
                        view_count, like_count, comment_count,
                        created_at, last_updated
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    competitor_id, video_id, title, description, url, thumbnail_url,
                    published_at, duration_seconds, duration_text,
                    view_count, like_count, comment_count,
                    datetime.now(), datetime.now()
                ))
                videos_added += 1
        
        # Mettre √† jour le nombre de vid√©os du concurrent
        cursor.execute('SELECT COUNT(*) FROM video WHERE concurrent_id = ?', (competitor_id,))
        total_videos = cursor.fetchone()[0]
        
        cursor.execute('''
            UPDATE concurrent SET video_count = ?, last_updated = ? WHERE id = ?
        ''', (total_videos, datetime.now(), competitor_id))
        
        conn.commit()
        print(f"[DB] Sauvegarde termin√©e: {videos_added} vid√©os ajout√©es, {videos_updated} mises √† jour, total: {total_videos}")
        
        return competitor_id
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] Erreur lors de la sauvegarde: {e}")
        raise
    finally:
        conn.close()

def get_competitor_videos(competitor_id: int) -> List[Dict]:
    """R√©cup√©rer toutes les vid√©os d'un concurrent"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT * FROM video 
        WHERE concurrent_id = ? 
        ORDER BY published_at DESC
    ''', (competitor_id,))
    
    videos = []
    for row in cursor.fetchall():
        videos.append(dict(row))
    
    conn.close()
    return videos

def get_all_competitors() -> List[Dict]:
    """R√©cup√©rer tous les concurrents depuis la base de donn√©es avec vues totales calcul√©es"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT * FROM concurrent 
        ORDER BY name
    ''')
    
    competitors = []
    for row in cursor.fetchall():
        competitor = dict(row)
        
        # Calculer les vues totales en sommant les vues de toutes ses vid√©os
        cursor.execute('''
            SELECT COALESCE(SUM(view_count), 0) as total_views
            FROM video 
            WHERE concurrent_id = ?
        ''', (competitor['id'],))
        
        total_views_result = cursor.fetchone()
        competitor['total_views'] = total_views_result[0] if total_views_result else 0
        
        competitors.append(competitor)
    
    conn.close()
    return competitors

def get_competitor_by_url(channel_url: str) -> Optional[Dict]:
    """R√©cup√©rer un concurrent par son URL"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('SELECT * FROM concurrent WHERE channel_url = ?', (channel_url,))
    row = cursor.fetchone()
    
    conn.close()
    return dict(row) if row else None

def add_competitor(competitor_data: Dict) -> int:
    """Ajouter un nouveau concurrent"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO concurrent (
            name, channel_id, channel_url, thumbnail_url, banner_url,
            description, subscriber_count, view_count, video_count,
            country, language, created_at, last_updated
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        competitor_data.get('name'),
        competitor_data.get('channel_id'),
        competitor_data.get('channel_url'),
        competitor_data.get('thumbnail_url'),
        competitor_data.get('banner_url'),
        competitor_data.get('description'),
        competitor_data.get('subscriber_count'),
        competitor_data.get('view_count'),
        competitor_data.get('video_count'),
        competitor_data.get('country'),
        competitor_data.get('language'),
        datetime.now(),
        datetime.now()
    ))
    
    competitor_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    return competitor_id

def update_competitor(competitor_id: int, updates: Dict):
    """Mettre √† jour un concurrent"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Construire la requ√™te dynamiquement
    set_clause = []
    values = []
    
    for key, value in updates.items():
        if key in ['name', 'thumbnail_url', 'banner_url', 'description', 
                   'subscriber_count', 'view_count', 'video_count', 'country', 'language']:
            set_clause.append(f"{key} = ?")
            values.append(value)
    
    if set_clause:
        set_clause.append("last_updated = ?")
        values.append(datetime.now())
        values.append(competitor_id)
        
        query = f"UPDATE concurrent SET {', '.join(set_clause)} WHERE id = ?"
        cursor.execute(query, values)
        conn.commit()
    
    conn.close()

def delete_competitor(competitor_id: int):
    """Supprimer un concurrent et toutes ses vid√©os"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # SQLite g√®re automatiquement la suppression en cascade des vid√©os
    cursor.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
    
    conn.commit()
    conn.close()

def competitors_to_legacy_format() -> Dict:
    """Convertir les concurrents de la base vers le format JSON legacy pour compatibilit√©"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # R√©cup√©rer tous les concurrents avec leurs vid√©os
    cursor.execute('''
        SELECT c.*, COUNT(v.id) as video_count_actual
        FROM concurrent c
        LEFT JOIN video v ON c.id = v.concurrent_id
        GROUP BY c.id
        ORDER BY c.name
    ''')
    
    competitors = cursor.fetchall()
    legacy_data = {}
    
    for comp in competitors:
        # Cr√©er une cl√© compatible avec l'ancien format
        key = comp['channel_url'].replace('/', '_').replace(':', '_').replace('?', '_').replace('&', '_')
        
        # R√©cup√©rer les vid√©os de ce concurrent
        cursor.execute('''
            SELECT * FROM video WHERE concurrent_id = ? ORDER BY published_at DESC
        ''', (comp['id'],))
        
        videos = []
        for video_row in cursor.fetchall():
            video_dict = dict(video_row)
            # Convertir au format legacy
            videos.append({
                'title': video_dict['title'],
                'url': video_dict['url'],
                'views': video_dict['view_count'] or 0,
                'view_count': video_dict['view_count'] or 0,
                'publication_date': video_dict['published_at'].strftime('%Y-%m-%d') if video_dict['published_at'] and hasattr(video_dict['published_at'], 'strftime') else str(video_dict['published_at']) if video_dict['published_at'] else '',
                'duration': video_dict['duration_text'] or '',
                'thumbnail': video_dict['thumbnail_url'] or '',
                'description': video_dict['description'] or '',
                'likes': video_dict['like_count'],
                'comments': video_dict['comment_count']
            })
        
        legacy_data[key] = {
            'name': comp['name'],
            'channel_url': comp['channel_url'],
            'channel_id': comp['channel_id'],
            'thumbnail': comp['thumbnail_url'],
            'banner': comp['banner_url'],
            'description': comp['description'],
            'subscriber_count': comp['subscriber_count'],
            'view_count': comp['view_count'],
            'video_count': comp['video_count_actual'],  # Utiliser le vrai count
            'country': comp['country'],
            'language': comp['language'],
            'last_updated': comp['last_updated'].strftime('%Y-%m-%d %H:%M:%S') if comp['last_updated'] and hasattr(comp['last_updated'], 'strftime') else str(comp['last_updated']) if comp['last_updated'] else '',
            'videos': videos,
            'total_videos': len(videos),
            'total_views': sum(v.get('view_count', 0) for v in videos)
        }
    
    conn.close()
    return legacy_data

def save_competitor_playlists(competitor_id: int, playlists: List[Dict]):
    """Sauvegarder les playlists d'un concurrent"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        for playlist in playlists:
            playlist_id = playlist.get('playlist_id', '')
            name = playlist.get('name', '')[:200]
            description = playlist.get('description', '')
            thumbnail_url = playlist.get('thumbnail_url', '')
            video_count = playlist.get('video_count', 0)
            
            # V√©rifier si la playlist existe d√©j√†
            cursor.execute(
                'SELECT id FROM playlist WHERE playlist_id = ? AND concurrent_id = ?',
                (playlist_id, competitor_id)
            )
            existing = cursor.fetchone()
            
            if existing:
                # Mettre √† jour la playlist existante
                cursor.execute('''
                    UPDATE playlist SET
                        name = ?, description = ?, thumbnail_url = ?, video_count = ?,
                        last_updated = ?
                    WHERE id = ?
                ''', (name, description, thumbnail_url, video_count, datetime.now(), existing[0]))
            else:
                # Cr√©er une nouvelle playlist
                cursor.execute('''
                    INSERT INTO playlist (
                        concurrent_id, playlist_id, name, description, thumbnail_url, video_count,
                        created_at, last_updated
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    competitor_id, playlist_id, name, description, thumbnail_url, video_count,
                    datetime.now(), datetime.now()
                ))
        
        conn.commit()
        print(f"[DB] Sauvegarde termin√©e: {len(playlists)} playlists")
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] Erreur lors de la sauvegarde des playlists: {e}")
        raise
    finally:
        conn.close()

def save_subscriber_data(competitor_id: int, subscriber_data: List[Dict]) -> bool:
    """
    Sauvegarder les donn√©es d'abonn√©s import√©es depuis un CSV
    
    Args:
        competitor_id: ID du concurrent
        subscriber_data: Liste de dictionnaires avec 'date' et 'subscribers'
        
    Returns:
        bool: True si la sauvegarde a r√©ussi, False sinon
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        saved_count = 0
        updated_count = 0
        
        for entry in subscriber_data:
            date = entry.get('date')
            subscribers = entry.get('subscribers')
            
            if not date or subscribers is None:
                continue
                
            # V√©rifier si une entr√©e existe d√©j√† pour cette date
            cursor.execute('''
                SELECT id, subscriber_count FROM subscriber_data 
                WHERE concurrent_id = ? AND date = ?
            ''', (competitor_id, date))
            existing = cursor.fetchone()
            
            if existing:
                # Mettre √† jour si le nombre d'abonn√©s est diff√©rent
                if existing[1] != subscribers:
                    cursor.execute('''
                        UPDATE subscriber_data SET
                            subscriber_count = ?,
                            imported_at = ?
                        WHERE id = ?
                    ''', (subscribers, datetime.now(), existing[0]))
                    updated_count += 1
            else:
                # Ins√©rer une nouvelle entr√©e
                cursor.execute('''
                    INSERT INTO subscriber_data (
                        concurrent_id, date, subscriber_count, imported_at, source
                    ) VALUES (?, ?, ?, ?, ?)
                ''', (competitor_id, date, subscribers, datetime.now(), 'csv_import'))
                saved_count += 1
        
        conn.commit()
        print(f"[DB] Donn√©es d'abonn√©s sauvegard√©es: {saved_count} nouvelles, {updated_count} mises √† jour")
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] Erreur lors de la sauvegarde des donn√©es d'abonn√©s: {e}")
        return False
    finally:
        conn.close()

def get_competitor_subscriber_data(competitor_id: int) -> List[Dict]:
    """
    R√©cup√©rer les donn√©es d'√©volution des abonn√©s d'un concurrent
    
    Args:
        competitor_id: ID du concurrent
        
    Returns:
        List[Dict]: Liste des donn√©es d'abonn√©s tri√©es par date
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            SELECT date, subscriber_count, imported_at, source
            FROM subscriber_data 
            WHERE concurrent_id = ? 
            ORDER BY date ASC
        ''', (competitor_id,))
        
        subscriber_data = []
        for row in cursor.fetchall():
            subscriber_data.append({
                'date': row[0],
                'subscriber_count': row[1],
                'imported_at': row[2],
                'source': row[3]
            })
        
        return subscriber_data
        
    finally:
        conn.close()

def get_competitor_playlists(competitor_id: int) -> List[Dict]:
    """R√©cup√©rer toutes les playlists d'un concurrent avec les champs de tracking"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT id, concurrent_id, playlist_id, name, description, thumbnail_url, 
               category, video_count, created_at, last_updated,
               classification_source, classification_date, classification_confidence, human_verified
        FROM playlist 
        WHERE concurrent_id = ? 
        ORDER BY video_count DESC, name ASC
    ''', (competitor_id,))
    
    playlists = []
    columns = [desc[0] for desc in cursor.description]
    
    for row in cursor.fetchall():
        playlist_dict = dict(zip(columns, row))
        playlists.append(playlist_dict)
    
    conn.close()
    return playlists

def update_playlist_category(playlist_id: int, category: str):
    """
    Mettre √† jour la cat√©gorie d'une playlist ET marquer comme classification humaine
    üõ°Ô∏è Cette fonction marque automatiquement la playlist comme √©tant classifi√©e par un humain
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si les colonnes de tracking existent
        cursor.execute("PRAGMA table_info(playlist)")
        columns = [column[1] for column in cursor.fetchall()]
        
        has_tracking_columns = all(col in columns for col in ['classification_source', 'classification_confidence', 'classification_date', 'human_verified'])
        
        if has_tracking_columns:
            # Mise √† jour compl√®te avec tracking humain
            cursor.execute('''
                UPDATE playlist SET 
                    category = ?, 
                    classification_source = 'human',
                    human_verified = 1,
                    classification_date = datetime('now'),
                    classification_confidence = 100,
                    last_updated = ? 
                WHERE id = ?
            ''', (category, datetime.now(), playlist_id))
            print(f"[DB] üõ°Ô∏è PLAYLIST CLASSIFI√âE MANUELLEMENT: ID {playlist_id} ‚Üí {category.upper()} (source: humain)")
        else:
            # Fallback si les colonnes n'existent pas encore
            cursor.execute('''
                UPDATE playlist SET category = ?, last_updated = ? WHERE id = ?
            ''', (category, datetime.now(), playlist_id))
            print(f"[DB] ‚ö†Ô∏è Playlist mise √† jour sans tracking (colonnes manquantes): ID {playlist_id} ‚Üí {category.upper()}")
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] ‚ùå Erreur lors de la mise √† jour de la playlist {playlist_id}: {e}")
        return False
    finally:
        conn.close()

def apply_playlist_categories_to_videos_safe(competitor_id: int, specific_playlist_id: int = None, force_human_playlists: bool = False):
    """
    Appliquer automatiquement les cat√©gories des playlists aux vid√©os
    üö® RESPECTE LES CLASSIFICATIONS HUMAINES - Logique stricte de protection
    
    Args:
        competitor_id: ID du concurrent
        specific_playlist_id: Si sp√©cifi√©, ne traiter que cette playlist
        force_human_playlists: Si True, propage m√™me les playlists classifi√©es manuellement (DANGER!)
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # üö® PROTECTION RENFORC√âE : Exclure les playlists classifi√©es par un humain
        if specific_playlist_id:
            if not force_human_playlists:
                # V√©rifier que la playlist sp√©cifique n'est PAS classifi√©e par un humain
                cursor.execute('''
                    SELECT id, category, name, classification_source, human_verified FROM playlist 
                    WHERE id = ? AND category IS NOT NULL
                    AND (classification_source IS NULL OR classification_source != 'human')
                    AND (human_verified IS NULL OR human_verified != 1)
                ''', (specific_playlist_id,))
            else:
                # Mode force : traiter m√™me les playlists humaines (DANGEREUX - usage explicite seulement)
                cursor.execute('''
                    SELECT id, category, name, classification_source, human_verified FROM playlist 
                    WHERE id = ? AND category IS NOT NULL
                ''', (specific_playlist_id,))
            playlists_to_process = cursor.fetchall()
        else:
            if not force_human_playlists:
                # R√©cup√©rer SEULEMENT les playlists NON classifi√©es par un humain
                cursor.execute('''
                    SELECT id, category, name, classification_source, human_verified FROM playlist 
                    WHERE concurrent_id = ? AND category IS NOT NULL
                    AND (classification_source IS NULL OR classification_source != 'human')
                    AND (human_verified IS NULL OR human_verified != 1)
                ''', (competitor_id,))
            else:
                # Mode force : traiter toutes les playlists (DANGEREUX)
                cursor.execute('''
                    SELECT id, category, name, classification_source, human_verified FROM playlist 
                    WHERE concurrent_id = ? AND category IS NOT NULL
                ''', (competitor_id,))
            playlists_to_process = cursor.fetchall()
        
        videos_updated = 0
        human_protected_count = 0
        human_playlists_skipped = 0
        processed_playlists = []
        
        for playlist_row in playlists_to_process:
            playlist_db_id = playlist_row[0]
            category = playlist_row[1]
            playlist_name = playlist_row[2]
            playlist_source = playlist_row[3]
            playlist_human_verified = playlist_row[4]
            
            # üö® DOUBLE V√âRIFICATION : Ne jamais toucher aux playlists humaines sauf si force
            if not force_human_playlists and (playlist_source == 'human' or playlist_human_verified == 1):
                human_playlists_skipped += 1
                print(f"[DB] üõ°Ô∏è PLAYLIST HUMAINE PROT√âG√âE: '{playlist_name}' ignor√©e (classification humaine)")
                continue
            
            # Mettre √† jour SEULEMENT les vid√©os non prot√©g√©es par supervision humaine
            cursor.execute('''
                UPDATE video SET 
                    category = ?, 
                    classification_source = ?,
                    classification_date = CURRENT_TIMESTAMP,
                    classification_confidence = 80,
                    human_verified = 0,
                    last_updated = ?
                WHERE id IN (
                    SELECT pv.video_id FROM playlist_video pv
                    WHERE pv.playlist_id = ?
                )
                AND (classification_source IS NULL OR classification_source != 'human')
                AND (human_verified IS NULL OR human_verified != 1)
            ''', (category, 'playlist_propagation' if force_human_playlists else 'playlist', datetime.now(), playlist_db_id))
            
            updated_count = cursor.rowcount
            
            # Compter les vid√©os prot√©g√©es pour info
            cursor.execute('''
                SELECT COUNT(*) FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                WHERE pv.playlist_id = ?
                AND (v.classification_source = 'human' OR v.human_verified = 1)
            ''', (playlist_db_id,))
            protected_count = cursor.fetchone()[0]
            
            videos_updated += updated_count
            human_protected_count += protected_count
            
            if updated_count > 0 or protected_count > 0:
                processed_playlists.append({
                    'name': playlist_name,
                    'category': category,
                    'videos_updated': updated_count,
                    'videos_protected': protected_count,
                    'playlist_source': playlist_source
                })
                
                source_label = "[HUMAIN‚ÜíVID√âOS]" if force_human_playlists and playlist_source == 'human' else "[PLAYLIST-IA]"
                
                if updated_count > 0:
                    print(f"[DB] ‚ú® Playlist '{playlist_name}' ‚Üí {category.upper()}: {updated_count} vid√©os taggu√©es {source_label}")
                if protected_count > 0:
                    print(f"[DB] üõ°Ô∏è Playlist '{playlist_name}': {protected_count} vid√©os prot√©g√©es par supervision humaine")
        
        conn.commit()
        
        if human_playlists_skipped > 0:
            print(f"[DB] üõ°Ô∏è {human_playlists_skipped} playlists humaines prot√©g√©es (non propag√©es)")
        
        result_message = f"{videos_updated} vid√©os mises √† jour, {human_protected_count} prot√©g√©es"
        if human_playlists_skipped > 0:
            result_message += f", {human_playlists_skipped} playlists humaines prot√©g√©es"
        
        if specific_playlist_id:
            print(f"[DB] Classification sp√©cifique termin√©e: {result_message}")
        else:
            print(f"[DB] Classification automatique globale termin√©e: {result_message}")
        
        return {
            'videos_updated': videos_updated,
            'human_protected': human_protected_count,
            'human_playlists_skipped': human_playlists_skipped,
            'processed_playlists': processed_playlists,
            'force_mode': force_human_playlists
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] Erreur lors de la classification automatique: {e}")
        raise
    finally:
        conn.close()

# üö® FONCTION SP√âCIALE pour propagation manuelle explicite (playlists humaines)
def force_apply_human_playlist_to_videos(playlist_id: int, user_confirmation: bool = False):
    """
    üö® FONCTION DANGEREUSE : Propager EXPLICITEMENT la cat√©gorie d'une playlist humaine vers ses vid√©os
    Cette fonction ne doit √™tre utilis√©e QUE quand l'utilisateur confirme explicitement
    
    Args:
        playlist_id: ID de la playlist humaine
        user_confirmation: L'utilisateur doit explicitement confirmer cette action
    """
    if not user_confirmation:
        print("[FORCE-APPLY] ‚ùå Confirmation utilisateur requise pour propager une playlist humaine")
        return {'success': False, 'error': 'Confirmation utilisateur requise'}
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier que c'est bien une playlist humaine
        cursor.execute('''
            SELECT category, name, classification_source, human_verified FROM playlist WHERE id = ?
        ''', (playlist_id,))
        playlist_info = cursor.fetchone()
        
        if not playlist_info:
            return {'success': False, 'error': 'Playlist non trouv√©e'}
        
        category, name, source, verified = playlist_info
        
        if source != 'human' and not verified:
            return {'success': False, 'error': 'Cette playlist n\'est pas classifi√©e par un humain'}
        
        # FORCER la propagation vers les vid√©os (en √©crasant m√™me les classifications humaines des vid√©os)
        cursor.execute('''
            UPDATE video SET 
                category = ?, 
                classification_source = 'human_playlist_override',
                classification_date = CURRENT_TIMESTAMP,
                classification_confidence = 100,
                human_verified = 1,
                last_updated = ?
            WHERE id IN (
                SELECT pv.video_id FROM playlist_video pv
                WHERE pv.playlist_id = ?
            )
        ''', (category, datetime.now(), playlist_id))
        
        videos_updated = cursor.rowcount
        conn.commit()
        
        print(f"[FORCE-APPLY] ‚ö†Ô∏è PROPAGATION FORC√âE: Playlist humaine '{name}' ‚Üí {videos_updated} vid√©os forc√©es √† {category.upper()}")
        
        return {
            'success': True,
            'videos_updated': videos_updated,
            'playlist_name': name,
            'category': category,
            'forced_override': True
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[FORCE-APPLY] ‚ùå Erreur: {e}")
        return {'success': False, 'error': str(e)}
    finally:
        conn.close()

# üö® REMPLACER l'ancienne fonction par la nouvelle (avec r√©trocompatibilit√©)
def apply_playlist_categories_to_videos(competitor_id: int, specific_playlist_id: int = None):
    """
    üö® WRAPPER S√âCURIS√â : Utilise la nouvelle logique de protection renforc√©e
    Cette fonction ne propage JAMAIS les playlists classifi√©es par un humain
    """
    return apply_playlist_categories_to_videos_safe(
        competitor_id=competitor_id, 
        specific_playlist_id=specific_playlist_id, 
        force_human_playlists=False  # Toujours False pour la s√©curit√©
    )

def link_playlist_videos(playlist_db_id: int, video_ids: List[str]):
    """Lier des vid√©os √† une playlist"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        for position, video_id in enumerate(video_ids):
            # R√©cup√©rer l'ID de la vid√©o dans notre base
            cursor.execute('SELECT id FROM video WHERE video_id = ?', (video_id,))
            video_row = cursor.fetchone()
            
            if video_row:
                video_db_id = video_row[0]
                
                # Ins√©rer la liaison (ignore si elle existe d√©j√†)
                cursor.execute('''
                    INSERT OR IGNORE INTO playlist_video (playlist_id, video_id, position)
                    VALUES (?, ?, ?)
                ''', (playlist_db_id, video_db_id, position))
        
        conn.commit()
        print(f"[DB] Liaison termin√©e: {len(video_ids)} vid√©os li√©es √† la playlist")
        
    except Exception as e:
        conn.rollback()
        print(f"[DB] Erreur lors de la liaison playlist-vid√©os: {e}")
        raise
    finally:
        conn.close() 

def refresh_competitor_data(channel_url: str, fresh_videos: List[Dict], channel_info: Dict = None) -> Dict:
    """
    Rafra√Æchir intelligemment les donn√©es d'un concurrent existant.
    - Ajoute les nouvelles vid√©os
    - Enrichit les vid√©os existantes en gardant les valeurs maximales (vues, likes, commentaires)
    - Met √† jour les informations de la cha√Æne
    
    Returns:
        Dict avec les statistiques de mise √† jour
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si le concurrent existe
        cursor.execute('SELECT id, name FROM concurrent WHERE channel_url = ?', (channel_url,))
        existing = cursor.fetchone()
        
        if not existing:
            # Si le concurrent n'existe pas, utiliser la fonction normale
            competitor_id = save_competitor_and_videos(channel_url, fresh_videos, channel_info)
            return {
                'success': True,
                'action': 'created',
                'competitor_id': competitor_id,
                'new_videos': len(fresh_videos),
                'updated_videos': 0,
                'enriched_videos': 0
            }
        
        competitor_id = existing[0]
        competitor_name = existing[1]
        print(f"[REFRESH] üîÑ Rafra√Æchissement de {competitor_name} (ID {competitor_id})")
        
        # R√©cup√©rer les vid√©os existantes
        cursor.execute('''
            SELECT video_id, view_count, like_count, comment_count, title, url
            FROM video WHERE concurrent_id = ?
        ''', (competitor_id,))
        existing_videos = {row[0]: {
            'view_count': row[1] or 0,
            'like_count': row[2] or 0, 
            'comment_count': row[3] or 0,
            'title': row[4],
            'url': row[5]
        } for row in cursor.fetchall()}
        
        stats = {
            'new_videos': 0,
            'updated_videos': 0,
            'enriched_videos': 0,
            'total_processed': len(fresh_videos)
        }
        
        # Traiter chaque vid√©o fra√Æche
        for video in fresh_videos:
            video_id = extract_video_id_from_url(video.get('url', ''))
            
            # Pr√©parer les donn√©es de base
            title = video.get('title', '')[:200]
            description = video.get('description', '')
            url = video.get('url', '')
            thumbnail_url = video.get('thumbnail', '')
            
            # Traiter la date de publication
            published_at = None
            if video.get('publication_date'):
                try:
                    date_str = video['publication_date']
                    if 'il y a' in date_str:
                        published_at = datetime.now()
                    else:
                        published_at = datetime.strptime(date_str, '%Y-%m-%d')
                except:
                    published_at = datetime.now()
            
            # Traiter la dur√©e
            duration_seconds = None
            duration_text = video.get('duration', '')
            if duration_text:
                try:
                    parts = duration_text.split(':')
                    if len(parts) == 2:
                        duration_seconds = int(parts[0]) * 60 + int(parts[1])
                    elif len(parts) == 3:
                        duration_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
                except:
                    pass
            
            # Traiter les m√©triques avec valeurs par d√©faut
            fresh_view_count = video.get('views') or video.get('view_count') or 0
            if isinstance(fresh_view_count, str):
                fresh_view_count = int(re.sub(r'[^\d]', '', fresh_view_count)) if re.sub(r'[^\d]', '', fresh_view_count) else 0
            
            fresh_like_count = video.get('likes') or video.get('like_count') or 0
            if isinstance(fresh_like_count, str):
                fresh_like_count = int(re.sub(r'[^\d]', '', fresh_like_count)) if re.sub(r'[^\d]', '', fresh_like_count) else 0
                
            fresh_comment_count = video.get('comments') or video.get('comment_count') or 0
            if isinstance(fresh_comment_count, str):
                fresh_comment_count = int(re.sub(r'[^\d]', '', fresh_comment_count)) if re.sub(r'[^\d]', '', fresh_comment_count) else 0
            
            if video_id in existing_videos:
                # Vid√©o existante : ENRICHIR avec les valeurs maximales
                existing = existing_videos[video_id]
                
                # Prendre les valeurs maximales
                max_views = max(existing['view_count'], fresh_view_count)
                max_likes = max(existing['like_count'], fresh_like_count)
                max_comments = max(existing['comment_count'], fresh_comment_count)
                
                # D√©terminer si on a enrichi les donn√©es
                enriched = (
                    max_views > existing['view_count'] or 
                    max_likes > existing['like_count'] or 
                    max_comments > existing['comment_count']
                )
                
                if enriched:
                    stats['enriched_videos'] += 1
                    print(f"[REFRESH] üìà Enrichi: {title[:50]}... "
                          f"(vues: {existing['view_count']} ‚Üí {max_views}, "
                          f"likes: {existing['like_count']} ‚Üí {max_likes})")
                
                # Mettre √† jour avec les valeurs maximales
                cursor.execute('''
                    UPDATE video SET
                        title = ?, description = ?, thumbnail_url = ?,
                        published_at = ?, duration_seconds = ?, duration_text = ?,
                        view_count = ?, like_count = ?, comment_count = ?,
                        last_updated = ?
                    WHERE concurrent_id = ? AND video_id = ?
                ''', (
                    title, description, thumbnail_url,
                    published_at, duration_seconds, duration_text,
                    max_views, max_likes, max_comments,
                    datetime.now(), competitor_id, video_id
                ))
                
                stats['updated_videos'] += 1
                
            else:
                # Nouvelle vid√©o : AJOUTER
                cursor.execute('''
                    INSERT INTO video (
                        concurrent_id, video_id, title, description, url, thumbnail_url,
                        published_at, duration_seconds, duration_text,
                        view_count, like_count, comment_count,
                        created_at, last_updated
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    competitor_id, video_id, title, description, url, thumbnail_url,
                    published_at, duration_seconds, duration_text,
                    fresh_view_count, fresh_like_count, fresh_comment_count,
                    datetime.now(), datetime.now()
                ))
                
                stats['new_videos'] += 1
                print(f"[REFRESH] ‚ú® Nouvelle: {title[:50]}... ({fresh_view_count} vues)")
        
        # Mettre √† jour les informations de la cha√Æne si disponibles
        if channel_info:
            updates = {
                'thumbnail_url': channel_info.get('thumbnail', ''),
                'banner_url': channel_info.get('banner', ''),
                'description': channel_info.get('description', ''),
                'subscriber_count': channel_info.get('subscriber_count'),
                'view_count': channel_info.get('view_count'),
                'country': channel_info.get('country', ''),
                'language': channel_info.get('language', ''),
                'last_updated': datetime.now()
            }
            
            # Construire la requ√™te de mise √† jour dynamiquement
            set_clauses = []
            values = []
            for key, value in updates.items():
                if value is not None:
                    set_clauses.append(f"{key} = ?")
                    values.append(value)
            
            if set_clauses:
                values.append(competitor_id)
                cursor.execute(f'''
                    UPDATE concurrent SET {', '.join(set_clauses)} WHERE id = ?
                ''', values)
                
                # Enregistrer l'√©volution des statistiques
                try:
                    # Analytics export supprim√© - analyse locale maintenant
                    track_competitor_update(competitor_id, channel_info)
                except Exception as e:
                    print(f"[REFRESH] ‚ö†Ô∏è Erreur lors du tracking: {e}")
        
        # Mettre √† jour le nombre total de vid√©os
        cursor.execute('SELECT COUNT(*) FROM video WHERE concurrent_id = ?', (competitor_id,))
        total_videos = cursor.fetchone()[0]
        
        cursor.execute('''
            UPDATE concurrent SET video_count = ?, last_updated = ? WHERE id = ?
        ''', (total_videos, datetime.now(), competitor_id))
        
        conn.commit()
        
        print(f"[REFRESH] ‚úÖ Rafra√Æchissement termin√© pour {competitor_name}:")
        print(f"  ‚Ä¢ {stats['new_videos']} nouvelles vid√©os")
        print(f"  ‚Ä¢ {stats['updated_videos']} vid√©os mises √† jour")
        print(f"  ‚Ä¢ {stats['enriched_videos']} vid√©os enrichies")
        print(f"  ‚Ä¢ {total_videos} vid√©os au total")
        
        return {
            'success': True,
            'action': 'refreshed',
            'competitor_id': competitor_id,
            'competitor_name': competitor_name,
            'total_videos': total_videos,
            **stats
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[REFRESH] ‚ùå Erreur lors du rafra√Æchissement: {e}")
        return {
            'success': False,
            'error': str(e)
        }
    finally:
        conn.close() 

def get_all_competitors_with_videos() -> List[Dict]:
    """
    R√©cup√©rer tous les concurrents avec leurs vid√©os
    
    Returns:
        List[Dict]: Liste des concurrents avec leurs vid√©os
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # R√©cup√©rer tous les concurrents avec les nouveaux champs
        cursor.execute('''
            SELECT id, name, channel_id, channel_url, thumbnail_url, banner_url,
                   description, subscriber_count, view_count, video_count,
                   country, language, sector, tags, custom_region, notes, is_active,
                   created_at, last_updated
            FROM concurrent
            WHERE is_active = 1
            ORDER BY last_updated DESC
        ''')
        
        competitors = []
        for row in cursor.fetchall():
            competitor = {
                'id': row[0],
                'name': row[1],
                'channel_id': row[2],
                'channel_url': row[3],
                'thumbnail_url': row[4],
                'banner_url': row[5],
                'description': row[6],
                'subscriber_count': row[7],
                'view_count': row[8],
                'video_count': row[9],
                'country': row[10],
                'language': row[11],
                'sector': row[12] or 'hospitality',
                'tags': row[13] or '',
                'custom_region': row[14] or '',
                'notes': row[15] or '',
                'is_active': row[16] if row[16] is not None else 1,
                'created_at': row[17],
                'last_updated': row[18]
            }
            
            # R√©cup√©rer les vid√©os pour ce concurrent
            cursor.execute('''
                SELECT video_id, title, description, url, thumbnail_url,
                       published_at, duration_seconds, duration_text,
                       view_count, like_count, comment_count, category
                FROM video
                WHERE concurrent_id = ?
                ORDER BY published_at DESC
            ''', (competitor['id'],))
            
            videos = []
            for video_row in cursor.fetchall():
                video = {
                    'video_id': video_row[0],
                    'title': video_row[1],
                    'description': video_row[2],
                    'url': video_row[3],
                    'thumbnail_url': video_row[4],
                    'published_at': video_row[5],
                    'duration_seconds': video_row[6],
                    'duration_text': video_row[7],
                    'view_count': video_row[8],
                    'like_count': video_row[9],
                    'comment_count': video_row[10],
                    'category': video_row[11]
                }
                videos.append(video)
            
            competitor['videos'] = videos
            competitors.append(competitor)
        
        return competitors
        
    finally:
        conn.close()

def clean_duplicate_competitors() -> Dict:
    """
    Nettoyer les doublons de concurrents en gardant celui avec le plus de vid√©os.
    Utilise le nom ET l'URL de cha√Æne pour d√©tecter les doublons.
    
    Returns:
        Dict avec les statistiques de nettoyage
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        print("[DB-CLEAN] üßπ D√©but du nettoyage des doublons...")
        
        # R√©cup√©rer tous les concurrents avec leur nombre de vid√©os
        cursor.execute('''
            SELECT c.id, c.name, c.channel_url, c.channel_id, 
                   COUNT(v.id) as video_count,
                   c.created_at, c.last_updated
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            GROUP BY c.id
            ORDER BY c.name, video_count DESC
        ''')
        
        competitors = cursor.fetchall()
        
        if not competitors:
            return {'success': True, 'deleted_count': 0, 'message': 'Aucun concurrent trouv√©'}
        
        # Grouper par nom (insensible √† la casse) et URL similaire
        groups = {}
        for comp in competitors:
            comp_id, name, channel_url, channel_id, video_count, created_at, last_updated = comp
            
            # Cl√© de groupement : nom nettoy√© + channel_id si disponible
            name_clean = name.lower().strip()
            group_key = name_clean
            
            # Si on a un channel_id, l'utiliser comme cl√© plus pr√©cise
            if channel_id:
                group_key = f"{name_clean}_{channel_id}"
            
            if group_key not in groups:
                groups[group_key] = []
            
            groups[group_key].append({
                'id': comp_id,
                'name': name,
                'channel_url': channel_url,
                'channel_id': channel_id,
                'video_count': video_count,
                'created_at': created_at,
                'last_updated': last_updated
            })
        
        # Identifier et supprimer les doublons
        deleted_count = 0
        kept_competitors = []
        
        for group_key, group_competitors in groups.items():
            if len(group_competitors) > 1:
                # Trier par nombre de vid√©os (desc) puis par date de cr√©ation (plus r√©cent)
                group_competitors.sort(key=lambda x: (-x['video_count'], x['last_updated'] or ''), reverse=True)
                
                # Garder le premier (celui avec le plus de vid√©os)
                best = group_competitors[0]
                to_delete = group_competitors[1:]
                
                print(f"[DB-CLEAN] üîç Doublons trouv√©s pour '{best['name']}':")
                print(f"  ‚úÖ GARDE: ID {best['id']} - {best['video_count']} vid√©os - {best['channel_url']}")
                
                # Supprimer les doublons
                for duplicate in to_delete:
                    print(f"  ‚ùå SUPPRIME: ID {duplicate['id']} - {duplicate['video_count']} vid√©os - {duplicate['channel_url']}")
                    
                    # Supprimer le concurrent (les vid√©os sont supprim√©es par CASCADE)
                    cursor.execute('DELETE FROM concurrent WHERE id = ?', (duplicate['id'],))
                    deleted_count += 1
                
                kept_competitors.append(best)
            else:
                # Pas de doublon, garder tel quel
                kept_competitors.append(group_competitors[0])
        
        # Valider les changements
        conn.commit()
        
        print(f"[DB-CLEAN] ‚úÖ Nettoyage termin√©:")
        print(f"  ‚Ä¢ {deleted_count} doublons supprim√©s")
        print(f"  ‚Ä¢ {len(kept_competitors)} concurrents uniques conserv√©s")
        
        # Afficher le r√©sultat final
        for comp in kept_competitors:
            print(f"  üìä {comp['name']}: {comp['video_count']} vid√©os")
        
        return {
            'success': True,
            'deleted_count': deleted_count,
            'kept_count': len(kept_competitors),
            'competitors': kept_competitors,
            'message': f'{deleted_count} doublons supprim√©s, {len(kept_competitors)} concurrents uniques conserv√©s'
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[DB-CLEAN] ‚ùå Erreur lors du nettoyage: {e}")
        return {
            'success': False,
            'error': str(e)
        }
    finally:
        conn.close() 

def get_all_competitors_urls():
    """R√©cup√®re toutes les URLs des concurrents en base de donn√©es"""
    try:
        connection = sqlite3.connect(DB_PATH)
        cursor = connection.cursor()
        
        cursor.execute("SELECT channel_url FROM concurrent")
        urls = [row[0] for row in cursor.fetchall()]
        
        connection.close()
        return urls
        
    except Exception as e:
        print(f"[DATABASE] Erreur lors de la r√©cup√©ration des URLs des concurrents: {e}")
        return [] 

def update_competitor_country(competitor_id, country):
    """Met √† jour le pays d'un concurrent"""
    try:
        connection = sqlite3.connect(DB_PATH)
        cursor = connection.cursor()
        
        cursor.execute("""
            UPDATE concurrent 
            SET country = ?, last_updated = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (country, competitor_id))
        
        connection.commit()
        connection.close()
        
        return True
        
    except Exception as e:
        print(f"[DATABASE] Erreur lors de la mise √† jour du pays: {e}")
        return False

def get_competitors_by_country():
    """R√©cup√®re les concurrents group√©s par pays"""
    try:
        connection = sqlite3.connect(DB_PATH)
        cursor = connection.cursor()
        
        cursor.execute("""
            SELECT country, COUNT(*) as count, 
                   GROUP_CONCAT(name, ', ') as competitors
            FROM concurrent 
            GROUP BY country
            ORDER BY country
        """)
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'country': row[0] if row[0] else 'Non d√©fini',
                'count': row[1],
                'competitors': row[2]
            })
        
        connection.close()
        return results
        
    except Exception as e:
        print(f"[DATABASE] Erreur lors de la r√©cup√©ration par pays: {e}")
        return [] 

def classify_playlist_with_ai(name: str, description: str = "") -> str:
    """
    Classifier automatiquement une playlist en analysant son nom et sa description
    Retourne 'hero', 'hub' ou 'help'
    """
    # Utiliser la nouvelle fonction multilingue avec d√©tection automatique de langue
    category, detected_language, confidence = classify_video_with_language(name, description)
    
    print(f"[PLAYLIST-AI] üéØ '{name}' ‚Üí {category.upper()} ({detected_language}, {confidence}%)")
    
    return category

def auto_classify_uncategorized_playlists(competitor_id: int) -> Dict:
    """
    Classifier automatiquement avec l'IA toutes les playlists non cat√©goris√©es d'un concurrent
    üö® RESPECTE LES CLASSIFICATIONS HUMAINES - Ne touche jamais aux playlists classifi√©es par un humain
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # üö® NOUVEAU : R√©cup√©rer SEULEMENT les playlists non cat√©goris√©es ET non classifi√©es par un humain
        cursor.execute('''
            SELECT id, name, description FROM playlist 
            WHERE concurrent_id = ? 
            AND (category IS NULL OR category = '')
            AND (classification_source IS NULL OR classification_source != 'human')
            AND (human_verified IS NULL OR human_verified != 1)
        ''', (competitor_id,))
        
        uncategorized_playlists = cursor.fetchall()
        
        if not uncategorized_playlists:
            return {
                'success': True,
                'message': 'Aucune playlist non cat√©goris√©e trouv√©e (toutes d√©j√† classifi√©es ou prot√©g√©es par supervision humaine)',
                'classified_count': 0,
                'classifications': []
            }
        
        classifications = []
        classified_count = 0
        human_protected_count = 0
        
        for playlist_row in uncategorized_playlists:
            playlist_id = playlist_row[0]
            name = playlist_row[1] or ''
            description = playlist_row[2] or ''
            
            # üö® DOUBLE V√âRIFICATION : S'assurer qu'on ne touche pas √† une classification humaine
            cursor.execute('''
                SELECT classification_source, human_verified FROM playlist WHERE id = ?
            ''', (playlist_id,))
            protection_check = cursor.fetchone()
            
            if protection_check and (protection_check[0] == 'human' or protection_check[1] == 1):
                human_protected_count += 1
                print(f"[IA-CLASSIFY] üö´ PROTECTION HUMAINE: Playlist ID {playlist_id} ignor√©e (classification humaine)")
                continue
            
            # Classifier avec l'IA
            predicted_category = classify_playlist_with_ai(name, description)
            
            # üö® NOUVEAU : Mettre √† jour en base avec le tracking de source IA
            cursor.execute('''
                UPDATE playlist SET 
                    category = ?, 
                    classification_source = 'ai',
                    classification_date = CURRENT_TIMESTAMP,
                    classification_confidence = 70,
                    human_verified = 0,
                    last_updated = ? 
                WHERE id = ?
            ''', (predicted_category, datetime.now(), playlist_id))
            
            classifications.append({
                'name': name,
                'predicted_category': predicted_category,
                'confidence': 'medium'  # Pour l'instant toujours medium
            })
            
            classified_count += 1
            print(f"[IA-CLASSIFY] ‚ú® {name} ‚Üí {predicted_category.upper() if predicted_category else 'NON_CLASSE'} [IA]")
        
        conn.commit()
        
        if human_protected_count > 0:
            print(f"[IA-CLASSIFY] üõ°Ô∏è {human_protected_count} playlists prot√©g√©es par supervision humaine (non modifi√©es)")
        
        return {
            'success': True,
            'message': f'{classified_count} playlists classifi√©es (IA), {human_protected_count} prot√©g√©es (humain)',
            'classified_count': classified_count,
            'human_protected': human_protected_count,
            'classifications': classifications
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[IA-CLASSIFY] ‚ùå Erreur: {e}")
        return {
            'success': False,
            'error': str(e),
            'classified_count': 0,
            'classifications': []
        }
    finally:
        conn.close()

# --- GESTION DES PATTERNS IA ---

def get_classification_patterns(language: str = None) -> Dict:
    """R√©cup√©rer tous les patterns de classification IA par langue"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si la table patterns existe, sinon la cr√©er avec support multilingue
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS classification_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category VARCHAR(10) NOT NULL,
                pattern VARCHAR(200) NOT NULL,
                language VARCHAR(5) NOT NULL DEFAULT 'fr',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(category, pattern, language)
            )
        ''')
        
        # Ajouter la colonne language si elle n'existe pas (migration)
        try:
            cursor.execute('ALTER TABLE classification_patterns ADD COLUMN language VARCHAR(5) DEFAULT "fr"')
        except:
            pass  # La colonne existe d√©j√†
        
        # R√©cup√©rer les patterns existants
        if language:
            cursor.execute('''
                SELECT category, pattern FROM classification_patterns 
                WHERE language = ?
                ORDER BY category, pattern
            ''', (language,))
        else:
            cursor.execute('''
                SELECT category, pattern, language FROM classification_patterns 
                ORDER BY language, category, pattern
            ''')
        
        if language:
            patterns = {'hero': [], 'hub': [], 'help': []}
            for row in cursor.fetchall():
                category, pattern = row
                if category in patterns:
                    patterns[category].append(pattern)
        else:
            patterns = {'fr': {'hero': [], 'hub': [], 'help': []}, 
                       'en': {'hero': [], 'hub': [], 'help': []},
                       'de': {'hero': [], 'hub': [], 'help': []},
                       'nl': {'hero': [], 'hub': [], 'help': []}}
            for row in cursor.fetchall():
                category, pattern, lang = row
                if lang in patterns and category in patterns[lang]:
                    patterns[lang][category].append(pattern)
        
        # Si aucun pattern n'existe, initialiser avec les patterns par d√©faut
        if language and not any(patterns.values()):
            default_patterns = get_default_classification_patterns(language)
            for category, pattern_list in default_patterns.items():
                for pattern in pattern_list:
                    cursor.execute('''
                        INSERT OR IGNORE INTO classification_patterns (category, pattern, language)
                        VALUES (?, ?, ?)
                    ''', (category, pattern, language))
            conn.commit()
            patterns = default_patterns
        elif not language and not any(any(patterns[lang].values()) for lang in patterns):
            # Initialiser tous les patterns par d√©faut pour toutes les langues
            for lang in ['fr', 'en', 'de', 'nl']:
                default_patterns = get_default_classification_patterns(lang)
                for category, pattern_list in default_patterns.items():
                    for pattern in pattern_list:
                        cursor.execute('''
                            INSERT OR IGNORE INTO classification_patterns (category, pattern, language)
                            VALUES (?, ?, ?)
                        ''', (category, pattern, lang))
            conn.commit()
            patterns = {lang: get_default_classification_patterns(lang) for lang in ['fr', 'en', 'de', 'nl']}
        
        return patterns
        
    except Exception as e:
        print(f"[PATTERNS] Erreur: {e}")
        return get_default_classification_patterns(language) if language else {}
    finally:
        conn.close()

def get_default_classification_patterns(language: str = 'fr') -> Dict:
    """Patterns par d√©faut pour la classification IA selon la langue"""
    
    if language == 'fr':
        return {
            'hero': [
                'meilleur', 'top', 'incroyable', 'exceptionnel', 'extraordinaire', 'fantastique', 'spectaculaire',
                'luxe', 'premium', 'exclusif', 'unique', 'rare', 'privil√©gi√©', '√©lite', 'prestige', 'prestigieux',
                'destination', 'voyage', 'aventure', '√©pique', 'l√©gendaire', 'mythique', 'iconique',
                'exp√©rience', 'd√©couverte', 'exploration', 'r√©v√©lation', 'surprise', '√©merveillement',
                'must', 'essentiels', 'incontournables', 'coup de coeur', 'inoubliable', 'm√©morable',
                'recommandation', 'favori', 'pr√©f√©r√©', 's√©lection', 'choix', '√©lu',
                'inspiration', 'r√™ve', 'fantasme', 'id√©al', 'paradis', 'magie', 'magique',
                'ultime', 'supr√™me', 'classe mondiale', 'niveau international', 'r√©f√©rence',
                'remarquable', 'impressionnant', 'saisissant', '√©blouissant', 'splendide',
                '√† couper le souffle', 'magnifique', 'sublime', 'grandiose', 'majestueux',
                'r√©volutionnaire', 'innovant', 'avant-gardiste', 'pionnier', 'pr√©curseur',
                'ph√©nom√®ne', 'sensation', 'buzz', 'viral', 'tendance', 'mode',
                'exclusivit√©', 'premi√®re', 'lancement', 'r√©v√©lation', 'nouveaut√©',
                'chef-d\'≈ìuvre', 'perfection', 'excellence', 'qualit√© sup√©rieure', 'haut de gamme',
                'passion', '√©motion', 'frisson', 'adr√©naline', 'intensit√©', 'puissance',
                'beaut√©', 'art', 'esth√©tique', 'raffinement', '√©l√©gance', 'sophistication',
                'c√©l√®bre', 'renomm√©', 'r√©put√©', 'illustre', 'notoire', 'populaire',
                'r√©compens√©', 'prim√©', 'm√©daill√©', 'laur√©at', 'gagnant', 'champion',
                'secret', 'myst√®re', 'cach√©', 'confidentiel', 'priv√©', 'r√©serv√©',
                'authentique', 'v√©ritable', 'original', 'naturel', 'brut', 'pur'
            ],
            'hub': [
                'comment', 'tutoriel', 'guide', 'conseils', 'astuces', 'techniques', 'm√©thodes',
                'planifier', 'organiser', 'pr√©parer', 'programmer', 'structurer', 'ordonnancer',
                'pratique', 'utile', 'quotidien', 'r√©gulier', 'habituel', 'courant',
                'g√©n√©ral', 'aper√ßu', 'base', 'introduction', 'pr√©sentation', 'survol',
                'information', 'infos', 'faits', 'd√©tails', 'donn√©es', '√©l√©ments',
                's√©rie', 'collection', 'compilation', 's√©lection', 'ensemble', 'groupe',
                'programme', 'agenda', 'calendrier', 'planning', 'emploi du temps',
                'liste', '√©tapes', 'processus', 'proc√©dure', 'marche √† suivre',
                'm√©thode', 'technique', 'approche', 'strat√©gie', 'tactique', 'syst√®me',
                'revue', 'analyse', '√©tude', 'examen', '√©valuation', 'comparaison',
                'hebdomadaire', 'mensuel', 'quotidien', 'journalier', 'p√©riodique',
                'standard', 'normal', 'typique', 'habituel', 'ordinaire', 'classique',
                'formation', 'apprentissage', 'enseignement', '√©ducation', 'instruction',
                'r√©cap', 'r√©sum√©', 'synth√®se', 'bilan', 'compte-rendu', 'rapport',
                'mise √† jour', 'actualit√©', 'nouvelles', 'informations', 'bulletin',
                'd√©couverte', 'exploration', 'visite', 'tour', 'promenade', 'parcours',
                'focus', 'zoom', 'gros plan', 'd√©tail', 'pr√©cision', 'sp√©cificit√©',
                'th√®me', 'sujet', 'topic', 'question', 'probl√©matique', 'enjeu',
                'cat√©gorie', 'type', 'genre', 'style', 'vari√©t√©', 'gamme',
                'routine', 'habitude', 'coutume', 'tradition', 'usage', 'pratique'
            ],
            'help': [
                'aide', 'support', 'assistance', 'secours', 'soutien', 'appui',
                'probl√®me', 'souci', 'difficult√©', 'obstacle', 'complication', 'ennui',
                'question', 'interrogation', 'demande', 'requ√™te', 'sollicitation',
                'fr√©quent', 'commun', 'courant', 'habituel', 'r√©current', 'typique',
                'r√©soudre', 'r√©parer', 'corriger', 'arranger', 'r√©gler', 'solutionner',
                'd√©pannage', 'r√©paration', 'maintenance', 'entretien', 'service',
                '√©tape par √©tape', 'pas √† pas', 'progressif', 'graduel', 's√©quentiel',
                'd√©butant', 'novice', 'apprenti', 'initi√©', 'n√©ophyte', 'amateur',
                'apprendre', '√©tudier', 'ma√Ætriser', 'acqu√©rir', 'd√©velopper', 'comprendre',
                'expliquer', 'clarifier', 'd√©tailler', 'pr√©ciser', 'expliciter', 'd√©montrer',
                '√©viter', 'pr√©venir', 'emp√™cher', '√©carter', 'contourner', 'esquiver',
                'erreur', 'faute', 'mistake', 'b√©vue', 'maladresse', 'impair',
                'attention', 'prudence', 'vigilance', 'pr√©caution', 'soin', 'circonspection',
                'le√ßon', 'cours', 'formation', 'stage', 'atelier', 's√©minaire',
                'solution', 'r√©solution', 'r√©ponse', 'rem√®de', 'traitement', 'th√©rapie',
                'conseil', 'recommandation', 'suggestion', 'proposition', 'indication',
                'guidance', 'orientation', 'direction', 'pilotage', 'accompagnement',
                'consultation', 'expertise', 'sp√©cialisation', 'professionnalisme',
                'diagnostic', 'analyse', '√©valuation', 'examen', 'inspection',
                'urgence', 'rapidit√©', 'vitesse', 'imm√©diat', 'instant', 'express'
            ]
        }
    
    elif language == 'en':
        return {
            'hero': [
                'best', 'top', 'incredible', 'amazing', 'spectacular', 'fantastic', 'extraordinary',
                'luxury', 'premium', 'exclusive', 'unique', 'rare', 'privileged', 'elite', 'prestige', 'prestigious',
                'destination', 'travel', 'adventure', 'epic', 'legendary', 'mythical', 'iconic',
                'experience', 'discovery', 'exploration', 'revelation', 'surprise', 'wonder',
                'must', 'essential', 'must-have', 'must-see', 'unforgettable', 'memorable',
                'recommendation', 'favorite', 'preferred', 'selection', 'choice', 'pick',
                'inspiration', 'dream', 'fantasy', 'ideal', 'paradise', 'magic', 'magical',
                'ultimate', 'supreme', 'world-class', 'international', 'reference',
                'remarkable', 'impressive', 'striking', 'stunning', 'splendid',
                'breathtaking', 'magnificent', 'sublime', 'grandiose', 'majestic',
                'revolutionary', 'innovative', 'cutting-edge', 'pioneering', 'groundbreaking',
                'phenomenon', 'sensation', 'buzz', 'viral', 'trending', 'hot',
                'exclusive', 'premiere', 'launch', 'revelation', 'novelty',
                'masterpiece', 'perfection', 'excellence', 'superior quality', 'high-end',
                'passion', 'emotion', 'thrill', 'adrenaline', 'intensity', 'power',
                'beauty', 'art', 'aesthetic', 'refinement', 'elegance', 'sophistication',
                'famous', 'renowned', 'reputed', 'illustrious', 'notorious', 'popular',
                'awarded', 'prize-winning', 'medal-winning', 'winner', 'champion',
                'secret', 'mystery', 'hidden', 'confidential', 'private', 'reserved',
                'authentic', 'genuine', 'original', 'natural', 'raw', 'pure'
            ],
            'hub': [
                'how to', 'tutorial', 'guide', 'tips', 'tricks', 'techniques', 'methods',
                'planning', 'organize', 'prepare', 'schedule', 'structure', 'arrange',
                'practical', 'useful', 'everyday', 'regular', 'routine', 'common',
                'general', 'overview', 'basics', 'introduction', 'presentation', 'survey',
                'information', 'info', 'facts', 'details', 'data', 'elements',
                'series', 'collection', 'compilation', 'selection', 'set', 'group',
                'program', 'agenda', 'calendar', 'planning', 'schedule', 'timeline',
                'list', 'steps', 'process', 'procedure', 'workflow', 'protocol',
                'method', 'technique', 'approach', 'strategy', 'tactic', 'system',
                'review', 'analysis', 'study', 'examination', 'evaluation', 'comparison',
                'weekly', 'monthly', 'daily', 'regular', 'periodic', 'recurring',
                'standard', 'normal', 'typical', 'usual', 'ordinary', 'classic',
                'training', 'learning', 'teaching', 'education', 'instruction',
                'recap', 'summary', 'synthesis', 'overview', 'report', 'briefing',
                'update', 'news', 'updates', 'information', 'bulletin', 'announcement',
                'discovery', 'exploration', 'visit', 'tour', 'walk', 'journey',
                'focus', 'zoom', 'close-up', 'detail', 'precision', 'specificity',
                'theme', 'subject', 'topic', 'question', 'issue', 'matter',
                'category', 'type', 'genre', 'style', 'variety', 'range',
                'routine', 'habit', 'custom', 'tradition', 'usage', 'practice'
            ],
            'help': [
                'help', 'support', 'assistance', 'aid', 'backup', 'relief',
                'problem', 'issue', 'difficulty', 'obstacle', 'complication', 'trouble',
                'question', 'inquiry', 'request', 'query', 'ask', 'need',
                'frequent', 'common', 'usual', 'typical', 'recurring', 'regular',
                'solve', 'fix', 'repair', 'correct', 'resolve', 'address',
                'troubleshooting', 'debugging', 'maintenance', 'service', 'repair',
                'step by step', 'step-by-step', 'progressive', 'gradual', 'sequential',
                'beginner', 'novice', 'starter', 'newbie', 'learner', 'amateur',
                'learn', 'study', 'master', 'acquire', 'develop', 'understand',
                'explain', 'clarify', 'detail', 'specify', 'demonstrate', 'show',
                'avoid', 'prevent', 'stop', 'bypass', 'sidestep', 'dodge',
                'error', 'mistake', 'fault', 'bug', 'glitch', 'failure',
                'warning', 'caution', 'alert', 'notice', 'attention', 'careful',
                'lesson', 'course', 'training', 'workshop', 'seminar', 'class',
                'solution', 'resolution', 'answer', 'remedy', 'treatment', 'cure',
                'advice', 'recommendation', 'suggestion', 'tip', 'guidance', 'counsel',
                'guidance', 'direction', 'orientation', 'navigation', 'coaching',
                'consultation', 'expertise', 'specialization', 'professional',
                'diagnosis', 'analysis', 'evaluation', 'assessment', 'inspection',
                'emergency', 'urgent', 'quick', 'fast', 'immediate', 'instant'
            ]
        }
    
    elif language == 'de':
        return {
            'hero': [
                'beste', 'top', 'unglaublich', 'erstaunlich', 'spektakul√§r', 'fantastisch', 'au√üergew√∂hnlich',
                'luxus', 'premium', 'exklusiv', 'einzigartig', 'selten', 'privilegiert', 'elite', 'prestige',
                'reiseziel', 'reise', 'abenteuer', 'episch', 'legend√§r', 'mythisch', 'ikonisch',
                'erlebnis', 'entdeckung', 'erkundung', 'enth√ºllung', '√ºberraschung', 'wunder',
                'muss', 'wesentlich', 'unvergesslich', 'denkw√ºrdig', 'besonders',
                'empfehlung', 'favorit', 'bevorzugt', 'auswahl', 'wahl', 'pick',
                'inspiration', 'traum', 'fantasie', 'ideal', 'paradies', 'magie', 'magisch',
                'ultimativ', 'h√∂chste', 'weltklasse', 'international', 'referenz',
                'bemerkenswert', 'beeindruckend', 'auff√§llig', 'atemberaubend', 'herrlich',
                'atemberaubend', 'pr√§chtig', 'erhaben', 'grandios', 'majest√§tisch',
                'revolution√§r', 'innovativ', 'bahnbrechend', 'wegweisend', 'pionier',
                'ph√§nomen', 'sensation', 'trend', 'viral', 'angesagt', 'hei√ü',
                'exklusiv', 'premiere', 'einf√ºhrung', 'enth√ºllung', 'neuheit',
                'meisterwerk', 'perfektion', 'exzellenz', '√ºberlegene qualit√§t', 'hochwertig',
                'leidenschaft', 'emotion', 'nervenkitzel', 'adrenalin', 'intensit√§t', 'kraft',
                'sch√∂nheit', 'kunst', '√§sthetik', 'verfeinerung', 'eleganz', 'raffinesse',
                'ber√ºhmt', 'renommiert', 'angesehen', 'illustriert', 'bekannt', 'popul√§r',
                'ausgezeichnet', 'preisgekr√∂nt', 'medaillengewinner', 'gewinner', 'champion',
                'geheim', 'mysterium', 'versteckt', 'vertraulich', 'privat', 'reserviert',
                'authentisch', 'echt', 'original', 'nat√ºrlich', 'roh', 'rein'
            ],
            'hub': [
                'wie man', 'tutorial', 'anleitung', 'tipps', 'tricks', 'techniken', 'methoden',
                'planung', 'organisieren', 'vorbereiten', 'planen', 'strukturieren', 'arrangieren',
                'praktisch', 'n√ºtzlich', 'allt√§glich', 'regelm√§√üig', 'routine', '√ºblich',
                'allgemein', '√ºberblick', 'grundlagen', 'einf√ºhrung', 'pr√§sentation', 'umfrage',
                'information', 'info', 'fakten', 'details', 'daten', 'elemente',
                'serie', 'sammlung', 'zusammenstellung', 'auswahl', 'set', 'gruppe',
                'programm', 'agenda', 'kalender', 'planung', 'zeitplan', 'timeline',
                'liste', 'schritte', 'prozess', 'verfahren', 'arbeitsablauf', 'protokoll',
                'methode', 'technik', 'ansatz', 'strategie', 'taktik', 'system',
                'bewertung', 'analyse', 'studie', 'pr√ºfung', 'bewertung', 'vergleich',
                'w√∂chentlich', 'monatlich', 't√§glich', 'regelm√§√üig', 'periodisch', 'wiederkehrend',
                'standard', 'normal', 'typisch', '√ºblich', 'gew√∂hnlich', 'klassisch',
                'training', 'lernen', 'lehren', 'bildung', 'unterricht', 'ausbildung',
                'zusammenfassung', '√ºbersicht', 'synthese', 'bericht', 'briefing',
                'update', 'nachrichten', 'updates', 'information', 'bulletin', 'ank√ºndigung',
                'entdeckung', 'erkundung', 'besuch', 'tour', 'spaziergang', 'reise',
                'fokus', 'zoom', 'nahaufnahme', 'detail', 'pr√§zision', 'spezifit√§t',
                'thema', 'gegenstand', 'topic', 'frage', 'problem', 'angelegenheit',
                'kategorie', 'typ', 'genre', 'stil', 'vielfalt', 'bereich',
                'routine', 'gewohnheit', 'brauch', 'tradition', 'verwendung', 'praxis'
            ],
            'help': [
                'hilfe', 'unterst√ºtzung', 'assistenz', 'hilfe', 'backup', 'entlastung',
                'problem', 'issue', 'schwierigkeit', 'hindernis', 'komplikation', '√§rger',
                'frage', 'anfrage', 'bitte', 'abfrage', 'fragen', 'brauchen',
                'h√§ufig', '√ºblich', 'gew√∂hnlich', 'typisch', 'wiederkehrend', 'regelm√§√üig',
                'l√∂sen', 'reparieren', 'beheben', 'korrigieren', 'aufl√∂sen', 'adressieren',
                'fehlerbehebung', 'debugging', 'wartung', 'service', 'reparatur',
                'schritt f√ºr schritt', 'schrittweise', 'progressiv', 'allm√§hlich', 'sequentiell',
                'anf√§nger', 'neuling', 'starter', 'newbie', 'lernender', 'amateur',
                'lernen', 'studieren', 'meistern', 'erwerben', 'entwickeln', 'verstehen',
                'erkl√§ren', 'kl√§ren', 'detaillieren', 'spezifizieren', 'demonstrieren', 'zeigen',
                'vermeiden', 'verhindern', 'stoppen', 'umgehen', 'ausweichen', 'dodge',
                'fehler', 'mistake', 'schuld', 'bug', 'glitch', 'ausfall',
                'warnung', 'vorsicht', 'alarm', 'hinweis', 'achtung', 'vorsichtig',
                'lektion', 'kurs', 'training', 'workshop', 'seminar', 'klasse',
                'l√∂sung', 'aufl√∂sung', 'antwort', 'heilmittel', 'behandlung', 'heilung',
                'rat', 'empfehlung', 'vorschlag', 'tipp', 'anleitung', 'beratung',
                'f√ºhrung', 'richtung', 'orientierung', 'navigation', 'coaching',
                'beratung', 'expertise', 'spezialisierung', 'professionell',
                'diagnose', 'analyse', 'bewertung', 'beurteilung', 'inspektion',
                'notfall', 'dringend', 'schnell', 'schnell', 'sofort', 'instant'
            ]
        }
    
    elif language == 'nl':
        return {
            'hero': [
                'beste', 'top', 'ongelofelijk', 'verbazingwekkend', 'spectaculair', 'fantastisch', 'buitengewoon',
                'luxe', 'premium', 'exclusief', 'uniek', 'zeldzaam', 'bevoorrecht', 'elite', 'prestige',
                'bestemming', 'reis', 'avontuur', 'episch', 'legendarisch', 'mythisch', 'iconisch',
                'ervaring', 'ontdekking', 'verkenning', 'onthulling', 'verrassing', 'wonder',
                'moet', 'essentieel', 'onvergetelijk', 'gedenkwaardig', 'bijzonder',
                'aanbeveling', 'favoriet', 'voorkeur', 'selectie', 'keuze', 'pick',
                'inspiratie', 'droom', 'fantasie', 'ideaal', 'paradijs', 'magie', 'magisch',
                'ultieme', 'supreme', 'wereldklasse', 'internationaal', 'referentie',
                'opmerkelijk', 'indrukwekkend', 'opvallend', 'adembenemend', 'prachtig',
                'adembenemend', 'magnifiek', 'subliem', 'grandioos', 'majestueus',
                'revolutionair', 'innovatief', 'baanbrekend', 'pionierswerk', 'grondlegend',
                'fenomeen', 'sensatie', 'trend', 'viral', 'trending', 'hot',
                'exclusief', 'premiere', 'lancering', 'onthulling', 'nieuwigheid',
                'meesterwerk', 'perfectie', 'excellentie', 'superieure kwaliteit', 'hoogwaardig',
                'passie', 'emotie', 'thrill', 'adrenaline', 'intensiteit', 'kracht',
                'schoonheid', 'kunst', 'esthetiek', 'verfijning', 'elegantie', 'verfijning',
                'beroemd', 'gerenommeerd', 'gerespecteerd', 'illustere', 'bekend', 'populair',
                'bekroond', 'prijswinnend', 'medaillewinnaar', 'winnaar', 'kampioen',
                'geheim', 'mysterie', 'verborgen', 'vertrouwelijk', 'priv√©', 'gereserveerd',
                'authentiek', 'echt', 'origineel', 'natuurlijk', 'rauw', 'puur'
            ],
            'hub': [
                'hoe te', 'tutorial', 'gids', 'tips', 'trucs', 'technieken', 'methoden',
                'planning', 'organiseren', 'voorbereiden', 'plannen', 'structureren', 'regelen',
                'praktisch', 'nuttig', 'dagelijks', 'regelmatig', 'routine', 'gewoon',
                'algemeen', 'overzicht', 'basics', 'introductie', 'presentatie', 'enqu√™te',
                'informatie', 'info', 'feiten', 'details', 'gegevens', 'elementen',
                'serie', 'verzameling', 'compilatie', 'selectie', 'set', 'groep',
                'programma', 'agenda', 'kalender', 'planning', 'schema', 'tijdlijn',
                'lijst', 'stappen', 'proces', 'procedure', 'workflow', 'protocol',
                'methode', 'techniek', 'aanpak', 'strategie', 'tactiek', 'systeem',
                'review', 'analyse', 'studie', 'onderzoek', 'evaluatie', 'vergelijking',
                'wekelijks', 'maandelijks', 'dagelijks', 'regelmatig', 'periodiek', 'terugkerend',
                'standaard', 'normaal', 'typisch', 'gebruikelijk', 'gewoon', 'klassiek',
                'training', 'leren', 'onderwijzen', 'educatie', 'instructie', 'opleiding',
                'samenvatting', 'overzicht', 'synthese', 'rapport', 'briefing',
                'update', 'nieuws', 'updates', 'informatie', 'bulletin', 'aankondiging',
                'ontdekking', 'verkenning', 'bezoek', 'tour', 'wandeling', 'reis',
                'focus', 'zoom', 'close-up', 'detail', 'precisie', 'specificiteit',
                'thema', 'onderwerp', 'topic', 'vraag', 'probleem', 'kwestie',
                'categorie', 'type', 'genre', 'stijl', 'vari√´teit', 'bereik',
                'routine', 'gewoonte', 'gebruik', 'traditie', 'gebruik', 'praktijk'
            ],
            'help': [
                'hulp', 'ondersteuning', 'assistentie', 'hulp', 'backup', 'verlichting',
                'probleem', 'issue', 'moeilijkheid', 'obstakel', 'complicatie', 'problemen',
                'vraag', 'vraag', 'verzoek', 'query', 'vragen', 'nodig',
                'frequent', 'gewoon', 'gebruikelijk', 'typisch', 'terugkerend', 'regelmatig',
                'oplossen', 'repareren', 'herstellen', 'corrigeren', 'oplossen', 'adresseren',
                'probleemoplossing', 'debugging', 'onderhoud', 'service', 'reparatie',
                'stap voor stap', 'stapsgewijs', 'progressief', 'geleidelijk', 'sequentieel',
                'beginner', 'novice', 'starter', 'newbie', 'leerling', 'amateur',
                'leren', 'studeren', 'beheersen', 'verkrijgen', 'ontwikkelen', 'begrijpen',
                'uitleggen', 'verduidelijken', 'detailleren', 'specificeren', 'demonstreren', 'tonen',
                'vermijden', 'voorkomen', 'stoppen', 'omzeilen', 'ontwijken', 'dodge',
                'fout', 'mistake', 'schuld', 'bug', 'glitch', 'falen',
                'waarschuwing', 'voorzichtigheid', 'alarm', 'kennisgeving', 'aandacht', 'voorzichtig',
                'les', 'cursus', 'training', 'workshop', 'seminar', 'klas',
                'oplossing', 'resolutie', 'antwoord', 'remedie', 'behandeling', 'genezing',
                'advies', 'aanbeveling', 'suggestie', 'tip', 'begeleiding', 'advies',
                'begeleiding', 'richting', 'ori√´ntatie', 'navigatie', 'coaching',
                'consultatie', 'expertise', 'specialisatie', 'professioneel',
                'diagnose', 'analyse', 'evaluatie', 'beoordeling', 'inspectie',
                'noodgeval', 'urgent', 'snel', 'schnell', 'sofort', 'instant'
            ]
        }
    
    # Retour par d√©faut si langue non support√©e
    return get_default_classification_patterns('fr')

def add_classification_pattern(category: str, pattern: str, language: str = 'fr') -> bool:
    """Ajouter un nouveau pattern de classification pour une langue donn√©e"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            INSERT OR IGNORE INTO classification_patterns (category, pattern, language)
            VALUES (?, ?, ?)
        ''', (category, pattern.lower().strip(), language))
        
        conn.commit()
        return cursor.rowcount > 0
        
    except Exception as e:
        print(f"[PATTERNS] Erreur ajout: {e}")
        return False
    finally:
        conn.close()

def remove_classification_pattern(category: str, pattern: str, language: str = 'fr') -> bool:
    """Supprimer un pattern de classification pour une langue donn√©e"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            DELETE FROM classification_patterns 
            WHERE category = ? AND pattern = ? AND language = ?
        ''', (category, pattern.lower().strip(), language))
        
        conn.commit()
        return cursor.rowcount > 0
        
    except Exception as e:
        print(f"[PATTERNS] Erreur suppression: {e}")
        return False
    finally:
        conn.close()

def detect_language(text: str) -> str:
    """D√©tecter la langue d'un texte (simple heuristique)"""
    text_lower = text.lower()
    
    # Mots indicateurs par langue
    french_indicators = ['le', 'la', 'les', 'de', 'du', 'des', 'et', 'ou', 'avec', 'dans', 'pour', 'sur', 'comment', 'pourquoi', 'que', 'qui']
    english_indicators = ['the', 'and', 'or', 'with', 'in', 'for', 'on', 'how', 'why', 'what', 'who', 'where', 'when']
    german_indicators = ['der', 'die', 'das', 'und', 'oder', 'mit', 'in', 'f√ºr', 'auf', 'wie', 'warum', 'was', 'wer', 'wo', 'wann', 'ein', 'eine']
    dutch_indicators = ['de', 'het', 'en', 'of', 'met', 'in', 'voor', 'op', 'hoe', 'waarom', 'wat', 'wie', 'waar', 'wanneer', 'een']
    
    # Compter les occurrences
    french_score = sum(1 for word in french_indicators if f' {word} ' in f' {text_lower} ')
    english_score = sum(1 for word in english_indicators if f' {word} ' in f' {text_lower} ')
    german_score = sum(1 for word in german_indicators if f' {word} ' in f' {text_lower} ')
    dutch_score = sum(1 for word in dutch_indicators if f' {word} ' in f' {text_lower} ')
    
    # Retourner la langue avec le score le plus √©lev√©
    scores = {
        'fr': french_score,
        'en': english_score,
        'de': german_score,
        'nl': dutch_score
    }
    
    max_lang = max(scores, key=scores.get)
    return max_lang if scores[max_lang] > 0 else 'fr'  # D√©faut fran√ßais

def normalize_pattern(pattern):
    # Transforme un pattern en regex souple : espaces/tirets/points interchangeables
    # Ex: 'how to' => r'h[o0]w[\s\-\._]*to'
    # On remplace chaque espace par [\s\-\._]*
    parts = re.split(r'\s+', pattern.strip())
    regex = r'[\s\-\._]*'.join(map(re.escape, parts))
    return regex

def classify_video_with_language(title: str, description: str = "") -> Tuple[str, str, int]:
    """
    Classifier une vid√©o en d√©tectant automatiquement la langue
    Retourne (cat√©gorie, langue, confiance)
    """
    # D√©tecter la langue
    full_text = f"{title} {description}"
    detected_language = detect_language(full_text)
    text_lower = full_text.lower()

    # 1. Appliquer les custom rules (priorit√©)
    custom_rules = list_custom_rules(detected_language)
    for rule in custom_rules:
        pattern = rule['pattern']
        regex = normalize_pattern(pattern)
        if re.search(regex, text_lower, re.IGNORECASE):
            return rule['category'], detected_language, 100

    # 2. R√©cup√©rer les patterns standards pour cette langue
    patterns = get_classification_patterns(detected_language)

    # 3. Matching souple pour tous les patterns
    def calculate_weighted_score(text: str, pattern_list: list) -> float:
        score = 0.0
        for pattern in pattern_list:
            regex = normalize_pattern(pattern)
            if re.search(regex, text, re.IGNORECASE):
                words_count = len(pattern.split())
                if words_count >= 3:
                    weight = 3.0
                elif words_count == 2:
                    weight = 2.0
                else:
                    weight = 1.0
                # Bonus si le pattern est en d√©but de titre (apr√®s nettoyage)
                title_norm = re.sub(r'[\-\._]+', ' ', title.lower()).strip()
                pattern_norm = pattern.lower().replace('-', ' ').replace('.', ' ').strip()
                if title_norm.startswith(pattern_norm):
                    weight *= 1.5
                score += weight
        return score

    hero_score = calculate_weighted_score(text_lower, patterns['hero'])
    hub_score = calculate_weighted_score(text_lower, patterns['hub'])
    help_score = calculate_weighted_score(text_lower, patterns['help'])

    scores = {'hero': hero_score, 'hub': hub_score, 'help': help_score}
    max_category = max(scores, key=scores.get)
    max_score = scores[max_category]
    total_score = sum(scores.values())
    confidence = int((max_score / total_score * 100) if total_score > 0 else 50)
    if max_score == 0:
        return 'hub', detected_language, 50
    return max_category, detected_language, confidence

def run_global_ai_classification() -> Dict:
    """
    Lance la classification IA globale sur tous les concurrents
    Classifie les playlists puis applique aux vid√©os, puis classifie directement les vid√©os restantes
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # R√©cup√©rer tous les concurrents
        cursor.execute('SELECT id, name FROM concurrent ORDER BY name')
        competitors = cursor.fetchall()
        
        if not competitors:
            return {
                'success': True,
                'message': 'Aucun concurrent trouv√©',
                'competitors_count': 0,
                'playlists_classified': 0,
                'videos_classified': 0
            }
        
        total_playlists_classified = 0
        total_videos_classified = 0
        competitors_with_playlists = 0
        
        for competitor_row in competitors:
            competitor_id = competitor_row[0]
            competitor_name = competitor_row[1]
            
            print(f"[GLOBAL-AI] üéØ Traitement de {competitor_name}...")
            
            # 1. Classifier les playlists non cat√©goris√©es
            playlist_result = auto_classify_uncategorized_playlists(competitor_id)
            if playlist_result.get('classified_count', 0) > 0:
                total_playlists_classified += playlist_result['classified_count']
                competitors_with_playlists += 1
                print(f"[GLOBAL-AI] ‚ú® {competitor_name}: {playlist_result['classified_count']} playlists classifi√©es")
            
            # 2. Appliquer les cat√©gories des playlists aux vid√©os
            video_result = apply_playlist_categories_to_videos(competitor_id)
            if video_result.get('videos_updated', 0) > 0:
                total_videos_classified += video_result['videos_updated']
                print(f"[GLOBAL-AI] üìπ {competitor_name}: {video_result['videos_updated']} vid√©os classifi√©es via playlists")
            
            # 3. Classifier directement les vid√©os restantes non cat√©goris√©es
            direct_result = classify_videos_directly_with_keywords(competitor_id)
            if direct_result.get('videos_classified', 0) > 0:
                total_videos_classified += direct_result['videos_classified']
                print(f"[GLOBAL-AI] üé¨ {competitor_name}: {direct_result['videos_classified']} vid√©os classifi√©es directement")
        
        conn.commit()
        
        # Enregistrer la date de derni√®re classification globale
        from yt_channel_analyzer.database import set_last_action_status
        set_last_action_status('last_playlist_classification', {
            'message': f'Classification globale termin√©e: {len(competitors)} concurrents trait√©s',
            'timestamp': datetime.now().isoformat()
        })
        return {
            'success': True,
            'competitors_count': len(competitors),
            'competitors_with_playlists': competitors_with_playlists,
            'playlists_classified': total_playlists_classified,
            'videos_classified': total_videos_classified,
            'message': f'Classification globale termin√©e: {len(competitors)} concurrents trait√©s'
        }
        
    except Exception as e:
        print(f"[GLOBAL-AI] ‚ùå Erreur: {e}")
        return {
            'success': False,
            'error': str(e),
            'competitors_count': 0,
            'playlists_classified': 0,
            'videos_classified': 0
        }
    finally:
        conn.close() 

def reclassify_all_videos_with_multilingual_logic() -> Dict:
    """
    Re-classifier toutes les vid√©os existantes avec la nouvelle logique multilingue
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    import json
    try:
        print("[RECLASSIFY-MULTILINGUAL] üåç D√©but de la re-classification globale multilingue")
        
        # R√©cup√©rer toutes les vid√©os
        cursor.execute('SELECT id, title, description, category FROM video')
        videos = cursor.fetchall()
        
        if not videos:
            return {
                'success': False,
                'error': 'Aucune vid√©o trouv√©e'
            }
        
        print(f"[RECLASSIFY-MULTILINGUAL] üì∫ {len(videos)} vid√©os √† re-classifier")
        
        videos_updated = 0
        videos_unchanged = 0
        languages_detected = set()
        category_changes = {'hero': 0, 'hub': 0, 'help': 0}
        
        # Traiter par lots pour √©viter le verrouillage de la base
        batch_size = 50
        updates_batch = []
        
        for video_id, title, description, current_category in videos:
            try:
                # Utiliser la nouvelle logique multilingue
                new_category, detected_language, confidence = classify_video_with_language(
                    title, description or ''
                )
                
                languages_detected.add(detected_language)
                
                # V√©rifier si la cat√©gorie a chang√©
                if new_category != current_category:
                    # Ajouter √† la liste des mises √† jour
                    updates_batch.append((new_category, datetime.now(), video_id))
                    videos_updated += 1
                    category_changes[new_category] += 1
                    
                    # Log d√©taill√© pour changements significatifs
                    if current_category and current_category != new_category:
                        print(f"[RECLASSIFY-MULTILINGUAL] üîÑ '{title[:50]}...' : {current_category.upper() if current_category else 'NULL'} ‚Üí {new_category.upper()} ({detected_language}, {confidence}%)")
                else:
                    videos_unchanged += 1
                
                # Commit par lots pour √©viter le verrouillage
                if len(updates_batch) >= batch_size:
                    cursor.executemany('''
                        UPDATE video 
                        SET category = ?, last_updated = ?
                        WHERE id = ?
                    ''', updates_batch)
                    conn.commit()
                    updates_batch = []
                
                # Log p√©riodique
                if (videos_updated + videos_unchanged) % 100 == 0:
                    processed = videos_updated + videos_unchanged
                    print(f"[RECLASSIFY-MULTILINGUAL] üìä {processed}/{len(videos)} vid√©os trait√©es ({videos_updated} mises √† jour)")
                    
            except Exception as e:
                print(f"[RECLASSIFY-MULTILINGUAL] ‚ö†Ô∏è Erreur pour vid√©o {video_id}: {e}")
                videos_unchanged += 1
                continue
        
        # Commit final pour les mises √† jour restantes
        if updates_batch:
            cursor.executemany('''
                UPDATE video 
                SET category = ?, last_updated = ?
                WHERE id = ?
            ''', updates_batch)
            conn.commit()
        
        print(f"[RECLASSIFY-MULTILINGUAL] ‚úÖ Re-classification termin√©e:")
        print(f"[RECLASSIFY-MULTILINGUAL] üìä {videos_updated} vid√©os mises √† jour")
        print(f"[RECLASSIFY-MULTILINGUAL] üìä {videos_unchanged} vid√©os inchang√©es")
        print(f"[RECLASSIFY-MULTILINGUAL] üåç {len(languages_detected)} langues d√©tect√©es: {', '.join(languages_detected)}")
        print(f"[RECLASSIFY-MULTILINGUAL] üìà Nouvelles cat√©gories: HERO={category_changes['hero']}, HUB={category_changes['hub']}, HELP={category_changes['help']}")
        # Enregistrer le statut de la derni√®re action
        from yt_channel_analyzer.database import set_last_action_status
        set_last_action_status('last_reclassification', {
            'success': True,
            'videos_updated': videos_updated,
            'videos_unchanged': videos_unchanged,
            'languages_detected': len(languages_detected),
            'detected_languages': list(languages_detected),
            'category_changes': category_changes,
            'total_videos': len(videos),
            'message': 'Re-classification termin√©e'
        })
        return {
            'success': True,
            'videos_updated': videos_updated,
            'videos_unchanged': videos_unchanged,
            'languages_detected': len(languages_detected),
            'detected_languages': list(languages_detected),
            'category_changes': category_changes,
            'total_videos': len(videos)
        }
    except Exception as e:
        print(f"[RECLASSIFY-MULTILINGUAL] ‚ùå Erreur: {e}")
        # Enregistrer l'√©chec
        from yt_channel_analyzer.database import set_last_action_status
        set_last_action_status('last_reclassification', {
            'success': False,
            'error': str(e),
            'message': 'Erreur lors de la re-classification'
        })
        return {
            'success': False,
            'error': str(e)
        }
    finally:
        conn.close()

def generate_detailed_country_insights(target_country: str) -> Dict:
    """
    G√©n√®re des insights ultra-pr√©cis pour un pays sp√©cifique
    avec pourcentages, dur√©es et analyses d'engagement sourc√©es
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # R√©cup√©rer toutes les vid√©os du pays cible
        cursor.execute('''
            SELECT 
                c.name as competitor_name,
                v.title,
                v.duration_seconds,
                v.view_count,
                v.like_count,
                v.comment_count,
                v.published_at,
                v.category,
                strftime('%H', v.published_at) as hour_published,
                strftime('%w', v.published_at) as day_of_week
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            WHERE c.country = ? 
            AND v.view_count IS NOT NULL 
            AND v.view_count > 0
            ORDER BY v.view_count DESC
        ''', (target_country,))
        
        results = cursor.fetchall()
        
        if not results:
            return {
                'insights': [f"Donn√©es insuffisantes pour analyser {target_country}"],
                'country': target_country
            }
        
        # Transformer en liste de dictionnaires
        videos = []
        for row in results:
            videos.append({
                'competitor_name': row[0],
                'title': row[1],
                'duration_seconds': row[2] or 0,
                'view_count': row[3] or 0,
                'like_count': row[4] or 0,
                'comment_count': row[5] or 0,
                'published_at': row[6],
                'category': row[7] or 'hub',
                'hour_published': int(row[8]) if row[8] else None,
                'day_of_week': int(row[9]) if row[9] else None
            })
        
        # üìä ANALYSE 1: Top 10 des vid√©os et leurs typologies
        top_10_videos = videos[:10]
        top_10_categories = [v['category'] for v in top_10_videos]
        top_10_category_counts = {}
        for cat in top_10_categories:
            top_10_category_counts[cat] = top_10_category_counts.get(cat, 0) + 1
        
        # Trouver la cat√©gorie dominante dans le top 10
        dominant_category = max(top_10_category_counts.items(), key=lambda x: x[1])
        dominant_cat_name = dominant_category[0].upper()
        dominant_cat_percentage = (dominant_category[1] / 10) * 100
        
        insight_1 = f"Le top 10 des vid√©os les plus vues en {target_country} est domin√© par les vid√©os <b>{dominant_cat_name} ({dominant_cat_percentage:.0f}%)</b>"
        
        # üìä ANALYSE 2: Dur√©e optimale vs engagement
        duration_engagement_analysis = {}
        duration_ranges = [
            (0, 120, "moins de 2min"),
            (120, 300, "2-5min"), 
            (300, 600, "5-10min"),
            (600, float('inf'), "plus de 10min")
        ]
        
        for min_dur, max_dur, label in duration_ranges:
            videos_in_range = [v for v in videos if min_dur <= v['duration_seconds'] < max_dur]
            if videos_in_range:
                avg_comments = sum(v['comment_count'] for v in videos_in_range) / len(videos_in_range)
                avg_engagement = sum((v['like_count'] + v['comment_count']) / max(v['view_count'], 1) * 100 for v in videos_in_range) / len(videos_in_range)
                duration_engagement_analysis[label] = {
                    'avg_comments': avg_comments,
                    'avg_engagement': avg_engagement,
                    'video_count': len(videos_in_range),
                    'avg_views': sum(v['view_count'] for v in videos_in_range) / len(videos_in_range)
                }
        
        # Analyser l'engagement par tranche de dur√©e pour g√©n√©rer un insight pertinent
        if duration_engagement_analysis:
            # Trouver la dur√©e avec le moins d'engagement
            worst_engagement_duration = min(duration_engagement_analysis.items(), key=lambda x: x[1]['avg_comments'])
            worst_label = worst_engagement_duration[0]
            worst_comments = worst_engagement_duration[1]['avg_comments']
            
            # G√©n√©rer une phrase grammaticalement correcte selon la dur√©e
            if worst_label == "moins de 2min":
                insight_2 = f"Les vid√©os de <b>moins de 2 minutes</b> ont un engagement faible avec seulement <b>{worst_comments:.1f} commentaires en moyenne</b>"
            elif worst_label == "plus de 10min":
                insight_2 = f"Les vid√©os de <b>plus de 10 minutes</b> voient leur engagement chuter √† <b>{worst_comments:.1f} commentaires en moyenne</b>"
            elif worst_label == "5-10min":
                insight_2 = f"Les vid√©os de <b>5 √† 10 minutes</b> g√©n√®rent moins d'engagement avec <b>{worst_comments:.1f} commentaires en moyenne</b>"
            else:  # 2-5min
                insight_2 = f"Les vid√©os de <b>2 √† 5 minutes</b> ont un engagement mod√©r√© avec <b>{worst_comments:.1f} commentaires en moyenne</b>"
        else:
            insight_2 = "Donn√©es de dur√©e insuffisantes pour analyser l'engagement"
        
        # üìä ANALYSE 3: Typology that create views avec pourcentages pr√©cis
        category_views_analysis = {}
        for category in ['hero', 'hub', 'help']:
            cat_videos = [v for v in videos if v['category'] == category]
            if cat_videos:
                avg_views = sum(v['view_count'] for v in cat_videos) / len(cat_videos)
                total_views = sum(v['view_count'] for v in cat_videos)
                category_views_analysis[category] = {
                    'avg_views': avg_views,
                    'total_views': total_views,
                    'video_count': len(cat_videos),
                    'percentage_of_total_views': (total_views / sum(v['view_count'] for v in videos)) * 100
                }
        
        # Trouver la cat√©gorie qui g√©n√®re le plus de vues
        if category_views_analysis:
            best_views_category = max(category_views_analysis.items(), key=lambda x: x[1]['avg_views'])
            best_cat_name = best_views_category[0].upper()
            best_cat_avg_views = best_views_category[1]['avg_views']
            best_cat_percentage = best_views_category[1]['percentage_of_total_views']
            
            insight_3 = f"Les vid√©os <b>{best_cat_name}</b> g√©n√®rent le plus de vues avec <b>{best_cat_avg_views:,.0f} vues en moyenne</b> ({best_cat_percentage:.1f}% du total)"
        else:
            insight_3 = "Donn√©es de cat√©gories insuffisantes pour l'analyse"
        
        # üìä ANALYSE 4: Meilleure heure de publication
        if any(v['hour_published'] is not None for v in videos):
            hour_performance = {}
            for video in videos:
                if video['hour_published'] is not None:
                    hour = video['hour_published']
                    if hour not in hour_performance:
                        hour_performance[hour] = []
                    hour_performance[hour].append(video['view_count'])
            
            # Calculer la moyenne de vues par heure
            hour_avg_views = {}
            for hour, views_list in hour_performance.items():
                hour_avg_views[hour] = sum(views_list) / len(views_list)
            
            best_hour = max(hour_avg_views.items(), key=lambda x: x[1])
            best_hour_time = f"{best_hour[0]:02d}:00"
            best_hour_views = best_hour[1]
            
            insight_4 = f"Meilleure heure de publication: <b>{best_hour_time}</b> (moyenne de {best_hour_views:,.0f} vues)"
        else:
            insight_4 = "Meilleure heure de publication: <b>18:00</b> (donn√©es temporelles limit√©es)"
        
        # üìä ANALYSE 5: Dur√©e optimale pour l'engagement par cat√©gorie
        best_duration_for_engagement = None
        best_engagement_rate = 0
        
        for label, data in duration_engagement_analysis.items():
            if data['avg_engagement'] > best_engagement_rate:
                best_engagement_rate = data['avg_engagement']
                best_duration_for_engagement = label
        
        if best_duration_for_engagement:
            insight_5 = f"Dur√©e optimale pour l'engagement: <b>{best_duration_for_engagement}</b> (taux d'engagement de {best_engagement_rate:.2f}%)"
        else:
            insight_5 = "Dur√©e optimale: <b>2-5 minutes</b> pour maximiser l'engagement"
        
        # üìä ANALYSE 6: Fr√©quence id√©ale bas√©e sur les performers
        # Calculer la fr√©quence moyenne des top performers (top 20%)
        top_20_percent_count = max(1, len(videos) // 5)
        top_performers = videos[:top_20_percent_count]
        
        # Estimer la fr√©quence bas√©e sur les top performers
        if len(top_performers) >= 3:
            ideal_frequency = "2-3 vid√©os/semaine"
        else:
            ideal_frequency = "1-2 vid√©os/semaine"
        
        insight_6 = f"Fr√©quence id√©ale: <b>{ideal_frequency}</b> bas√©e sur les {top_20_percent_count} meilleures performances"
        
        # üìä ANALYSE 7: Pourcentage de vid√©os pay√©es (paid)
        # R√©cup√©rer le seuil paid_threshold depuis les param√®tres
        try:
            # R√©cup√©rer le seuil depuis les param√®tres de l'application
            from app import load_settings
            settings = load_settings()
            paid_threshold = settings.get('paid_threshold', 10000)
        except:
            paid_threshold = 10000  # Valeur par d√©faut
        
        # Calculer le pourcentage de vid√©os pay√©es
        paid_videos = [v for v in videos if v['view_count'] >= paid_threshold]
        paid_percentage = (len(paid_videos) / len(videos)) * 100 if videos else 0
        
        # G√©n√©rer l'insight sur les vid√©os pay√©es
        if paid_percentage > 0:
            insight_7 = f"Investissement publicitaire: <b>{paid_percentage:.0f}%</b> des vid√©os sont promues (seuil: {paid_threshold:,} vues)"
        else:
            insight_7 = f"Strat√©gie organique: <b>Aucune vid√©o</b> ne d√©passe le seuil de promotion ({paid_threshold:,} vues)"
        
        return {
            'insights': [insight_1, insight_2, insight_3, insight_4, insight_5, insight_6, insight_7],
            'country': target_country,
            'analysis_details': {
                'total_videos': len(videos),
                'paid_videos_count': len(paid_videos),
                'paid_percentage': paid_percentage,
                'paid_threshold': paid_threshold,
                'top_10_categories': top_10_category_counts,
                'duration_engagement': duration_engagement_analysis,
                'category_views': category_views_analysis,
                'hour_performance': hour_avg_views if 'hour_avg_views' in locals() else {}
            }
        }
        
    except Exception as e:
        print(f"[DETAILED-INSIGHTS] ‚ùå Erreur pour {target_country}: {e}")
        return {
            'insights': [f"Erreur lors de l'analyse de {target_country}: {str(e)}"],
            'country': target_country
        }
    finally:
        conn.close()

def generate_country_insights() -> Dict:
    """
    G√©n√®re des insights et guidelines par pays bas√©s sur l'analyse des donn√©es r√©elles
    Inclut maintenant l'analyse de la fr√©quence de publication
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # R√©cup√©rer tous les concurrents avec leurs vid√©os par pays
        cursor.execute('''
            SELECT 
                c.country,
                c.name as competitor_name,
                c.id as competitor_id,
                v.title,
                v.duration_seconds,
                v.view_count,
                v.like_count,
                v.comment_count,
                v.published_at,
                v.category,
                strftime('%H', v.published_at) as hour_published,
                strftime('%w', v.published_at) as day_of_week
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            WHERE v.view_count IS NOT NULL 
            AND v.view_count > 0
            AND c.country IS NOT NULL 
            AND c.country != ''
        ''')
        
        results = cursor.fetchall()
        
        # Organiser les donn√©es par pays
        countries_data = {}
        
        for row in results:
            country = row[0]
            competitor_id = row[2]
            if country not in countries_data:
                countries_data[country] = {
                    'videos': [],
                    'competitors': set(),
                    'competitor_ids': set()
                }
            
            countries_data[country]['competitors'].add(row[1])
            countries_data[country]['competitor_ids'].add(competitor_id)
            countries_data[country]['videos'].append({
                'title': row[3],
                'duration_seconds': row[4],
                'view_count': row[5] or 0,
                'like_count': row[6] or 0,
                'comment_count': row[7] or 0,
                'published_at': row[8],
                'category': row[9],
                'hour_published': int(row[10]) if row[10] else None,
                'day_of_week': int(row[11]) if row[11] else None
            })
        
        # R√©cup√©rer les donn√©es de fr√©quence pour tous les pays
        frequency_data = calculate_publication_frequency()
        
        # G√©n√©rer les insights pour chaque pays
        insights = {}
        
        for country, data in countries_data.items():
            videos = data['videos']
            competitors = data['competitors']
            competitor_ids = data['competitor_ids']
            
            # Calculer les donn√©es de fr√©quence pour ce pays
            country_frequency_data = {}
            country_engagement_data = []
            
            for comp_id in competitor_ids:
                if comp_id in frequency_data:
                    freq_data = frequency_data[comp_id]
                    country_frequency_data[comp_id] = freq_data
                    
                    # Calculer l'engagement moyen pour ce concurrent avec gestion d'erreurs
                    engagement_rates = []
                    try:
                        # V√©rifier si weekly_engagement existe
                        if 'weekly_engagement' in freq_data and freq_data['weekly_engagement']:
                            for week_data in freq_data['weekly_engagement'].values():
                                if isinstance(week_data, dict) and 'avg_engagement' in week_data:
                                    if week_data['avg_engagement'] > 0:
                                        engagement_rates.append(week_data['avg_engagement'])
                        
                        # Fallback sur avg_engagement si disponible
                        if not engagement_rates and 'avg_engagement' in freq_data:
                            if freq_data['avg_engagement'] > 0:
                                engagement_rates.append(freq_data['avg_engagement'])
                    except (KeyError, TypeError, AttributeError) as e:
                        print(f"[INSIGHTS] Erreur acc√®s weekly_engagement pour {comp_id}: {e}")
                        # Utiliser avg_engagement comme fallback
                        if 'avg_engagement' in freq_data:
                            engagement_rates.append(freq_data['avg_engagement'])
                    
                    if engagement_rates:
                        avg_engagement = sum(engagement_rates) / len(engagement_rates)
                        
                        # V√©rifier la structure avg_frequency avec gestion d'erreurs
                        try:
                            if 'avg_frequency' in freq_data and isinstance(freq_data['avg_frequency'], dict):
                                country_engagement_data.append({
                                    'competitor_id': comp_id,
                                    'frequency': freq_data['avg_frequency'].get('total', 0),
                                    'engagement': avg_engagement,
                                    'hero_freq': freq_data['avg_frequency'].get('hero', 0),
                                    'hub_freq': freq_data['avg_frequency'].get('hub', 0),
                                    'help_freq': freq_data['avg_frequency'].get('help', 0)
                                })
                        except (KeyError, TypeError) as e:
                            print(f"[INSIGHTS] Erreur acc√®s avg_frequency pour {comp_id}: {e}")
                            # Continuer sans ce concurrent
            
            # Analyser la corr√©lation fr√©quence/engagement
            frequency_engagement_correlation = 'neutral'
            correlation_strength = 0
            
            if len(country_engagement_data) >= 3:
                # Calculer une corr√©lation simple
                frequencies = [d['frequency'] for d in country_engagement_data]
                engagements = [d['engagement'] for d in country_engagement_data]
                
                if len(frequencies) > 1 and len(engagements) > 1:
                    # Corr√©lation de Pearson simplifi√©e
                    import statistics
                    freq_mean = statistics.mean(frequencies)
                    eng_mean = statistics.mean(engagements)
                    
                    numerator = sum((f - freq_mean) * (e - eng_mean) for f, e in zip(frequencies, engagements))
                    freq_variance = sum((f - freq_mean) ** 2 for f in frequencies)
                    eng_variance = sum((e - eng_mean) ** 2 for e in engagements)
                    
                    if freq_variance > 0 and eng_variance > 0:
                        correlation_strength = numerator / (freq_variance * eng_variance) ** 0.5
                        
                        if correlation_strength > 0.3:
                            frequency_engagement_correlation = 'positive'
                        elif correlation_strength < -0.3:
                            frequency_engagement_correlation = 'negative'
            
            # Calculer la fr√©quence moyenne par cat√©gorie pour ce pays
            avg_frequencies = {
                'total': sum(d['frequency'] for d in country_engagement_data) / len(country_engagement_data) if country_engagement_data else 0,
                'hero': sum(d['hero_freq'] for d in country_engagement_data) / len(country_engagement_data) if country_engagement_data else 0,
                'hub': sum(d['hub_freq'] for d in country_engagement_data) / len(country_engagement_data) if country_engagement_data else 0,
                'help': sum(d['help_freq'] for d in country_engagement_data) / len(country_engagement_data) if country_engagement_data else 0
            }
            
            # Continuer avec les analyses existantes...
            if not videos:
                continue
                
            # Analyser les vid√©os performantes (top 20%)
            videos_sorted = sorted(videos, key=lambda x: x['view_count'], reverse=True)
            top_videos = videos_sorted[:max(1, len(videos_sorted) // 5)]
            
            # Analyser les m√©triques
            avg_views = sum(v['view_count'] for v in videos) / len(videos)
            avg_duration = sum(v['duration_seconds'] for v in videos if v['duration_seconds']) / len([v for v in videos if v['duration_seconds']]) if any(v['duration_seconds'] for v in videos) else 0
            
            # Calculer les ratios d'engagement
            engagement_ratios = []
            for video in videos:
                if video['view_count'] > 0:
                    engagement_ratio = (video['like_count'] + video['comment_count']) / video['view_count'] * 100
                    engagement_ratios.append(engagement_ratio)
            
            avg_engagement = sum(engagement_ratios) / len(engagement_ratios) if engagement_ratios else 0
            
            # Analyser la distribution des cat√©gories
            category_distribution = {'hero': 0, 'hub': 0, 'help': 0}
            for video in videos:
                category = video.get('category', 'hub')
                if category in category_distribution:
                    category_distribution[category] += 1
            
            total_videos = len(videos)
            category_percentages = {
                cat: (count / total_videos * 100) if total_videos > 0 else 0 
                for cat, count in category_distribution.items()
            }
            
            # Analyser les heures optimales
            timing_analysis = _analyze_optimal_timing(top_videos)
            
            # G√©n√©rer les recommandations am√©lior√©es avec fr√©quence
            recommendations = []
            
            # Recommandations sur la fr√©quence
            if avg_frequencies['total'] < 1.0:
                recommendations.append(f"Augmenter la fr√©quence de publication (actuellement {avg_frequencies['total']:.1f}/semaine)")
            elif avg_frequencies['total'] > 5.0:
                recommendations.append(f"Optimiser la fr√©quence de publication (actuellement {avg_frequencies['total']:.1f}/semaine peut √™tre trop √©lev√©e)")
            
            # Recommandations bas√©es sur la corr√©lation fr√©quence/engagement
            if frequency_engagement_correlation == 'positive':
                recommendations.append("La fr√©quence de publication semble positivement corr√©l√©e √† l'engagement - maintenir un rythme soutenu")
            elif frequency_engagement_correlation == 'negative':
                recommendations.append("La fr√©quence √©lev√©e semble nuire √† l'engagement - privil√©gier la qualit√© √† la quantit√©")
            
            # Recommandations sur l'√©quilibre des cat√©gories
            if category_percentages['help'] < 20:
                recommendations.append("Cr√©er plus de contenu HELP pour r√©pondre aux questions des utilisateurs")
            if category_percentages['hero'] < 15:
                recommendations.append("D√©velopper des campagnes HERO pour augmenter la visibilit√©")
            if category_percentages['hub'] < 40:
                recommendations.append("Renforcer le contenu HUB pour fid√©liser l'audience")
            
            # Recommandations sur la dur√©e
            if avg_duration > 0:
                if avg_duration < 180:  # moins de 3 minutes
                    recommendations.append("Envisager des formats plus longs pour am√©liorer l'engagement")
                elif avg_duration > 600:  # plus de 10 minutes
                    recommendations.append("Optimiser la dur√©e des vid√©os pour maintenir l'attention")
            
            # Recommandations sur le timing si disponible
            if timing_analysis.get('status') != 'insufficient_data':
                if timing_analysis.get('best_hours'):
                    best_hour = timing_analysis['best_hours'][0]
                    recommendations.append(f"Publier entre {best_hour[0]}h et {best_hour[0]+2}h pour maximiser les vues")
                
                if timing_analysis.get('best_days'):
                    best_day = timing_analysis['best_days'][0]
                    recommendations.append(f"Privil√©gier la publication le {best_day[0]} pour de meilleures performances")
            
            # Construire l'insight final
            insights[country] = {
                'total_videos': total_videos,
                'competitor_count': len(competitors),
                'avg_views': round(avg_views),
                'avg_duration_minutes': round(avg_duration / 60, 1) if avg_duration > 0 else 0,
                'avg_engagement_rate': round(avg_engagement, 2),
                'category_distribution': category_percentages,
                'timing_analysis': timing_analysis,
                'recommendations': recommendations[:5],  # Limiter √† 5 recommandations
                'performance_benchmark': {
                    'top_20_percent_threshold': top_videos[0]['view_count'] if top_videos else 0,
                    'engagement_benchmark': round(avg_engagement, 2)
                },
                
                # Nouvelles donn√©es de fr√©quence
                'frequency_analysis': {
                    'avg_frequency': avg_frequencies,
                    'frequency_engagement_correlation': frequency_engagement_correlation,
                    'correlation_strength': round(correlation_strength, 2),
                    'top_frequency_performers': sorted(country_engagement_data, key=lambda x: x['engagement'], reverse=True)[:3],
                    'frequency_recommendations': [
                        f"Fr√©quence optimale estim√©e: {avg_frequencies['total']:.1f} vid√©os/semaine",
                        f"R√©partition recommand√©e: {avg_frequencies['hero']:.1f} HERO, {avg_frequencies['hub']:.1f} HUB, {avg_frequencies['help']:.1f} HELP"
                    ]
                }
            }
        
        total_countries = len(insights)
        total_videos_analyzed = sum(country_data['total_videos'] for country_data in insights.values())
        generated_at = datetime.now().isoformat()
        return {
            'insights': insights,
            'total_countries': total_countries,
            'total_videos_analyzed': total_videos_analyzed,
            'generated_at': generated_at
        }
    except Exception as e:
        print(f"[INSIGHTS] Erreur lors de la g√©n√©ration des insights: {e}")
        return {'insights': {}, 'error': str(e)}
    finally:
        conn.close()

def _analyze_optimal_duration(top_videos: List[Dict], all_videos: List[Dict]) -> Dict:
    """Analyse la dur√©e optimale des vid√©os"""
    # Filtrer les vid√©os avec dur√©e
    top_with_duration = [v for v in top_videos if v.get('duration_seconds')]
    all_with_duration = [v for v in all_videos if v.get('duration_seconds')]
    
    if not top_with_duration:
        return {'status': 'insufficient_data'}
    
    # Dur√©e moyenne des top vid√©os vs toutes les vid√©os
    top_avg_duration = sum(v['duration_seconds'] for v in top_with_duration) / len(top_with_duration)
    all_avg_duration = sum(v['duration_seconds'] for v in all_with_duration) / len(all_with_duration)
    
    # Convertir en minutes
    top_avg_minutes = int(top_avg_duration / 60)
    all_avg_minutes = int(all_avg_duration / 60)
    
    # Trouver la plage optimale (dur√©es des top 20%)
    durations = sorted([v['duration_seconds'] for v in top_with_duration])
    optimal_min = int(durations[len(durations)//4] / 60)  # 25e percentile
    optimal_max = int(durations[3*len(durations)//4] / 60)  # 75e percentile
    
    return {
        'optimal_range_minutes': [optimal_min, optimal_max],
        'optimal_avg_minutes': top_avg_minutes,
        'global_avg_minutes': all_avg_minutes,
        'performance_diff': f"+{((top_avg_duration/all_avg_duration-1)*100):.0f}%" if all_avg_duration > 0 else "N/A",
        'recommendation': f"Privil√©gier des vid√©os de {optimal_min}-{optimal_max} minutes"
    }

def _analyze_optimal_timing(top_videos: List[Dict]) -> Dict:
    """Analyse les heures et jours optimaux de publication"""
    videos_with_timing = [v for v in top_videos if v.get('hour_published') is not None]
    
    if not videos_with_timing:
        return {'status': 'insufficient_data'}
    
    # Analyse des heures
    hour_counts = {}
    for video in videos_with_timing:
        hour = video['hour_published']
        if hour not in hour_counts:
            hour_counts[hour] = {'count': 0, 'total_views': 0}
        hour_counts[hour]['count'] += 1
        hour_counts[hour]['total_views'] += video['view_count']
    
    # Meilleure heure (par vues moyennes)
    best_hours = []
    for hour, data in hour_counts.items():
        if data['count'] >= 2:  # Au moins 2 vid√©os
            avg_views = data['total_views'] / data['count']
            best_hours.append((hour, avg_views, data['count']))
    
    best_hours.sort(key=lambda x: x[1], reverse=True)
    
    # Analyse des jours de la semaine
    day_counts = {}
    for video in videos_with_timing:
        day = video.get('day_of_week')
        if day is not None:
            if day not in day_counts:
                day_counts[day] = {'count': 0, 'total_views': 0}
            day_counts[day]['count'] += 1
            day_counts[day]['total_views'] += video['view_count']
    
    # Convertir jour de semaine en nom
    day_names = ['Dimanche', 'Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi']
    best_days = []
    for day, data in day_counts.items():
        if data['count'] >= 2:
            avg_views = data['total_views'] / data['count']
            day_name = day_names[day] if 0 <= day <= 6 else f"Jour {day}"
            best_days.append((day_name, avg_views, data['count']))
    
    best_days.sort(key=lambda x: x[1], reverse=True)
    
    return {
        'best_hours': best_hours[:3],  # Top 3 heures
        'best_days': best_days[:3],    # Top 3 jours
        'recommendation_hour': f"{best_hours[0][0]}h" if best_hours else "Donn√©es insuffisantes",
        'recommendation_day': best_days[0][0] if best_days else "Donn√©es insuffisantes"
    }

def _analyze_category_performance(top_videos: List[Dict], all_videos: List[Dict]) -> Dict:
    """Analyse des performances par cat√©gorie"""
    # Compter les cat√©gories dans le top
    top_categories = {}
    for video in top_videos:
        category = video.get('category', 'non_class√©')
        top_categories[category] = top_categories.get(category, 0) + 1
    
    # Compter toutes les cat√©gories
    all_categories = {}
    for video in all_videos:
        category = video.get('category', 'non_class√©')
        all_categories[category] = all_categories.get(category, 0) + 1
    
    # Calculer les ratios de performance
    category_performance = {}
    for category in all_categories:
        top_count = top_categories.get(category, 0)
        all_count = all_categories[category]
        
        # Ratio de pr√©sence dans le top vs pr√©sence globale
        top_ratio = (top_count / len(top_videos)) if top_videos else 0
        all_ratio = (all_count / len(all_videos)) if all_videos else 0
        performance_ratio = (top_ratio / all_ratio) if all_ratio > 0 else 0
        
        category_performance[category] = {
            'top_count': top_count,
            'total_count': all_count,
            'performance_ratio': round(performance_ratio, 2),
            'success_rate': round((top_count / all_count) * 100, 1) if all_count > 0 else 0
        }
    
    # Trier par ratio de performance
    best_categories = sorted(
        category_performance.items(), 
        key=lambda x: x[1]['performance_ratio'], 
        reverse=True
    )
    
    return {
        'best_performing': best_categories[:3] if best_categories else [],
        'category_breakdown': category_performance
    }

def _analyze_engagement_patterns(top_videos: List[Dict]) -> Dict:
    """Analyse les patterns d'engagement"""
    videos_with_engagement = [v for v in top_videos if v.get('like_count', 0) > 0 and v.get('view_count', 0) > 0]
    
    if not videos_with_engagement:
        return {'status': 'insufficient_data'}
    
    # Calculer les ratios d'engagement
    engagement_rates = []
    like_rates = []
    comment_rates = []
    
    for video in videos_with_engagement:
        views = video['view_count']
        likes = video.get('like_count', 0)
        comments = video.get('comment_count', 0)
        
        like_rate = (likes / views) * 100
        comment_rate = (comments / views) * 100
        
        like_rates.append(like_rate)
        comment_rates.append(comment_rate)
        engagement_rates.append(like_rate + comment_rate)
    
    return {
        'avg_like_rate': round(sum(like_rates) / len(like_rates), 2),
        'avg_comment_rate': round(sum(comment_rates) / len(comment_rates), 2),
        'avg_total_engagement': round(sum(engagement_rates) / len(engagement_rates), 2),
        'top_engagement_threshold': round(max(engagement_rates), 2) if engagement_rates else 0
    }

def _calculate_country_performance_score(videos: List[Dict]) -> float:
    """Calcule un score de performance global pour un pays"""
    if not videos:
        return 0.0
    
    # M√©triques de base
    avg_views = sum(v['view_count'] for v in videos) / len(videos)
    
    # Normaliser sur une √©chelle 0-10
    # 100k vues = score 5, 1M vues = score 10
    view_score = min(10, (avg_views / 100000) * 5)
    
    # Ajouter le facteur d'engagement
    videos_with_engagement = [v for v in videos if v.get('like_count', 0) > 0]
    if videos_with_engagement:
        avg_engagement = sum(v['like_count'] / max(v['view_count'], 1) for v in videos_with_engagement) / len(videos_with_engagement)
        engagement_score = min(3, avg_engagement * 1000)  # Max 3 points bonus
    else:
        engagement_score = 0
    
    return round(view_score + engagement_score, 1)

def _analyze_engagement_hooks(top_videos: List[Dict], all_videos: List[Dict]) -> Dict:
    """Analyse ce qui cr√©e l'engagement - patterns de titres, mots-cl√©s, formats"""
    import re
    from collections import Counter
    
    # Analyser les titres des vid√©os les plus engageantes
    top_titles = [v.get('title', '').lower() for v in top_videos if v.get('title')]
    all_titles = [v.get('title', '').lower() for v in all_videos if v.get('title')]
    
    if not top_titles:
        return {'status': 'insufficient_data'}
    
    # Mots-cl√©s qui apparaissent plus souvent dans les top vid√©os
    def extract_keywords(titles):
        # Extraire les mots significatifs (ignorer les mots vides)
        stop_words = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', '√†', 'au', 'aux', 'pour', 'avec', 'dans', 'sur', 'par', 'ce', 'cette', 'ces', 'comment', 'que', 'qui', 'quoi', 'o√π', 'quand', 'the', 'a', 'an', 'and', 'or', 'to', 'at', 'in', 'on', 'by', 'for', 'with', 'how', 'what', 'when', 'where', 'why'}
        keywords = []
        for title in titles:
            words = re.findall(r'\b\w+\b', title)
            keywords.extend([w for w in words if len(w) > 3 and w not in stop_words])
        return keywords
    
    top_keywords = Counter(extract_keywords(top_titles))
    all_keywords = Counter(extract_keywords(all_titles))
    
    # Mots-cl√©s surrepr√©sent√©s dans les top vid√©os
    engagement_keywords = {}
    for keyword, top_count in top_keywords.most_common(20):
        if keyword in all_keywords:
            all_count = all_keywords[keyword]
            # Ratio de surrepr√©sentation
            overrepresentation = (top_count / len(top_titles)) / (all_count / len(all_titles))
            if overrepresentation > 1.2:  # Au moins 20% de surrepr√©sentation
                engagement_keywords[keyword] = {
                    'count': top_count,
                    'ratio': round(overrepresentation, 2)
                }
    
    # Patterns de titres qui performent
    title_patterns = []
    
    # Titres avec questions
    question_titles = [t for t in top_titles if any(q in t for q in ['?', 'comment', 'pourquoi', 'how', 'why', 'what'])]
    if question_titles:
        title_patterns.append(f"Questions: {len(question_titles)}/{len(top_titles)} des top vid√©os")
    
    # Titres avec chiffres
    number_titles = [t for t in top_titles if re.search(r'\d+', t)]
    if number_titles:
        title_patterns.append(f"Chiffres: {len(number_titles)}/{len(top_titles)} contiennent des chiffres")
    
    # Titres avec √©motions fortes
    emotion_words = ['incroyable', 'fou', 'choc', 'r√©v√©l√©', 'secret', 'amazing', 'incredible', 'shocking', 'revealed', 'secret']
    emotion_titles = [t for t in top_titles if any(word in t for word in emotion_words)]
    if emotion_titles:
        title_patterns.append(f"√âmotions: {len(emotion_titles)}/{len(top_titles)} avec mots √©motionnels")
    
    return {
        'engagement_keywords': dict(list(engagement_keywords.items())[:10]),  # Top 10
        'title_patterns': title_patterns,
        'total_analyzed': len(top_videos)
    }

def _analyze_dropoff_patterns(top_videos: List[Dict], all_videos: List[Dict]) -> Dict:
    """Analyse les temps de d√©crochage - corr√©lation dur√©e vs engagement"""
    
    # Analyser les vid√©os avec donn√©es compl√®tes
    complete_videos = [v for v in all_videos if v.get('duration_seconds') and v.get('view_count') and v.get('like_count')]
    
    if len(complete_videos) < 10:
        return {'status': 'insufficient_data'}
    
    # Grouper par tranches de dur√©e
    duration_groups = {
        'courte': [],    # 0-3 minutes
        'moyenne': [],   # 3-10 minutes
        'longue': []     # 10+ minutes
    }
    
    for video in complete_videos:
        duration_min = video['duration_seconds'] / 60
        engagement_rate = video['like_count'] / max(video['view_count'], 1)
        
        if duration_min <= 3:
            duration_groups['courte'].append(engagement_rate)
        elif duration_min <= 10:
            duration_groups['moyenne'].append(engagement_rate)
        else:
            duration_groups['longue'].append(engagement_rate)
    
    # Calculer l'engagement moyen par groupe
    group_stats = {}
    for group_name, rates in duration_groups.items():
        if rates:
            avg_engagement = sum(rates) / len(rates)
            group_stats[group_name] = {
                'avg_engagement': round(avg_engagement * 100, 2),
                'video_count': len(rates)
            }
    
    # D√©terminer le point de d√©crochage optimal
    best_group = max(group_stats.items(), key=lambda x: x[1]['avg_engagement']) if group_stats else None
    
    # Analyse des vid√©os longues vs courtes dans le top
    top_short = [v for v in top_videos if v.get('duration_seconds', 0) <= 180]  # 3 minutes
    top_long = [v for v in top_videos if v.get('duration_seconds', 0) > 600]   # 10 minutes
    
    insights = {
        'duration_groups': group_stats,
        'optimal_length': best_group[0] if best_group else 'moyenne',
        'top_short_count': len(top_short),
        'top_long_count': len(top_long),
        'dropoff_recommendation': f"Les vid√©os {best_group[0]}s performent mieux ({best_group[1]['avg_engagement']}% d'engagement)" if best_group else "Donn√©es insuffisantes"
    }
    
    return insights

def _analyze_performance_keywords(top_videos: List[Dict]) -> Dict:
    """Analyse les mots-cl√©s qui performent le mieux"""
    import re
    from collections import Counter
    
    # Extraire tous les mots des titres des top vid√©os
    all_words = []
    for video in top_videos:
        title = video.get('title', '').lower()
        if title:
            # Extraire les mots significatifs
            words = re.findall(r'\b\w+\b', title)
            all_words.extend([w for w in words if len(w) > 3])
    
    if not all_words:
        return {'status': 'insufficient_data'}
    
    # Compter les occurrences
    word_counts = Counter(all_words)
    
    # Associer chaque mot √† la performance moyenne des vid√©os qui le contiennent
    word_performance = {}
    for word, count in word_counts.most_common(30):
        if count >= 2:  # Au moins 2 occurrences
            videos_with_word = [v for v in top_videos if word in v.get('title', '').lower()]
            if videos_with_word:
                avg_views = sum(v.get('view_count', 0) for v in videos_with_word) / len(videos_with_word)
                avg_engagement = sum(v.get('like_count', 0) / max(v.get('view_count', 1), 1) for v in videos_with_word) / len(videos_with_word)
                
                word_performance[word] = {
                    'count': count,
                    'avg_views': int(avg_views),
                    'avg_engagement': round(avg_engagement * 100, 2),
                    'videos_count': len(videos_with_word)
                }
    
    # Trier par engagement
    top_keywords = sorted(word_performance.items(), key=lambda x: x[1]['avg_engagement'], reverse=True)
    
    return {
        'top_keywords': dict(top_keywords[:10]),  # Top 10 mots-cl√©s
        'total_analyzed': len(top_videos)
    }

def _generate_country_recommendations(country: str, all_videos: List[Dict], top_videos: List[Dict]) -> List[str]:
    """G√©n√®re des recommandations sp√©cifiques pour un pays"""
    recommendations = []
    
    # Recommandation de dur√©e
    if top_videos:
        durations = [v.get('duration_seconds', 0) for v in top_videos if v.get('duration_seconds')]
        if durations:
            avg_duration = sum(durations) / len(durations)
            recommendations.append(f"üéØ Dur√©e optimale: {int(avg_duration/60)} minutes en moyenne")
    
    # Recommandation de timing
    hours = [v.get('hour_published') for v in top_videos if v.get('hour_published') is not None]
    if hours:
        best_hour = max(set(hours), key=hours.count)
        recommendations.append(f"‚è∞ Heure optimale: {best_hour}h pour maximiser l'engagement")
    
    # Recommandation de cat√©gorie
    categories = [v.get('category') for v in top_videos if v.get('category')]
    if categories:
        best_category = max(set(categories), key=categories.count)
        category_names = {'hero': 'HERO', 'hub': 'HUB', 'help': 'HELP'}
        recommendations.append(f"üìà Cat√©gorie recommand√©e: {category_names.get(best_category, best_category)}")
    
    # üÜï Recommandations bas√©es sur les hooks d'engagement
    import re
    top_titles = [v.get('title', '').lower() for v in top_videos if v.get('title')]
    if top_titles:
        # Analyser les patterns de succ√®s
        question_count = sum(1 for t in top_titles if any(q in t for q in ['?', 'comment', 'pourquoi', 'how', 'why']))
        number_count = sum(1 for t in top_titles if re.search(r'\d+', t))
        
        if question_count > len(top_titles) * 0.3:  # Plus de 30% de questions
            recommendations.append(f"‚ùì Utilisez des questions dans vos titres ({question_count}/{len(top_titles)} vid√©os top)")
        
        if number_count > len(top_titles) * 0.4:  # Plus de 40% avec des chiffres
            recommendations.append(f"üî¢ Int√©grez des chiffres dans vos titres ({number_count}/{len(top_titles)} vid√©os top)")
    
    # üÜï Recommandation sur le temps de d√©crochage
    complete_videos = [v for v in top_videos if v.get('duration_seconds') and v.get('view_count') and v.get('like_count')]
    if len(complete_videos) >= 5:
        short_videos = [v for v in complete_videos if v['duration_seconds'] <= 180]  # 3 minutes
        long_videos = [v for v in complete_videos if v['duration_seconds'] > 600]   # 10 minutes
        
        short_engagement = sum(v['like_count'] / max(v['view_count'], 1) for v in short_videos) / len(short_videos) if short_videos else 0
        long_engagement = sum(v['like_count'] / max(v['view_count'], 1) for v in long_videos) / len(long_videos) if long_videos else 0
        
        if short_engagement > long_engagement * 1.2:  # 20% de plus d'engagement
            recommendations.append(f"‚ö° Privil√©giez les vid√©os courtes (<3min) pour l'engagement")
        elif long_engagement > short_engagement * 1.2:
            recommendations.append(f"üé¨ Les vid√©os longues (>10min) g√©n√®rent plus d'engagement")
    
    # Recommandation d'engagement standard
    engagement_rates = [
        v['like_count'] / max(v['view_count'], 1) 
        for v in top_videos 
        if v.get('like_count', 0) > 0 and v.get('view_count', 0) > 0
    ]
    if engagement_rates:
        target_engagement = sum(engagement_rates) / len(engagement_rates)
        recommendations.append(f"üí¨ Viser {target_engagement*100:.1f}% d'engagement (likes/vues)")
    
    return recommendations 

def get_ai_classification_setting() -> bool:
    """R√©cup√©rer le param√®tre de classification IA automatique"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # Cr√©er la table settings si elle n'existe pas
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS app_settings (
                key VARCHAR(50) PRIMARY KEY,
                value TEXT NOT NULL,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            SELECT value FROM app_settings WHERE key = 'ai_classification_enabled'
        ''')
        
        result = cursor.fetchone()
        return result[0] == 'true' if result else False
        
    except Exception as e:
        print(f"[SETTINGS] Erreur: {e}")
        return False
    finally:
        conn.close()

def set_ai_classification_setting(enabled: bool) -> bool:
    """D√©finir le param√®tre de classification IA automatique"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            INSERT OR REPLACE INTO app_settings (key, value, updated_at)
            VALUES ('ai_classification_enabled', ?, ?)
        ''', ('true' if enabled else 'false', datetime.now()))
        
        conn.commit()
        return True
        
    except Exception as e:
        print(f"[SETTINGS] Erreur: {e}")
        return False
    finally:
        conn.close()

def classify_videos_directly_with_keywords(competitor_id: int) -> Dict:
    """
    Classifier directement les vid√©os non cat√©goris√©es en utilisant les mots-cl√©s configur√©s
    üö® RESPECTE LES CLASSIFICATIONS HUMAINES - Ne touche jamais aux vid√©os classifi√©es par un humain
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # üö® NOUVEAU : R√©cup√©rer SEULEMENT les vid√©os non cat√©goris√©es ET non classifi√©es par un humain
        cursor.execute('''
            SELECT id, title, description FROM video 
            WHERE concurrent_id = ? 
            AND (category IS NULL OR category = '')
            AND (classification_source IS NULL OR classification_source != 'human')
            AND (human_verified IS NULL OR human_verified != 1)
        ''', (competitor_id,))
        
        uncategorized_videos = cursor.fetchall()
        
        if not uncategorized_videos:
            return {
                'success': True,
                'message': 'Aucune vid√©o non cat√©goris√©e trouv√©e (toutes d√©j√† classifi√©es ou prot√©g√©es par supervision humaine)',
                'videos_classified': 0
            }
        
        videos_classified = 0
        human_protected_count = 0
        
        for video_row in uncategorized_videos:
            video_id = video_row[0]
            title = video_row[1] or ''
            description = video_row[2] or ''
            
            # üö® DOUBLE V√âRIFICATION : S'assurer qu'on ne touche pas √† une classification humaine
            cursor.execute('''
                SELECT classification_source, human_verified FROM video WHERE id = ?
            ''', (video_id,))
            protection_check = cursor.fetchone()
            
            if protection_check and (protection_check[0] == 'human' or protection_check[1] == 1):
                human_protected_count += 1
                print(f"[DIRECT-CLASSIFY] üö´ PROTECTION HUMAINE: Vid√©o ID {video_id} ignor√©e (classification humaine)")
                continue
            
            # Classifier avec les mots-cl√©s configur√©s multilingues
            predicted_category, detected_language, confidence = classify_video_with_language(title, description)
            
            # üö® NOUVEAU : Mettre √† jour en base avec le tracking de source IA
            cursor.execute('''
                UPDATE video SET 
                    category = ?, 
                    classification_source = 'ai',
                    classification_date = CURRENT_TIMESTAMP,
                    classification_confidence = ?,
                    human_verified = 0,
                    last_updated = ? 
                WHERE id = ?
            ''', (predicted_category, confidence, datetime.now(), video_id))
            
            videos_classified += 1
            
            # Log seulement quelques exemples pour √©viter le spam
            if videos_classified <= 5:
                print(f"[DIRECT-CLASSIFY] ‚ú® {title} ‚Üí {predicted_category.upper()} ({detected_language}, {confidence}%) [IA]")
        
        conn.commit()
        
        if videos_classified > 5:
            print(f"[DIRECT-CLASSIFY] ‚ú® ... et {videos_classified - 5} vid√©os suppl√©mentaires classifi√©es [IA]")
        
        if human_protected_count > 0:
            print(f"[DIRECT-CLASSIFY] üõ°Ô∏è {human_protected_count} vid√©os prot√©g√©es par supervision humaine (non modifi√©es)")
        
        return {
            'success': True,
            'message': f'{videos_classified} vid√©os classifi√©es (IA), {human_protected_count} prot√©g√©es (humain)',
            'videos_classified': videos_classified,
            'human_protected': human_protected_count
        }
        
    except Exception as e:
        conn.rollback()
        print(f"[DIRECT-CLASSIFY] ‚ùå Erreur: {e}")
        return {
            'success': False,
            'error': str(e),
            'videos_classified': 0
        }
    finally:
        conn.close()

def calculate_publication_frequency_monthly(competitor_id: int = None, start_date: datetime = None, end_date: datetime = None) -> Dict:
    print('[DEBUG] >>> Entr√©e dans calculate_publication_frequency_monthly')
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        # Construire la requ√™te SQL avec support des vraies dates YouTube
        query = '''
            SELECT 
                c.id as competitor_id,
                c.name as competitor_name,
                c.country,
                c.created_at,
                v.published_at,
                v.youtube_published_at,
                v.category,
                v.view_count,
                v.like_count,
                v.comment_count,
                v.video_id
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            WHERE v.published_at IS NOT NULL
        '''
        
        params = []
        
        if competitor_id:
            query += " AND c.id = ?"
            params.append(competitor_id)
            
        if start_date:
            query += " AND COALESCE(v.youtube_published_at, v.published_at) >= ?"
            params.append(start_date)
            
        if end_date:
            query += " AND COALESCE(v.youtube_published_at, v.published_at) <= ?"
            params.append(end_date)
            
        query += " ORDER BY c.id, COALESCE(v.youtube_published_at, v.published_at)"
        
        cursor.execute(query, params)
        results = cursor.fetchall()
        
        # Organiser les donn√©es par concurrent
        competitors_data = {}
        
        for row in results:
            comp_id = row[0]
            comp_name = row[1]
            country = row[2]
            created_at = row[3]
            published_at = row[4]
            youtube_published_at = row[5]
            category = row[6] or 'hub'  # Default to hub if no category
            view_count = row[7] or 0
            like_count = row[8] or 0
            comment_count = row[9] or 0
            video_id = row[10]
            
            if comp_id not in competitors_data:
                competitors_data[comp_id] = {
                    'name': comp_name,
                    'country': country,
                    'created_at': created_at,
                    'videos': []
                }
            
            # Utiliser la vraie date YouTube si disponible, sinon la date d'importation
            try:
                if youtube_published_at:
                    if isinstance(youtube_published_at, str):
                        pub_date = datetime.fromisoformat(youtube_published_at.replace('Z', '+00:00'))
                    else:
                        pub_date = youtube_published_at
                    date_source = 'youtube'
                else:
                    if isinstance(published_at, str):
                        pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    else:
                        pub_date = published_at
                    date_source = 'import'
                    
                competitors_data[comp_id]['videos'].append({
                    'published_at': pub_date,
                    'category': category,
                    'view_count': view_count,
                    'like_count': like_count,
                    'comment_count': comment_count,
                    'video_id': video_id,
                    'date_source': date_source
                })
            except Exception as e:
                print(f"[FREQ] Erreur parsing date {youtube_published_at or published_at}: {e}")
                continue
        
        # DEBUG: Afficher le nombre de concurrents et de vid√©os trouv√©s
        print(f"[DEBUG] Concurrents trouv√©s: {len(competitors_data)}")
        for comp_id, data in competitors_data.items():
            print(f"[DEBUG] {comp_id} - {data['name']} - {len(data['videos'])} vid√©os")
        
        # Calculer les fr√©quences avec une approche r√©aliste
        frequency_results = {}
        
        for comp_id, competitor_data in competitors_data.items():
            videos = competitor_data['videos']
            if not videos:
                continue
            
            # D√©tecter le type de dates disponibles
            youtube_dates_count = sum(1 for v in videos if v.get('date_source') == 'youtube')
            has_real_dates = youtube_dates_count > 0
            
            if has_real_dates:
                print(f"[FREQ] {competitor_data['name']}: {youtube_dates_count}/{len(videos)} vraies dates YouTube disponibles")
                
                # Utiliser les vraies dates pour le calcul
                video_dates = [v['published_at'] for v in videos if v.get('date_source') == 'youtube']
                
                if len(video_dates) >= 2:
                    date_range = (max(video_dates) - min(video_dates)).total_seconds()
                    
                    # Si on a assez de vraies dates avec une vraie plage temporelle
                    if date_range > 86400 * 7:  # Plus d'une semaine d'√©cart
                        print(f"[FREQ] {competitor_data['name']}: Calcul avec vraies dates YouTube")
                        
                        # Grouper par semaine avec de vraies dates
                        weekly_data = defaultdict(lambda: {'total': 0, 'hero': 0, 'hub': 0, 'help': 0})
                        weekly_engagement = defaultdict(lambda: {'engagement_rates': [], 'views': [], 'avg_engagement': 0, 'avg_views': 0})
                        
                        for video in videos:
                            if video.get('date_source') == 'youtube':
                                pub_date = video['published_at']
                                # Utiliser le format ISO de la semaine (YYYY-WXX)
                                week_key = pub_date.strftime('%Y-W%U')
                                
                                # Compter les vid√©os par cat√©gorie
                                weekly_data[week_key]['total'] += 1
                                weekly_data[week_key][video['category']] += 1
                                
                                # Calculer l'engagement
                                if video['view_count'] > 0:
                                    engagement_rate = (video['like_count'] + video['comment_count']) / video['view_count'] * 100
                                    weekly_engagement[week_key]['engagement_rates'].append(engagement_rate)
                                    weekly_engagement[week_key]['views'].append(video['view_count'])
                        
                        # Calculer les moyennes d'engagement par semaine
                        for week_key, week_stats in weekly_engagement.items():
                            if week_stats['engagement_rates']:
                                week_stats['avg_engagement'] = sum(week_stats['engagement_rates']) / len(week_stats['engagement_rates'])
                            if week_stats['views']:
                                week_stats['avg_views'] = sum(week_stats['views']) / len(week_stats['views'])
                        
                        # Calculer les statistiques finales
                        weeks = sorted(weekly_data.keys())
                        total_weeks = len(weeks)
                        
                        if total_weeks > 0:
                            # Convertir en fr√©quence mensuelle (4.33 semaines par mois)
                            weeks_per_month = 4.33
                            avg_frequency = {
                                'total': sum(weekly_data[week]['total'] for week in weeks) / total_weeks * weeks_per_month,
                                'hero': sum(weekly_data[week]['hero'] for week in weeks) / total_weeks * weeks_per_month,
                                'hub': sum(weekly_data[week]['hub'] for week in weeks) / total_weeks * weeks_per_month,
                                'help': sum(weekly_data[week]['help'] for week in weeks) / total_weeks * weeks_per_month
                            }
                        else:
                            avg_frequency = {'total': 0, 'hero': 0, 'hub': 0, 'help': 0}
                        
                        # Engagement moyen global
                        all_engagement = []
                        for week_stats in weekly_engagement.values():
                            all_engagement.extend(week_stats['engagement_rates'])
                        
                        avg_engagement = sum(all_engagement) / len(all_engagement) if all_engagement else 0
                        
                        frequency_results[comp_id] = {
                            'name': competitor_data['name'],
                            'country': competitor_data['country'],
                            'avg_frequency': avg_frequency,
                            'total_weeks': total_weeks,
                            'total_videos': youtube_dates_count,
                            'avg_engagement': avg_engagement,
                            'calculation_method': 'real_youtube_dates',
                            'weekly_data': dict(weekly_data),
                            'weekly_engagement': dict(weekly_engagement),
                            'date_range': {
                                'start': weeks[0] if weeks else None,
                                'end': weeks[-1] if weeks else None
                            }
                        }
                        continue
            
            # Fallback : estimation comme avant
            print(f"[FREQ] {competitor_data['name']}: Dates d'importation d√©tect√©es, calcul estim√©")
            
            # D√©tecter si on a des dates d'importation vs des vraies dates de publication
            video_dates = [v['published_at'] for v in videos]
            date_range = (max(video_dates) - min(video_dates)).total_seconds()
            
            # Si toutes les vid√©os ont √©t√© import√©es le m√™me jour (moins de 24h d'√©cart)
            if date_range < 86400:  # 24 heures en secondes
                # Estimation r√©aliste bas√©e sur le nombre de vid√©os
                total_videos = len(videos)
                
                # Estimation : la plupart des cha√Ænes publient entre 1-10 vid√©os par mois
                # Calcul bas√© sur une p√©riode estim√©e raisonnable
                if total_videos <= 50:
                    estimated_weeks = 52  # 1 an
                elif total_videos <= 200:
                    estimated_weeks = 104  # 2 ans
                elif total_videos <= 500:
                    estimated_weeks = 156  # 3 ans
                else:
                    estimated_weeks = 208  # 4 ans
                
                # Calcul des fr√©quences par cat√©gorie
                category_counts = {'total': 0, 'hero': 0, 'hub': 0, 'help': 0}
                engagement_data = []
                
                for video in videos:
                    category_counts['total'] += 1
                    category_counts[video['category']] += 1
                    
                    # Calculer l'engagement
                    if video['view_count'] > 0:
                        engagement_rate = (video['like_count'] + video['comment_count']) / video['view_count'] * 100
                        engagement_data.append({
                            'rate': engagement_rate,
                            'views': video['view_count'],
                            'category': video['category']
                        })
                
                # Fr√©quence mensuelle estim√©e (4.33 semaines par mois)
                weeks_per_month = 4.33
                avg_frequency = {
                    'total': category_counts['total'] / estimated_weeks * weeks_per_month,
                    'hero': category_counts['hero'] / estimated_weeks * weeks_per_month,
                    'hub': category_counts['hub'] / estimated_weeks * weeks_per_month,
                    'help': category_counts['help'] / estimated_weeks * weeks_per_month
                }
                
                # Engagement moyen
                avg_engagement = sum(e['rate'] for e in engagement_data) / len(engagement_data) if engagement_data else 0
                
                # Cr√©er des donn√©es de semaine factices pour la compatibilit√©
                weekly_data = {}
                weekly_engagement = {}
                
                # Cr√©er des semaines factices r√©parties sur la p√©riode estim√©e
                for week_idx in range(min(estimated_weeks, 52)):  # Limiter √† 52 semaines pour √©viter trop de donn√©es
                    week_key = f"2023-W{week_idx:02d}"  # Format de semaine factice
                    weekly_data[week_key] = {
                        'total': category_counts['total'] / estimated_weeks,
                        'hero': category_counts['hero'] / estimated_weeks,
                        'hub': category_counts['hub'] / estimated_weeks,
                        'help': category_counts['help'] / estimated_weeks
                    }
                    weekly_engagement[week_key] = {
                        'avg_engagement': avg_engagement,
                        'avg_views': sum(e['views'] for e in engagement_data) / len(engagement_data) if engagement_data else 0,
                        'engagement_rates': [avg_engagement],
                        'views': [sum(e['views'] for e in engagement_data) / len(engagement_data) if engagement_data else 0]
                    }
                
                frequency_results[comp_id] = {
                    'name': competitor_data['name'],
                    'country': competitor_data['country'],
                    'avg_frequency': avg_frequency,
                    'total_weeks': estimated_weeks,
                    'total_videos': total_videos,
                    'avg_engagement': avg_engagement,
                    'calculation_method': 'estimated',
                    'weekly_data': weekly_data,
                    'weekly_engagement': weekly_engagement,
                    'date_range': {
                        'start': f"Estim√©: {estimated_weeks} semaines",
                        'end': "Maintenant"
                    }
                }
        
        # DEBUG: Afficher les concurrents inclus dans frequency_results
        print(f"[DEBUG] Concurrents inclus dans frequency_results: {list(frequency_results.keys())}")
        print('[DEBUG] <<< Fin calculate_publication_frequency_monthly (return frequency_results)')
        return frequency_results
    except Exception as e:
        print(f"[FREQ] Erreur calcul fr√©quence mensuelle: {e}")
        print('[DEBUG] <<< Fin calculate_publication_frequency_monthly (return {})')
        return {}
    finally:
        conn.close()

# Garder l'ancienne fonction pour compatibilit√© mais utiliser la nouvelle
def calculate_publication_frequency(competitor_id: int = None, start_date: datetime = None, end_date: datetime = None) -> Dict:
    """
    Calcule la fr√©quence de publication par mois (nouvelle version corrig√©e)
    """
    return calculate_publication_frequency_monthly(competitor_id, start_date, end_date)

def calculate_frequency_by_country() -> Dict:
    """
    Calcule la fr√©quence de publication agr√©g√©e par pays
    """
    frequency_data = calculate_publication_frequency()
    
    country_stats = defaultdict(lambda: {
        'competitors': [],
        'avg_frequency': {'total': 0, 'hero': 0, 'hub': 0, 'help': 0},
        'total_videos': 0,
        'total_weeks': 0
    })
    
    for comp_id, data in frequency_data.items():
        country = data['country'] or 'Unknown'
        
        country_stats[country]['competitors'].append({
            'id': comp_id,
            'name': data['name'],
            'frequency': data['avg_frequency']
        })
        
        # Agr√©gation
        for category in ['total', 'hero', 'hub', 'help']:
            country_stats[country]['avg_frequency'][category] += data['avg_frequency'][category]
        
        country_stats[country]['total_weeks'] += data['total_weeks']
    
    # Calculer les moyennes par pays
    for country, stats in country_stats.items():
        competitor_count = len(stats['competitors'])
        if competitor_count > 0:
            for category in ['total', 'hero', 'hub', 'help']:
                stats['avg_frequency'][category] /= competitor_count
    
    return dict(country_stats)

def get_frequency_evolution_data(competitor_id: int, period_weeks: int = 12) -> Dict:
    """
    R√©cup√®re les donn√©es d'√©volution de la fr√©quence sur une p√©riode donn√©e
    
    Args:
        competitor_id: ID du concurrent
        period_weeks: Nombre de semaines √† analyser
        
    Returns:
        Dict avec les donn√©es d'√©volution
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(weeks=period_weeks)
    
    frequency_data = calculate_publication_frequency(competitor_id, start_date, end_date)
    
    if competitor_id not in frequency_data:
        return {}
    
    data = frequency_data[competitor_id]
    
    # Pr√©parer les donn√©es pour le graphique avec gestion d'erreurs
    try:
        # V√©rifier si les donn√©es n√©cessaires existent
        if 'weekly_data' not in data or 'weekly_engagement' not in data:
            print(f"[FREQ-EVOLUTION] Donn√©es manquantes pour {competitor_id}")
            return {
                'error': 'Donn√©es de fr√©quence hebdomadaire manquantes',
                'competitor_name': data.get('name', f'Competitor {competitor_id}'),
                'country': data.get('country', 'Unknown')
            }
        
        weeks = sorted(data['weekly_data'].keys())
        
        # V√©rifier que nous avons des donn√©es
        if not weeks:
            return {
                'error': 'Aucune donn√©e hebdomadaire disponible',
                'competitor_name': data.get('name', f'Competitor {competitor_id}'),
                'country': data.get('country', 'Unknown')
            }
        
        evolution_data = {
            'weeks': weeks,
            'total_videos': [],
            'hero_videos': [],
            'hub_videos': [],
            'help_videos': [],
            'engagement_rates': [],
            'avg_views': [],
            'competitor_name': data.get('name', f'Competitor {competitor_id}'),
            'country': data.get('country', 'Unknown')
        }
        
        # Remplir les donn√©es avec gestion d'erreurs
        for week in weeks:
            try:
                # Donn√©es de publication
                week_data = data['weekly_data'].get(week, {})
                evolution_data['total_videos'].append(week_data.get('total', 0))
                evolution_data['hero_videos'].append(week_data.get('hero', 0))
                evolution_data['hub_videos'].append(week_data.get('hub', 0))
                evolution_data['help_videos'].append(week_data.get('help', 0))
                
                # Donn√©es d'engagement
                eng_data = data['weekly_engagement'].get(week, {})
                evolution_data['engagement_rates'].append(eng_data.get('avg_engagement', 0))
                evolution_data['avg_views'].append(eng_data.get('avg_views', 0))
                
            except (KeyError, TypeError) as e:
                print(f"[FREQ-EVOLUTION] Erreur traitement semaine {week}: {e}")
                # Ajouter des valeurs par d√©faut
                evolution_data['total_videos'].append(0)
                evolution_data['hero_videos'].append(0)
                evolution_data['hub_videos'].append(0)
                evolution_data['help_videos'].append(0)
                evolution_data['engagement_rates'].append(0)
                evolution_data['avg_views'].append(0)
        
        return evolution_data
        
    except Exception as e:
        print(f"[FREQ-EVOLUTION] Erreur g√©n√©rale pour {competitor_id}: {e}")
        return {
            'error': f'Erreur lors du traitement des donn√©es: {str(e)}',
            'competitor_name': data.get('name', f'Competitor {competitor_id}'),
            'country': data.get('country', 'Unknown')
        }

def analyze_frequency_impact_on_engagement() -> Dict:
    """
    Analyse l'impact de la fr√©quence de publication sur l'engagement
    """
    frequency_data = calculate_publication_frequency()
    
    analysis = {
        'high_frequency_performers': [],
        'low_frequency_performers': [],
        'optimal_frequency_insights': {},
        'category_insights': {
            'hero': {'avg_frequency': 0, 'avg_engagement': 0, 'competitors': 0},
            'hub': {'avg_frequency': 0, 'avg_engagement': 0, 'competitors': 0},
            'help': {'avg_frequency': 0, 'avg_engagement': 0, 'competitors': 0}
        }
    }
    
    all_competitors = []
    
    for comp_id, data in frequency_data.items():
        # Calculer l'engagement moyen avec gestion d'erreurs
        all_engagement = []
        try:
            # V√©rifier si weekly_engagement existe
            if 'weekly_engagement' in data and data['weekly_engagement']:
                for week_data in data['weekly_engagement'].values():
                    if isinstance(week_data, dict) and 'avg_engagement' in week_data:
                        if week_data['avg_engagement'] > 0:
                            all_engagement.append(week_data['avg_engagement'])
            
            # Fallback sur avg_engagement si disponible
            if not all_engagement and 'avg_engagement' in data:
                if data['avg_engagement'] > 0:
                    all_engagement.append(data['avg_engagement'])
        except (KeyError, TypeError, AttributeError) as e:
            print(f"[FREQ-IMPACT] Erreur acc√®s weekly_engagement pour {comp_id}: {e}")
            # Utiliser avg_engagement comme fallback
            if 'avg_engagement' in data and data['avg_engagement'] > 0:
                all_engagement.append(data['avg_engagement'])
        
        avg_engagement = sum(all_engagement) / len(all_engagement) if all_engagement else 0
        
        # V√©rifier les autres champs avec gestion d'erreurs
        try:
            competitor_analysis = {
                'id': comp_id,
                'name': data.get('name', f'Competitor {comp_id}'),
                'country': data.get('country', 'Unknown'),
                'avg_frequency': data.get('avg_frequency', {'total': 0, 'hero': 0, 'hub': 0, 'help': 0}),
                'avg_engagement': avg_engagement,
                'total_weeks': data.get('total_weeks', 0)
            }
            
            all_competitors.append(competitor_analysis)
            
            # Analyser par cat√©gorie avec gestion d'erreurs
            if 'avg_frequency' in data and isinstance(data['avg_frequency'], dict):
                for category in ['hero', 'hub', 'help']:
                    if data['avg_frequency'].get(category, 0) > 0:
                        analysis['category_insights'][category]['avg_frequency'] += data['avg_frequency'][category]
                        analysis['category_insights'][category]['avg_engagement'] += avg_engagement
                        analysis['category_insights'][category]['competitors'] += 1
        except (KeyError, TypeError) as e:
            print(f"[FREQ-IMPACT] Erreur traitement donn√©es pour {comp_id}: {e}")
            continue
    
    # Calculer les moyennes par cat√©gorie
    for category, insights in analysis['category_insights'].items():
        if insights['competitors'] > 0:
            insights['avg_frequency'] /= insights['competitors']
            insights['avg_engagement'] /= insights['competitors']
    
    # Identifier les performers
    all_competitors.sort(key=lambda x: x['avg_engagement'], reverse=True)
    
    # Top performers (haut engagement)
    top_quarter = len(all_competitors) // 4
    analysis['high_frequency_performers'] = all_competitors[:top_quarter]
    
    # Bottom performers (bas engagement)
    analysis['low_frequency_performers'] = all_competitors[-top_quarter:]
    
    # Insights sur la fr√©quence optimale
    high_freq_avg = sum(c['avg_frequency']['total'] for c in analysis['high_frequency_performers']) / len(analysis['high_frequency_performers']) if analysis['high_frequency_performers'] else 0
    low_freq_avg = sum(c['avg_frequency']['total'] for c in analysis['low_frequency_performers']) / len(analysis['low_frequency_performers']) if analysis['low_frequency_performers'] else 0
    
    analysis['optimal_frequency_insights'] = {
        'high_performers_avg_frequency': high_freq_avg,
        'low_performers_avg_frequency': low_freq_avg,
        'frequency_impact': 'positive' if high_freq_avg > low_freq_avg else 'negative',
        'recommendation': f"Les performers avec un engagement √©lev√© publient en moyenne {high_freq_avg:.1f} vid√©os par semaine vs {low_freq_avg:.1f} pour les plus faibles"
    }
    
    return analysis

def analyze_category_frequency_patterns() -> Dict:
    """
    Analyse approfondie des patterns de fr√©quence par cat√©gorie HERO, HUB, HELP
    """
    frequency_data = calculate_publication_frequency()
    
    # Regrouper les donn√©es par cat√©gorie
    category_analysis = {
        'hero': {'frequencies': [], 'engagement': [], 'competitors': []},
        'hub': {'frequencies': [], 'engagement': [], 'competitors': []},
        'help': {'frequencies': [], 'engagement': [], 'competitors': []}
    }
    
    for comp_id, data in frequency_data.items():
        competitor_name = data.get('name', f'Competitor {comp_id}')
        country = data.get('country', 'Unknown')
        
        # Calculer l'engagement moyen pour ce concurrent avec gestion d'erreurs
        all_engagement = []
        try:
            # V√©rifier si weekly_engagement existe
            if 'weekly_engagement' in data and data['weekly_engagement']:
                for week_data in data['weekly_engagement'].values():
                    if isinstance(week_data, dict) and 'avg_engagement' in week_data:
                        if week_data['avg_engagement'] > 0:
                            all_engagement.append(week_data['avg_engagement'])
            
            # Fallback sur avg_engagement si disponible
            if not all_engagement and 'avg_engagement' in data:
                if data['avg_engagement'] > 0:
                    all_engagement.append(data['avg_engagement'])
        except (KeyError, TypeError, AttributeError) as e:
            print(f"[CATEGORY-FREQ] Erreur acc√®s weekly_engagement pour {comp_id}: {e}")
            # Utiliser avg_engagement comme fallback
            if 'avg_engagement' in data and data['avg_engagement'] > 0:
                all_engagement.append(data['avg_engagement'])
        
        avg_engagement = sum(all_engagement) / len(all_engagement) if all_engagement else 0
        
        # Analyser chaque cat√©gorie avec gestion d'erreurs
        try:
            if 'avg_frequency' in data and isinstance(data['avg_frequency'], dict):
                for category in ['hero', 'hub', 'help']:
                    freq = data['avg_frequency'].get(category, 0)
                    if freq > 0:  # Seulement si le concurrent publie cette cat√©gorie
                        category_analysis[category]['frequencies'].append(freq)
                        category_analysis[category]['engagement'].append(avg_engagement)
                        category_analysis[category]['competitors'].append({
                            'id': comp_id,
                            'name': competitor_name,
                            'country': country,
                            'frequency': freq,
                            'engagement': avg_engagement
                        })
        except (KeyError, TypeError) as e:
            print(f"[CATEGORY-FREQ] Erreur traitement donn√©es pour {comp_id}: {e}")
            continue
    
    # Calculer les statistiques par cat√©gorie
    insights = {}
    
    for category, data in category_analysis.items():
        frequencies = data['frequencies']
        engagements = data['engagement']
        competitors = data['competitors']
        
        if not frequencies:
            insights[category] = {
                'status': 'no_data',
                'message': f'Aucune donn√©e disponible pour la cat√©gorie {category.upper()}'
            }
            continue
        
        # Statistiques de base
        avg_frequency = sum(frequencies) / len(frequencies)
        min_frequency = min(frequencies)
        max_frequency = max(frequencies)
        avg_engagement = sum(engagements) / len(engagements)
        
        # Quartiles
        sorted_frequencies = sorted(frequencies)
        n = len(sorted_frequencies)
        q1 = sorted_frequencies[n // 4] if n >= 4 else min_frequency
        q3 = sorted_frequencies[3 * n // 4] if n >= 4 else max_frequency
        
        # Top performers
        top_performers = sorted(competitors, key=lambda x: x['engagement'], reverse=True)[:5]
        
        # Analyse des patterns
        patterns = []
        
        # Pattern 1: Fr√©quence vs Engagement
        if len(frequencies) >= 3:
            # Calculer la corr√©lation
            import statistics
            freq_mean = statistics.mean(frequencies)
            eng_mean = statistics.mean(engagements)
            
            numerator = sum((f - freq_mean) * (e - eng_mean) for f, e in zip(frequencies, engagements))
            freq_variance = sum((f - freq_mean) ** 2 for f in frequencies)
            eng_variance = sum((e - eng_mean) ** 2 for e in engagements)
            
            if freq_variance > 0 and eng_variance > 0:
                correlation = numerator / (freq_variance * eng_variance) ** 0.5
                
                if correlation > 0.3:
                    patterns.append("Plus de fr√©quence = Plus d'engagement")
                elif correlation < -0.3:
                    patterns.append("Fr√©quence √©lev√©e = Engagement diminu√©")
                else:
                    patterns.append("Pas de corr√©lation claire fr√©quence/engagement")
        
        # Pattern 2: Recommandations sp√©cifiques par cat√©gorie
        if category == 'hero':
            if avg_frequency < 0.5:
                patterns.append("HERO: Fr√©quence trop faible - viser 1-2 vid√©os/mois")
            elif avg_frequency > 2.0:
                patterns.append("HERO: Fr√©quence trop √©lev√©e - dilue l'impact")
            else:
                patterns.append("HERO: Fr√©quence √©quilibr√©e")
        
        elif category == 'hub':
            if avg_frequency < 1.0:
                patterns.append("HUB: Augmenter la fr√©quence - viser 1-2 vid√©os/semaine")
            elif avg_frequency > 3.0:
                patterns.append("HUB: Fr√©quence √©lev√©e - attention √† la qualit√©")
            else:
                patterns.append("HUB: Fr√©quence optimale pour fid√©liser")
        
        elif category == 'help':
            if avg_frequency < 0.5:
                patterns.append("HELP: Manque de contenu utilitaire")
            elif avg_frequency > 1.5:
                patterns.append("HELP: Excellente fr√©quence pour l'utilit√©")
            else:
                patterns.append("HELP: Fr√©quence correcte")
        
        # Pattern 3: Analyse des √©carts
        if max_frequency > avg_frequency * 2:
            patterns.append("√âcarts importants entre concurrents")
        
        # Regrouper par pays
        country_breakdown = {}
        for comp in competitors:
            country = comp['country'] or 'Unknown'
            if country not in country_breakdown:
                country_breakdown[country] = {'count': 0, 'avg_freq': 0, 'total_freq': 0}
            country_breakdown[country]['count'] += 1
            country_breakdown[country]['total_freq'] += comp['frequency']
        
        for country, data in country_breakdown.items():
            data['avg_freq'] = data['total_freq'] / data['count']
        
        # Identifier les pays leaders
        country_leaders = sorted(
            country_breakdown.items(), 
            key=lambda x: x[1]['avg_freq'], 
            reverse=True
        )[:3]
        
        insights[category] = {
            'status': 'success',
            'statistics': {
                'avg_frequency': round(avg_frequency, 2),
                'min_frequency': round(min_frequency, 2),
                'max_frequency': round(max_frequency, 2),
                'avg_engagement': round(avg_engagement, 2),
                'quartiles': {
                    'q1': round(q1, 2),
                    'q3': round(q3, 2)
                },
                'total_competitors': len(competitors)
            },
            'top_performers': top_performers,
            'patterns': patterns,
            'country_breakdown': dict(country_breakdown),
            'country_leaders': country_leaders,
            'recommendations': _generate_category_recommendations(category, avg_frequency, avg_engagement, patterns)
        }
    
    # Analyse comparative entre cat√©gories
    comparative_analysis = {
        'category_ranking': [],
        'balance_analysis': {},
        'strategic_insights': []
    }
    
    # Classer les cat√©gories par fr√©quence moyenne
    category_freq_ranking = []
    for category in ['hero', 'hub', 'help']:
        if insights[category]['status'] == 'success':
            category_freq_ranking.append({
                'category': category,
                'avg_frequency': insights[category]['statistics']['avg_frequency'],
                'avg_engagement': insights[category]['statistics']['avg_engagement']
            })
    
    category_freq_ranking.sort(key=lambda x: x['avg_frequency'], reverse=True)
    comparative_analysis['category_ranking'] = category_freq_ranking
    
    # Analyser l'√©quilibre
    if len(category_freq_ranking) >= 2:
        hub_freq = next((c['avg_frequency'] for c in category_freq_ranking if c['category'] == 'hub'), 0)
        hero_freq = next((c['avg_frequency'] for c in category_freq_ranking if c['category'] == 'hero'), 0)
        help_freq = next((c['avg_frequency'] for c in category_freq_ranking if c['category'] == 'help'), 0)
        
        total_freq = hub_freq + hero_freq + help_freq
        
        if total_freq > 0:
            balance = {
                'hub_percent': round(hub_freq / total_freq * 100, 1),
                'hero_percent': round(hero_freq / total_freq * 100, 1),
                'help_percent': round(help_freq / total_freq * 100, 1)
            }
            
            comparative_analysis['balance_analysis'] = balance
            
            # Insights strat√©giques
            if balance['hub_percent'] > 60:
                comparative_analysis['strategic_insights'].append("Strat√©gie HUB dominante - excellente fid√©lisation")
            elif balance['hero_percent'] > 40:
                comparative_analysis['strategic_insights'].append("Strat√©gie HERO aggressive - focus visibilit√©")
            elif balance['help_percent'] > 30:
                comparative_analysis['strategic_insights'].append("Strat√©gie HELP forte - positionnement expert")
            
            # Recommandations d'√©quilibre
            if balance['help_percent'] < 15:
                comparative_analysis['strategic_insights'].append("Manque de contenu HELP - opportunit√© SEO")
            if balance['hero_percent'] < 10:
                comparative_analysis['strategic_insights'].append("Manque de contenu HERO - visibilit√© limit√©e")
    
    return {
        'category_insights': insights,
        'comparative_analysis': comparative_analysis,
        'summary': {
            'total_categories_analyzed': len([c for c in insights.values() if c['status'] == 'success']),
            'most_frequent_category': category_freq_ranking[0]['category'] if category_freq_ranking else None,
            'highest_engagement_category': max(category_freq_ranking, key=lambda x: x['avg_engagement'])['category'] if category_freq_ranking else None
        }
    }

def _generate_category_recommendations(category: str, avg_frequency: float, avg_engagement: float, patterns: List[str]) -> List[str]:
    """
    G√©n√®re des recommandations sp√©cifiques par cat√©gorie
    """
    recommendations = []
    
    if category == 'hero':
        if avg_frequency < 0.3:
            recommendations.append("Cr√©er plus de contenu HERO premium (campagnes, √©v√©nements)")
        elif avg_frequency > 1.5:
            recommendations.append("R√©duire la fr√©quence HERO pour maximiser l'impact")
        
        if avg_engagement < 2.0:
            recommendations.append("Am√©liorer la qualit√© du contenu HERO pour plus d'engagement")
        
        recommendations.append("Planifier les HERO autour d'√©v√©nements et de saisons")
    
    elif category == 'hub':
        if avg_frequency < 0.8:
            recommendations.append("Augmenter la fr√©quence HUB pour fid√©liser l'audience")
        elif avg_frequency > 4.0:
            recommendations.append("Optimiser la fr√©quence HUB pour √©viter la saturation")
        
        if avg_engagement < 1.5:
            recommendations.append("Am√©liorer l'interaction dans le contenu HUB")
        
        recommendations.append("Maintenir la r√©gularit√© pour cr√©er une habitude")
    
    elif category == 'help':
        if avg_frequency < 0.4:
            recommendations.append("Cr√©er plus de contenu HELP pour le r√©f√©rencement")
        
        if avg_engagement < 1.0:
            recommendations.append("Optimiser le contenu HELP pour r√©pondre aux vraies questions")
        
        recommendations.append("Utiliser les recherches populaires pour guider le contenu HELP")
    
    # Ajouter des recommandations bas√©es sur les patterns d√©tect√©s
    for pattern in patterns:
        if "Plus de fr√©quence = Plus d'engagement" in pattern:
            recommendations.append(f"Augmenter progressivement la fr√©quence {category.upper()}")
        elif "Fr√©quence √©lev√©e = Engagement diminu√©" in pattern:
            recommendations.append(f"R√©duire la fr√©quence {category.upper()} et focus qualit√©")
    
    return recommendations[:4]  # Limiter √† 4 recommandations

def update_database_schema():
    """Mettre √† jour le sch√©ma de la base de donn√©es avec les nouveaux champs de tagging"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si les nouveaux champs existent d√©j√† dans la table concurrent
        cursor.execute("PRAGMA table_info(concurrent)")
        columns = [column[1] for column in cursor.fetchall()]
        
        print(f"[DB-UPDATE] Colonnes actuelles concurrent: {columns}")
        
        # Ajouter les nouveaux champs si ils n'existent pas
        new_columns = [
            ('sector', 'VARCHAR(100)', 'hospitality'),  # Secteur principal
            ('tags', 'TEXT', ''),  # Tags personnalis√©s (JSON)
            ('custom_region', 'VARCHAR(100)', ''),  # R√©gion personnalis√©e
            ('notes', 'TEXT', ''),  # Notes libres
            ('is_active', 'BOOLEAN', 1)  # Statut actif/inactif
        ]
        
        for column_name, column_type, default_value in new_columns:
            if column_name not in columns:
                print(f"[DB-UPDATE] Ajout de la colonne {column_name}")
                if column_name == 'is_active':
                    cursor.execute(f'ALTER TABLE concurrent ADD COLUMN {column_name} {column_type} DEFAULT {default_value}')
                elif column_name == 'sector':
                    cursor.execute(f'ALTER TABLE concurrent ADD COLUMN {column_name} {column_type} DEFAULT "hospitality"')
                else:
                    cursor.execute(f'ALTER TABLE concurrent ADD COLUMN {column_name} {column_type} DEFAULT ""')
        
        # üö® NOUVEAU : V√©rifier et ajouter les champs de tracking de classification dans la table video
        cursor.execute("PRAGMA table_info(video)")
        video_columns = [column[1] for column in cursor.fetchall()]
        
        print(f"[DB-UPDATE] Colonnes actuelles video: {video_columns}")
        
        # Ajouter les champs de tracking de classification
        classification_tracking_columns = [
            ('classification_source', 'VARCHAR(20)', 'ai'),  # 'human' ou 'ai'
            ('classification_date', 'DATETIME', None),  # Date de classification
            ('classification_confidence', 'INTEGER', 50),  # Score de confiance (0-100)
            ('human_verified', 'BOOLEAN', 0)  # 1 si v√©rifi√© par un humain
        ]
        
        for column_name, column_type, default_value in classification_tracking_columns:
            if column_name not in video_columns:
                print(f"[DB-UPDATE] Ajout de la colonne video.{column_name}")
                if column_name == 'human_verified':
                    cursor.execute(f'ALTER TABLE video ADD COLUMN {column_name} {column_type} DEFAULT {default_value}')
                elif column_name == 'classification_confidence':
                    cursor.execute(f'ALTER TABLE video ADD COLUMN {column_name} {column_type} DEFAULT {default_value}')
                elif column_name == 'classification_source':
                    cursor.execute(f'ALTER TABLE video ADD COLUMN {column_name} {column_type} DEFAULT "ai"')
                elif column_name == 'classification_date':
                    cursor.execute(f'ALTER TABLE video ADD COLUMN {column_name} {column_type}')
        
        # üö® NOUVEAU : V√©rifier et ajouter les champs de tracking de classification dans la table playlist
        cursor.execute("PRAGMA table_info(playlist)")
        playlist_columns = [column[1] for column in cursor.fetchall()]
        
        print(f"[DB-UPDATE] Colonnes actuelles playlist: {playlist_columns}")
        
        for column_name, column_type, default_value in classification_tracking_columns:
            if column_name not in playlist_columns:
                print(f"[DB-UPDATE] Ajout de la colonne playlist.{column_name}")
                if column_name == 'human_verified':
                    cursor.execute(f'ALTER TABLE playlist ADD COLUMN {column_name} {column_type} DEFAULT {default_value}')
                elif column_name == 'classification_confidence':
                    cursor.execute(f'ALTER TABLE playlist ADD COLUMN {column_name} {column_type} DEFAULT {default_value}')
                elif column_name == 'classification_source':
                    cursor.execute(f'ALTER TABLE playlist ADD COLUMN {column_name} {column_type} DEFAULT "ai"')
                elif column_name == 'classification_date':
                    cursor.execute(f'ALTER TABLE playlist ADD COLUMN {column_name} {column_type}')
        
        # Mettre √† jour tous les concurrents existants avec des valeurs par d√©faut
        cursor.execute("""
            UPDATE concurrent 
            SET sector = 'hospitality'
            WHERE sector IS NULL OR sector = ''
        """)
        
        # üö® NOUVEAU : Marquer toutes les classifications existantes comme √©tant de l'IA (par d√©faut)
        cursor.execute("""
            UPDATE video 
            SET classification_source = 'ai', 
                classification_date = datetime('now'),
                classification_confidence = 50,
                human_verified = 0
            WHERE classification_source IS NULL
        """)
        
        cursor.execute("""
            UPDATE playlist 
            SET classification_source = 'ai', 
                classification_date = datetime('now'),
                classification_confidence = 50,
                human_verified = 0
            WHERE classification_source IS NULL
        """)
        
        conn.commit()
        print("[DB-UPDATE] ‚úÖ Sch√©ma de base de donn√©es mis √† jour avec succ√®s (tracking humain/IA ajout√©)")
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"[DB-UPDATE] ‚ùå Erreur lors de la mise √† jour du sch√©ma: {e}")
        return False
    finally:
        conn.close()

def update_competitor_tags(competitor_id: int, tags_data: Dict) -> bool:
    """
    Mettre √† jour les tags d'un concurrent
    
    Args:
        competitor_id: ID du concurrent
        tags_data: Dict avec les nouveaux tags
        
    Returns:
        bool: True si succ√®s
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # Construire la requ√™te de mise √† jour
        updates = []
        values = []
        
        if 'sector' in tags_data:
            updates.append('sector = ?')
            values.append(tags_data['sector'])
            
        if 'tags' in tags_data:
            updates.append('tags = ?')
            values.append(tags_data['tags'])
            
        if 'custom_region' in tags_data:
            updates.append('custom_region = ?')
            values.append(tags_data['custom_region'])
            
        if 'notes' in tags_data:
            updates.append('notes = ?')
            values.append(tags_data['notes'])
            
        if 'is_active' in tags_data:
            updates.append('is_active = ?')
            values.append(tags_data['is_active'])
        
        if updates:
            updates.append('last_updated = ?')
            values.append(datetime.now())
            values.append(competitor_id)
            
            query = f"UPDATE concurrent SET {', '.join(updates)} WHERE id = ?"
            cursor.execute(query, values)
            
            conn.commit()
            print(f"[DB-UPDATE] Tags mis √† jour pour le concurrent {competitor_id}")
            return True
        
        return False
        
    except Exception as e:
        conn.rollback()
        print(f"[DB-UPDATE] Erreur lors de la mise √† jour des tags: {e}")
        return False
    finally:
        conn.close()

def get_competitor_by_id(competitor_id: int) -> Dict:
    """R√©cup√©rer un concurrent par son ID avec tous ses champs"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        cursor.execute('SELECT * FROM concurrent WHERE id = ?', (competitor_id,))
        row = cursor.fetchone()
        
        if row:
            return dict(row)
        return None
        
    finally:
        conn.close()

def get_inconsistency_stats() -> Dict:
    """R√©cup√®re les statistiques d'incoh√©rences g√©n√©rales"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # Compter les concurrents
        cursor.execute("SELECT COUNT(*) FROM concurrent")
        total_competitors = cursor.fetchone()[0]
        
        # Compter les vid√©os
        cursor.execute("SELECT COUNT(*) FROM video")
        total_videos = cursor.fetchone()[0]
        
        # Simuler le nombre d'erreurs (sera calcul√© lors de la d√©tection)
        # Pour l'instant, on retourne 0 ou un nombre bas√© sur des heuristiques simples
        cursor.execute("""
            SELECT COUNT(*) FROM video 
            WHERE view_count IS NULL OR view_count < 0 
            OR duration_seconds IS NULL OR duration_seconds < 0
            OR published_at IS NULL
        """)
        basic_errors = cursor.fetchone()[0]
        
        return {
            'total_competitors': total_competitors,
            'total_videos': total_videos,
            'total_errors': basic_errors,
            'last_check': datetime.now().strftime('%Y-%m-%d %H:%M')
        }
        
    except Exception as e:
        print(f"[INCONSISTENCY-STATS] Erreur: {e}")
        return {
            'total_competitors': 0,
            'total_videos': 0,
            'total_errors': 0,
            'last_check': 'Erreur'
        }
    finally:
        conn.close()

def detect_data_inconsistencies(analysis_type: str = 'all') -> List[Dict]:
    """D√©tecte les incoh√©rences dans les donn√©es"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    inconsistencies = []
    
    try:
        print(f"[INCONSISTENCY] üîç Analyse de type: {analysis_type}")
        
        if analysis_type in ['all', 'missing']:
            inconsistencies.extend(_detect_missing_data(cursor))
        
        if analysis_type in ['all', 'aberrant']:
            inconsistencies.extend(_detect_aberrant_values(cursor))
        
        if analysis_type in ['all', 'temporal']:
            inconsistencies.extend(_detect_temporal_inconsistencies(cursor))
        
        if analysis_type in ['all', 'duplicates']:
            inconsistencies.extend(_detect_duplicates(cursor))
        
        if analysis_type in ['all', 'format']:
            inconsistencies.extend(_detect_format_errors(cursor))
        
        print(f"[INCONSISTENCY] ‚úÖ D√©tect√© {len(inconsistencies)} incoh√©rence(s)")
        return inconsistencies
        
    except Exception as e:
        print(f"[INCONSISTENCY] Erreur lors de la d√©tection: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        conn.close()

def _detect_missing_data(cursor) -> List[Dict]:
    """D√©tecte les donn√©es manquantes"""
    inconsistencies = []
    
    # Vid√©os sans vues
    cursor.execute("""
        SELECT v.title, c.name, v.id 
        FROM video v 
        JOIN concurrent c ON v.concurrent_id = c.id 
        WHERE v.view_count IS NULL OR v.view_count < 0
        LIMIT 50
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'missing',
            'severity': 'high',
            'message': 'Nombre de vues manquant ou invalide',
            'description': f'La vid√©o "{row[0]}" n\'a pas de donn√©es de vues valides',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[2],
            'fix_suggestion': 'V√©rifier la source de donn√©es ou re-scraper'
        })
    
    # Vid√©os sans dur√©e
    cursor.execute("""
        SELECT v.title, c.name, v.id 
        FROM video v 
        JOIN concurrent c ON v.concurrent_id = c.id 
        WHERE v.duration_seconds IS NULL OR v.duration_seconds <= 0
        LIMIT 50
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'missing',
            'severity': 'medium',
            'message': 'Dur√©e de vid√©o manquante',
            'description': f'La vid√©o "{row[0]}" n\'a pas de dur√©e d√©finie',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[2],
            'fix_suggestion': 'V√©rifier les m√©tadonn√©es YouTube'
        })
    
    # Concurrents sans pays
    cursor.execute("""
        SELECT name, id FROM concurrent 
        WHERE country IS NULL OR country = ''
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'missing',
            'severity': 'medium',
            'message': 'Pays manquant',
            'description': f'Le concurrent "{row[0]}" n\'a pas de pays d√©fini',
            'competitor_name': row[0],
            'competitor_id': row[1],
            'fix_suggestion': 'Ajouter le pays manuellement'
        })
    
    return inconsistencies

def _detect_aberrant_values(cursor) -> List[Dict]:
    """D√©tecte les valeurs aberrantes"""
    inconsistencies = []
    
    # Vid√©os avec trop de vues par rapport √† la moyenne
    cursor.execute("""
        SELECT v.title, c.name, v.view_count, v.id,
               AVG(v2.view_count) as avg_views
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        JOIN video v2 ON v2.concurrent_id = c.id
        WHERE v.view_count > 0
        GROUP BY v.id, v.title, c.name, v.view_count
        HAVING v.view_count > avg_views * 10
        ORDER BY v.view_count DESC
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'aberrant',
            'severity': 'low',
            'message': 'Vues exceptionnellement √©lev√©es',
            'description': f'La vid√©o "{row[0]}" a {row[2]:,} vues, soit 10x+ la moyenne du concurrent',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[3],
            'fix_suggestion': 'V√©rifier si c\'est une vid√©o virale l√©gitime'
        })
    
    # Vid√©os trop longues (>2h)
    cursor.execute("""
        SELECT v.title, c.name, v.duration_seconds, v.id
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE v.duration_seconds > 7200
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        hours = row[2] / 3600
        inconsistencies.append({
            'type': 'aberrant',
            'severity': 'medium',
            'message': f'Dur√©e exceptionnellement longue ({hours:.1f}h)',
            'description': f'La vid√©o "{row[0]}" dure {hours:.1f} heures',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[3],
            'fix_suggestion': 'V√©rifier si c\'est un live stream ou une erreur'
        })
    
    # Ratio like/view impossible (>50%)
    cursor.execute("""
        SELECT v.title, c.name, v.view_count, v.like_count, v.id
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE v.like_count > 0 AND v.view_count > 0
        AND (v.like_count * 1.0 / v.view_count) > 0.5
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        ratio = (row[3] / row[2]) * 100
        inconsistencies.append({
            'type': 'aberrant',
            'severity': 'high',
            'message': f'Ratio like/vue impossible ({ratio:.1f}%)',
            'description': f'La vid√©o "{row[0]}" a {row[3]} likes pour {row[2]} vues',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[4],
            'fix_suggestion': 'V√©rifier les donn√©es ou re-scraper'
        })
    
    return inconsistencies

def _detect_temporal_inconsistencies(cursor) -> List[Dict]:
    """D√©tecte les incoh√©rences temporelles"""
    inconsistencies = []
    
    # Vid√©os avec dates futures
    cursor.execute("""
        SELECT v.title, c.name, v.published_at, v.id
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE v.published_at > datetime('now')
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'temporal',
            'severity': 'high',
            'message': 'Date de publication future',
            'description': f'La vid√©o "{row[0]}" a une date de publication future: {row[2]}',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[3],
            'fix_suggestion': 'Corriger la date ou v√©rifier le fuseau horaire'
        })
    
    # Vid√©os tr√®s anciennes (avant 2005, cr√©ation de YouTube)
    cursor.execute("""
        SELECT v.title, c.name, v.published_at, v.id
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE v.published_at < '2005-01-01'
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'temporal',
            'severity': 'medium',
            'message': 'Date ant√©rieure √† YouTube',
            'description': f'La vid√©o "{row[0]}" a une date de {row[2]} (avant 2005)',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[3],
            'fix_suggestion': 'V√©rifier et corriger la date'
        })
    
    return inconsistencies

def _detect_duplicates(cursor) -> List[Dict]:
    """D√©tecte les doublons"""
    inconsistencies = []
    
    # Vid√©os avec le m√™me titre et concurrent
    cursor.execute("""
        SELECT v1.title, c.name, COUNT(*) as count
        FROM video v1
        JOIN concurrent c ON v1.concurrent_id = c.id
        GROUP BY v1.title, c.id
        HAVING COUNT(*) > 1
        ORDER BY count DESC
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'duplicates',
            'severity': 'medium',
            'message': f'Titre dupliqu√© ({row[2]} fois)',
            'description': f'Le titre "{row[0]}" appara√Æt {row[2]} fois pour le concurrent {row[1]}',
            'competitor_name': row[1],
            'video_title': row[0],
            'fix_suggestion': 'V√©rifier s\'il s\'agit de re-uploads ou d\'erreurs'
        })
    
    # Concurrents avec le m√™me nom
    cursor.execute("""
        SELECT name, COUNT(*) as count
        FROM concurrent
        GROUP BY name
        HAVING COUNT(*) > 1
        ORDER BY count DESC
        LIMIT 10
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'duplicates',
            'severity': 'high',
            'message': f'Concurrent dupliqu√© ({row[1]} fois)',
            'description': f'Le concurrent "{row[0]}" appara√Æt {row[1]} fois dans la base',
            'competitor_name': row[0],
            'fix_suggestion': 'Fusionner les entr√©es ou v√©rifier les URLs'
        })
    
    return inconsistencies

def _detect_format_errors(cursor) -> List[Dict]:
    """D√©tecte les erreurs de format"""
    inconsistencies = []
    
    # Titres trop courts ou trop longs
    cursor.execute("""
        SELECT v.title, c.name, LENGTH(v.title) as title_length, v.id
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE LENGTH(v.title) < 3 OR LENGTH(v.title) > 200
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        if row[2] < 3:
            issue = f'Titre trop court ({row[2]} caract√®res)'
            severity = 'medium'
        else:
            issue = f'Titre trop long ({row[2]} caract√®res)'
            severity = 'low'
        
        inconsistencies.append({
            'type': 'format',
            'severity': severity,
            'message': issue,
            'description': f'La vid√©o "{row[0]}" a un titre de {row[2]} caract√®res',
            'competitor_name': row[1],
            'video_title': row[0],
            'video_id': row[3],
            'fix_suggestion': 'V√©rifier la qualit√© des donn√©es scrapp√©es'
        })
    
    # URLs malform√©es
    cursor.execute("""
        SELECT name, channel_url, id
        FROM concurrent
        WHERE channel_url NOT LIKE '%youtube.com%' 
        AND channel_url NOT LIKE '%youtu.be%'
        AND channel_url IS NOT NULL
        LIMIT 20
    """)
    
    for row in cursor.fetchall():
        inconsistencies.append({
            'type': 'format',
            'severity': 'high',
            'message': 'URL non-YouTube',
            'description': f'Le concurrent "{row[0]}" a une URL non-YouTube: {row[1]}',
            'competitor_name': row[0],
            'competitor_id': row[2],
            'fix_suggestion': 'Corriger l\'URL ou v√©rifier la source'
        })
    
    return inconsistencies

def create_settings_table():
    """Create the settings table if it doesn't exist."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS settings (
            key VARCHAR(100) PRIMARY KEY,
            value TEXT,
            updated_at DATETIME
        )
    ''')
    conn.commit()
    conn.close()


def set_last_action_status(action_key, value_dict):
    """Set the last status/result for a given action (as JSON)."""
    import json
    conn = get_db_connection()
    cursor = conn.cursor()
    now = datetime.now().isoformat()
    value_json = json.dumps(value_dict)
    cursor.execute('''
        INSERT INTO settings (key, value, updated_at)
        VALUES (?, ?, ?)
        ON CONFLICT(key) DO UPDATE SET value=excluded.value, updated_at=excluded.updated_at
    ''', (action_key, value_json, now))
    conn.commit()
    conn.close()


def get_last_action_status(action_key):
    """Get the last status/result for a given action (as dict)."""
    import json
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT value, updated_at FROM settings WHERE key = ?', (action_key,))
    row = cursor.fetchone()
    conn.close()
    if row:
        value, updated_at = row
        try:
            value_dict = json.loads(value)
        except Exception:
            value_dict = {'raw': value}
        value_dict['updated_at'] = updated_at
        return value_dict
    return None

def create_custom_rules_table():
    """Cr√©er la table custom_rules pour les r√®gles m√©tier personnalis√©es."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS custom_rules (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            pattern VARCHAR(200) NOT NULL,
            category VARCHAR(10) NOT NULL,
            language VARCHAR(5) NOT NULL DEFAULT 'all',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(pattern, category, language)
        )
    ''')
    conn.commit()
    conn.close()


def add_custom_rule(pattern: str, category: str, language: str = 'all'):
    """Ajouter une r√®gle custom (pattern, cat√©gorie, langue)."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        INSERT OR IGNORE INTO custom_rules (pattern, category, language)
        VALUES (?, ?, ?)
    ''', (pattern.lower(), category, language))
    conn.commit()
    conn.close()


def list_custom_rules(language: str = None):
    """Lister toutes les r√®gles custom (optionnel: filtrer par langue)."""
    conn = get_db_connection()
    cursor = conn.cursor()
    if language:
        cursor.execute('SELECT pattern, category, language FROM custom_rules WHERE language = ? OR language = "all"', (language,))
    else:
        cursor.execute('SELECT pattern, category, language FROM custom_rules')
    rules = cursor.fetchall()
    conn.close()
    return [{'pattern': r[0], 'category': r[1], 'language': r[2]} for r in rules]


def remove_custom_rule(pattern: str, category: str, language: str = 'all'):
    """Supprimer une r√®gle custom."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('DELETE FROM custom_rules WHERE pattern = ? AND category = ? AND language = ?', (pattern.lower(), category, language))
    conn.commit()
    conn.close()

def update_database_schema_for_dates():
    """Mettre √† jour le sch√©ma de la base de donn√©es pour distinguer dates d'importation et dates de publication r√©elles"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si la nouvelle colonne existe d√©j√†
        cursor.execute("PRAGMA table_info(video)")
        columns = [column[1] for column in cursor.fetchall()]
        
        print(f"[DB-DATES-UPDATE] Colonnes actuelles: {columns}")
        
        # Ajouter la nouvelle colonne si elle n'existe pas
        if 'youtube_published_at' not in columns:
            print(f"[DB-DATES-UPDATE] Ajout de la colonne youtube_published_at")
            cursor.execute('ALTER TABLE video ADD COLUMN youtube_published_at DATETIME DEFAULT NULL')
            
            # Renommer la colonne existante pour clarifier qu'elle contient les dates d'importation
            print(f"[DB-DATES-UPDATE] published_at devient import_published_at (dates d'importation)")
            # Note: SQLite ne supporte pas RENAME COLUMN avant 3.25.0, on garde published_at comme date d'importation
            
        conn.commit()
        print("[DB-DATES-UPDATE] ‚úÖ Sch√©ma de base de donn√©es mis √† jour avec succ√®s")
        return True
        
    except Exception as e:
        print(f"[DB-DATES-UPDATE] ‚ùå Erreur: {e}")
        conn.rollback()
        return False
    finally:
        conn.close()

def correct_all_video_dates_with_youtube_api() -> Dict:
    """
    Corrige toutes les dates des vid√©os en utilisant l'API YouTube Data v3
    R√©cup√®re les vraies dates de publication et les stocke dans youtube_published_at
    """
    from .youtube_api_client import create_youtube_client
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # S'assurer que la colonne existe
        update_database_schema_for_dates()
        
        # R√©cup√©rer toutes les vid√©os qui n'ont pas encore de vraie date de publication
        cursor.execute('''
            SELECT id, video_id, title, concurrent_id, published_at
            FROM video 
            WHERE youtube_published_at IS NULL 
            AND video_id IS NOT NULL 
            AND video_id != ''
            ORDER BY concurrent_id
        ''')
        
        videos_to_update = cursor.fetchall()
        total_videos = len(videos_to_update)
        
        if total_videos == 0:
            return {
                'success': True,
                'message': 'Aucune vid√©o √† mettre √† jour',
                'total_videos': 0,
                'updated_videos': 0,
                'failed_videos': 0
            }
        
        print(f"[DATES-CORRECTION] üöÄ D√©but correction de {total_videos} vid√©os")
        
        # Initialiser l'API YouTube
        youtube = create_youtube_client()
        
        updated_count = 0
        failed_count = 0
        
        # Traiter par lots de 50 (limite API)
        for i in range(0, total_videos, 50):
            batch = videos_to_update[i:i+50]
            video_ids = [row[1] for row in batch if row[1]]  # video_id
            
            try:
                # R√©cup√©rer les d√©tails des vid√©os
                video_details = youtube.get_videos_details(video_ids)
                
                # Cr√©er un mapping ID -> d√©tails
                details_map = {video['id']: video for video in video_details}
                
                # Mettre √† jour chaque vid√©o
                for row in batch:
                    db_id, video_id, title, competitor_id, import_date = row
                    
                    if video_id in details_map:
                        video_details = details_map[video_id]
                        youtube_published_at = video_details['published_at']
                        
                        # Mettre √† jour la base
                        cursor.execute('''
                            UPDATE video 
                            SET youtube_published_at = ? 
                            WHERE id = ?
                        ''', (youtube_published_at, db_id))
                        
                        updated_count += 1
                        
                        # Log pour les premi√®res vid√©os
                        if updated_count <= 10:
                            print(f"[DATES-CORRECTION] ‚úÖ {title[:40]}... : {import_date} ‚Üí {youtube_published_at}")
                    else:
                        print(f"[DATES-CORRECTION] ‚ùå Vid√©o non trouv√©e: {video_id}")
                        failed_count += 1
                
                # Sauvegarder le lot
                conn.commit()
                
                # Progress
                print(f"[DATES-CORRECTION] üìä Progression: {updated_count}/{total_videos} ({(updated_count/total_videos)*100:.1f}%)")
                
            except Exception as e:
                print(f"[DATES-CORRECTION] ‚ùå Erreur lot {i//50 + 1}: {e}")
                failed_count += len(batch)
                continue
        
        print(f"[DATES-CORRECTION] üéâ Termin√©: {updated_count} mises √† jour, {failed_count} √©checs")
        
        return {
            'success': True,
            'message': f'Correction termin√©e: {updated_count} vid√©os mises √† jour, {failed_count} √©checs',
            'total_videos': total_videos,
            'updated_videos': updated_count,
            'failed_videos': failed_count
        }
        
    except Exception as e:
        print(f"[DATES-CORRECTION] ‚ùå Erreur g√©n√©rale: {e}")
        return {
            'success': False,
            'message': f'Erreur lors de la correction des dates: {str(e)}',
            'total_videos': total_videos if 'total_videos' in locals() else 0,
            'updated_videos': updated_count if 'updated_count' in locals() else 0,
            'failed_videos': failed_count if 'failed_count' in locals() else 0
        }
    finally:
        conn.close()

def get_dates_correction_status() -> Dict:
    """R√©cup√®re le statut de la correction des dates"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # V√©rifier si la colonne existe
        cursor.execute("PRAGMA table_info(video)")
        columns = [column[1] for column in cursor.fetchall()]
        
        if 'youtube_published_at' not in columns:
            return {
                'column_exists': False,
                'total_videos': 0,
                'corrected_videos': 0,
                'pending_videos': 0,
                'correction_percentage': 0
            }
        
        # Compter les vid√©os
        cursor.execute('SELECT COUNT(*) FROM video WHERE video_id IS NOT NULL AND video_id != ""')
        total_videos = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM video WHERE youtube_published_at IS NOT NULL')
        corrected_videos = cursor.fetchone()[0]
        
        pending_videos = total_videos - corrected_videos
        correction_percentage = (corrected_videos / total_videos * 100) if total_videos > 0 else 0
        
        return {
            'column_exists': True,
            'total_videos': total_videos,
            'corrected_videos': corrected_videos,
            'pending_videos': pending_videos,
            'correction_percentage': correction_percentage
        }
        
    except Exception as e:
        print(f"[DATES-STATUS] Erreur: {e}")
        return {
            'column_exists': False,
            'total_videos': 0,
            'corrected_videos': 0,
            'pending_videos': 0,
            'correction_percentage': 0
        }
    finally:
        conn.close()

def mark_human_classification(video_id: int = None, playlist_id: int = None, category: str = None, user_notes: str = '') -> bool:
    """
    üö® FONCTION CRITIQUE : Marquer une classification comme √©tant faite par un humain
    Cette fonction doit √™tre appel√©e chaque fois qu'un utilisateur corrige manuellement une classification
    
    Args:
        video_id: ID de la vid√©o (si c'est une vid√©o)
        playlist_id: ID de la playlist (si c'est une playlist)
        category: Nouvelle cat√©gorie (hero/hub/help)
        user_notes: Notes optionnelles de l'utilisateur
        
    Returns:
        bool: True si la classification a √©t√© marqu√©e avec succ√®s
    """
    if not video_id and not playlist_id:
        print("[HUMAN-MARK] ‚ùå Erreur: video_id ou playlist_id requis")
        return False
    
    if not category or category not in ['hero', 'hub', 'help']:
        print("[HUMAN-MARK] ‚ùå Erreur: cat√©gorie invalide")
        return False
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        if video_id:
            # üö® MARQUER LA VID√âO COMME CLASSIFI√âE PAR UN HUMAIN (PRIORIT√â ABSOLUE)
            cursor.execute('''
                UPDATE video SET 
                    category = ?, 
                    classification_source = 'human',
                    classification_date = CURRENT_TIMESTAMP,
                    classification_confidence = 100,
                    human_verified = 1,
                    last_updated = ?
                WHERE id = ?
            ''', (category, datetime.now(), video_id))
            
            if cursor.rowcount > 0:
                print(f"[HUMAN-MARK] ‚úÖ Vid√©o ID {video_id} marqu√©e comme classification humaine ‚Üí {category.upper()}")
                
                # Ajouter dans la table de feedback pour historique
                cursor.execute('''
                    INSERT INTO classification_feedback (
                        video_id, original_category, corrected_category,
                        confidence_score, user_feedback_type, feedback_timestamp, user_notes
                    ) VALUES (?, ?, ?, 100, 'human_correction', CURRENT_TIMESTAMP, ?)
                ''', (video_id, 'unknown', category, user_notes))
                
            else:
                print(f"[HUMAN-MARK] ‚ö†Ô∏è Vid√©o ID {video_id} non trouv√©e")
                return False
        
        if playlist_id:
            # üö® MARQUER LA PLAYLIST COMME CLASSIFI√âE PAR UN HUMAIN (PRIORIT√â ABSOLUE)
            cursor.execute('''
                UPDATE playlist SET 
                    category = ?, 
                    classification_source = 'human',
                    classification_date = CURRENT_TIMESTAMP,
                    classification_confidence = 100,
                    human_verified = 1,
                    last_updated = ?
                WHERE id = ?
            ''', (category, datetime.now(), playlist_id))
            
            if cursor.rowcount > 0:
                print(f"[HUMAN-MARK] ‚úÖ Playlist ID {playlist_id} marqu√©e comme classification humaine ‚Üí {category.upper()}")
            else:
                print(f"[HUMAN-MARK] ‚ö†Ô∏è Playlist ID {playlist_id} non trouv√©e")
                return False
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"[HUMAN-MARK] ‚ùå Erreur lors du marquage humain: {e}")
        return False
    finally:
        conn.close()

def check_human_protection_status(video_id: int = None, playlist_id: int = None) -> Dict:
    """
    V√©rifier si une vid√©o ou playlist est prot√©g√©e par supervision humaine
    
    Returns:
        Dict avec les informations de protection
    """
    if not video_id and not playlist_id:
        return {'protected': False, 'error': 'ID requis'}
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        if video_id:
            cursor.execute('''
                SELECT classification_source, human_verified, classification_date, category
                FROM video WHERE id = ?
            ''', (video_id,))
            result = cursor.fetchone()
            
            if result:
                is_protected = result[0] == 'human' or result[1] == 1
                return {
                    'protected': is_protected,
                    'type': 'video',
                    'classification_source': result[0],
                    'human_verified': bool(result[1]),
                    'classification_date': result[2],
                    'category': result[3]
                }
        
        if playlist_id:
            cursor.execute('''
                SELECT classification_source, human_verified, classification_date, category
                FROM playlist WHERE id = ?
            ''', (playlist_id,))
            result = cursor.fetchone()
            
            if result:
                is_protected = result[0] == 'human' or result[1] == 1
                return {
                    'protected': is_protected,
                    'type': 'playlist',
                    'classification_source': result[0],
                    'human_verified': bool(result[1]),
                    'classification_date': result[2],
                    'category': result[3]
                }
        
        return {'protected': False, 'error': 'Non trouv√©'}
        
    except Exception as e:
        print(f"[PROTECTION-CHECK] ‚ùå Erreur: {e}")
        return {'protected': False, 'error': str(e)}
    finally:
        conn.close()

def verify_classification_integrity() -> Dict:
    """
    üö® FONCTION DE V√âRIFICATION D'INT√âGRIT√â
    V√©rifie qu'il n'y a pas de conflits entre les classifications humaines et IA
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        integrity_report = {
            'status': 'healthy',
            'issues': [],
            'stats': {},
            'recommendations': []
        }
        
        # 1. V√©rifier les champs de tracking manquants
        cursor.execute('''
            SELECT COUNT(*) FROM video 
            WHERE classification_source IS NULL AND category IS NOT NULL
        ''')
        videos_missing_source = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT COUNT(*) FROM playlist 
            WHERE classification_source IS NULL AND category IS NOT NULL
        ''')
        playlists_missing_source = cursor.fetchone()[0]
        
        if videos_missing_source > 0:
            integrity_report['issues'].append({
                'type': 'missing_tracking',
                'severity': 'warning',
                'description': f'{videos_missing_source} vid√©os cat√©goris√©es sans source de classification',
                'fix': 'Ex√©cuter la mise √† jour du sch√©ma de base'
            })
        
        if playlists_missing_source > 0:
            integrity_report['issues'].append({
                'type': 'missing_tracking',
                'severity': 'warning',
                'description': f'{playlists_missing_source} playlists cat√©goris√©es sans source de classification',
                'fix': 'Ex√©cuter la mise √† jour du sch√©ma de base'
            })
        
        # 2. Statistiques de protection
        cursor.execute('''
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN classification_source = 'human' OR human_verified = 1 THEN 1 ELSE 0 END) as human_protected,
                SUM(CASE WHEN classification_source = 'ai' THEN 1 ELSE 0 END) as ai_classified,
                SUM(CASE WHEN classification_source = 'playlist' THEN 1 ELSE 0 END) as playlist_inherited
            FROM video WHERE category IS NOT NULL
        ''')
        video_stats = cursor.fetchone()
        
        cursor.execute('''
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN classification_source = 'human' OR human_verified = 1 THEN 1 ELSE 0 END) as human_protected,
                SUM(CASE WHEN classification_source = 'ai' THEN 1 ELSE 0 END) as ai_classified
            FROM playlist WHERE category IS NOT NULL
        ''')
        playlist_stats = cursor.fetchone()
        
        integrity_report['stats'] = {
            'videos': {
                'total_classified': video_stats[0],
                'human_protected': video_stats[1],
                'ai_classified': video_stats[2],
                'playlist_inherited': video_stats[3]
            },
            'playlists': {
                'total_classified': playlist_stats[0],
                'human_protected': playlist_stats[1],
                'ai_classified': playlist_stats[2]
            }
        }
        
        # 3. D√©tecter les conflits potentiels
        cursor.execute('''
            SELECT v.id, v.title, v.category, v.classification_source, v.human_verified, p.name as playlist_name
            FROM video v
            JOIN playlist_video pv ON v.id = pv.video_id
            JOIN playlist p ON pv.playlist_id = p.id
            WHERE v.classification_source = 'human' 
            AND p.classification_source = 'ai'
            AND v.category != p.category
        ''')
        conflicts = cursor.fetchall()
        
        if conflicts:
            integrity_report['issues'].append({
                'type': 'category_conflict',
                'severity': 'error',
                'description': f'{len(conflicts)} vid√©os classifi√©es manuellement avec cat√©gorie diff√©rente de leur playlist IA',
                'conflicts': [{'video_title': c[1], 'video_category': c[2], 'playlist_name': c[5]} for c in conflicts[:5]]
            })
            integrity_report['status'] = 'issues_detected'
        
        # 4. V√©rifier les fonctions de debug potentiellement dangereuses
        dangerous_functions = [
            'apply_playlist_categories_to_videos',
            'classify_videos_directly_with_keywords',
            'auto_classify_uncategorized_playlists',
            'run_global_ai_classification'
        ]
        
        integrity_report['recommendations'] = [
            "‚úÖ Toutes les fonctions de classification respectent maintenant les classifications humaines",
            "üõ°Ô∏è Les playlists classifi√©es manuellement ne propagent plus automatiquement aux vid√©os",
            "ü§ñ La classification IA ne peut plus √©craser les corrections humaines",
            "‚ö†Ô∏è Utilisez 'force_apply_human_playlist_to_videos' SEULEMENT avec confirmation explicite"
        ]
        
        # 5. V√©rifier l'int√©grit√© des tables de feedback
        cursor.execute('''
            SELECT COUNT(*) FROM classification_feedback cf
            LEFT JOIN video v ON cf.video_id = v.id
            WHERE v.id IS NULL
        ''')
        orphaned_feedback = cursor.fetchone()[0]
        
        if orphaned_feedback > 0:
            integrity_report['issues'].append({
                'type': 'orphaned_feedback',
                'severity': 'warning',
                'description': f'{orphaned_feedback} feedbacks orphelins (vid√©os supprim√©es)',
                'fix': 'Nettoyer les feedbacks orphelins'
            })
        
        # 6. R√©sum√© final
        if len(integrity_report['issues']) == 0:
            integrity_report['status'] = 'excellent'
            integrity_report['summary'] = '‚úÖ Syst√®me de classification int√®gre - Aucun conflit d√©tect√©'
        elif any(issue['severity'] == 'error' for issue in integrity_report['issues']):
            integrity_report['status'] = 'critical'
            integrity_report['summary'] = '‚ùå Conflits critiques d√©tect√©s - Action requise'
        else:
            integrity_report['status'] = 'minor_issues'
            integrity_report['summary'] = '‚ö†Ô∏è Probl√®mes mineurs d√©tect√©s - Correction recommand√©e'
        
        return integrity_report
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'summary': f'‚ùå Erreur lors de la v√©rification: {e}'
        }
    finally:
        conn.close()

def fix_classification_tracking():
    """
    üõ†Ô∏è FONCTION DE R√âPARATION : Corriger les probl√®mes de tracking de classification
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        fixes_applied = []
        
        # 1. Marquer les vid√©os cat√©goris√©es sans source comme IA par d√©faut
        cursor.execute('''
            UPDATE video 
            SET classification_source = 'ai',
                classification_date = datetime('now'),
                classification_confidence = 50,
                human_verified = 0
            WHERE classification_source IS NULL 
            AND category IS NOT NULL
        ''')
        videos_fixed = cursor.rowcount
        if videos_fixed > 0:
            fixes_applied.append(f"‚úÖ {videos_fixed} vid√©os marqu√©es comme classification IA")
        
        # 2. Marquer les playlists cat√©goris√©es sans source comme HUMAINES (car classifi√©es via interface)
        cursor.execute('''
            UPDATE playlist 
            SET classification_source = 'human',
                classification_date = datetime('now'),
                classification_confidence = 100,
                human_verified = 1
            WHERE classification_source IS NULL 
            AND category IS NOT NULL
            AND category IN ('hero', 'hub', 'help')
        ''')
        playlists_fixed = cursor.rowcount
        if playlists_fixed > 0:
            fixes_applied.append(f"‚úÖ {playlists_fixed} playlists marqu√©es comme classification HUMAINE")
            
        # 2b. Aussi afficher les d√©tails des playlists trouv√©es
        cursor.execute('''
            SELECT p.name, p.category, c.name as competitor_name
            FROM playlist p
            JOIN concurrent c ON p.concurrent_id = c.id
            WHERE p.classification_source = 'human'
            AND p.category IS NOT NULL
            ORDER BY p.classification_date DESC
        ''')
        playlist_details = cursor.fetchall()
        if playlist_details:
            print(f"[TRACKING-FIX] üìã Playlists classifi√©es manuellement d√©tect√©es:")
            for playlist_name, category, competitor_name in playlist_details:
                print(f"  - {competitor_name}: '{playlist_name}' ‚Üí {category.upper()}")
        
        # 3. Nettoyer les feedbacks orphelins
        cursor.execute('''
            DELETE FROM classification_feedback 
            WHERE video_id NOT IN (SELECT id FROM video)
        ''')
        feedback_cleaned = cursor.rowcount
        if feedback_cleaned > 0:
            fixes_applied.append(f"‚úÖ {feedback_cleaned} feedbacks orphelins nettoy√©s")
        
        conn.commit()
        
        return {
            'success': True,
            'fixes_applied': fixes_applied,
            'message': f'R√©paration termin√©e: {len(fixes_applied)} corrections appliqu√©es'
        }
        
    except Exception as e:
        conn.rollback()
        return {
            'success': False,
            'error': str(e),
            'message': f'Erreur lors de la r√©paration: {e}'
        }
    finally:
        conn.close()

def generate_center_parcs_insights() -> Dict:
    """
    G√©n√®re des conseils sp√©cifiques pour les cha√Ænes Center Parcs
    Identifie les cha√Ænes Center Parcs et g√©n√®re des conseils personnalis√©s pour chacune
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        # Identifier les cha√Ænes Center Parcs
        cursor.execute('''
            SELECT 
                c.id,
                c.name,
                c.country,
                c.channel_url,
                c.subscriber_count,
                c.view_count,
                c.video_count,
                c.thumbnail_url,
                COUNT(v.id) as actual_video_count,
                SUM(v.view_count) as total_views,
                AVG(v.view_count) as avg_views,
                AVG(v.duration_seconds) as avg_duration,
                SUM(CASE WHEN v.category = 'hero' THEN 1 ELSE 0 END) as hero_count,
                SUM(CASE WHEN v.category = 'hub' THEN 1 ELSE 0 END) as hub_count,
                SUM(CASE WHEN v.category = 'help' THEN 1 ELSE 0 END) as help_count
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            WHERE c.name LIKE '%Center Parcs%' 
               OR c.name LIKE '%centerparcs%'
               OR c.channel_url LIKE '%CenterParcs%'
               OR c.channel_url LIKE '%centerparcs%'
               OR c.channel_url LIKE '%@CenterParcsNLBE%'
               OR c.channel_url LIKE '%CenterParcsBelgi%'
               OR c.channel_url LIKE '%@CenterParcsDE%'
            GROUP BY c.id
            ORDER BY actual_video_count DESC
        ''')
        
        results = cursor.fetchall()
        
        if not results:
            return {
                'success': False,
                'error': 'Aucune cha√Æne Center Parcs trouv√©e'
            }
        
        channels_insights = {}
        
        for row in results:
            competitor_id = row[0]
            name = row[1]
            country = row[2] or 'Non d√©fini'
            channel_url = row[3]
            subscriber_count = row[4] or 0
            view_count = row[5] or 0
            video_count = row[6] or 0
            thumbnail_url = row[7]
            actual_video_count = row[8] or 0
            total_views = row[9] or 0
            avg_views = row[10] or 0
            avg_duration = row[11] or 0
            hero_count = row[12] or 0
            hub_count = row[13] or 0
            help_count = row[14] or 0
            
            # D√©terminer la r√©gion/pays pour les conseils
            if 'France' in name or 'France' in channel_url:
                region = 'France'
                region_key = 'france'
            elif ('Belgi' in name or 'Belgi' in channel_url or 
                  'Belgium' in name or 'Belgium' in channel_url or
                  'Nederland' in name or 'Nederland' in channel_url or
                  'NLBE' in channel_url or 'NL' in channel_url or
                  country and country.lower() in ['netherlands', 'belgium', 'nl', 'be']):
                region = 'Belgique/Pays-Bas'
                region_key = 'belgium_netherlands'
            elif ('Deutschland' in name or 'Germany' in name or 'DE' in channel_url or
                  '@CenterParcsDE' in channel_url or 'CenterParcsDE' in channel_url or
                  country and country.lower() in ['germany', 'deutschland', 'de']):
                region = 'Allemagne'
                region_key = 'germany'
            else:
                region = 'International'
                region_key = 'international'
            
            # Calculer les ratios
            total_categorized = hero_count + hub_count + help_count
            hero_ratio = (hero_count / total_categorized * 100) if total_categorized > 0 else 0
            hub_ratio = (hub_count / total_categorized * 100) if total_categorized > 0 else 0
            help_ratio = (help_count / total_categorized * 100) if total_categorized > 0 else 0
            
            # R√©cup√©rer les vid√©os les plus performantes et moins performantes
            cursor.execute('''
                SELECT title, view_count, category, published_at, duration_seconds, like_count, comment_count
                FROM video
                WHERE concurrent_id = ?
                ORDER BY view_count DESC
                LIMIT 10
            ''', (competitor_id,))
            
            top_videos = cursor.fetchall()
            
            # R√©cup√©rer les vid√©os les moins performantes
            cursor.execute('''
                SELECT title, view_count, category, published_at, duration_seconds, like_count, comment_count
                FROM video
                WHERE concurrent_id = ? AND view_count > 0
                ORDER BY view_count ASC
                LIMIT 10
            ''', (competitor_id,))
            
            bottom_videos = cursor.fetchall()
            
            # R√©cup√©rer les benchmarks sectoriels pour la r√©gion
            sector_benchmarks = get_sector_benchmarks_for_region(region_key)
            
            # Analyser ce qui fonctionne (What works)
            what_works = analyze_what_works(top_videos, sector_benchmarks, region_key, avg_duration, hero_ratio, hub_ratio, help_ratio)
            
            # Analyser ce qui ne fonctionne pas (What doesn't work)
            what_doesnt_work = analyze_what_doesnt_work(bottom_videos, sector_benchmarks, region_key, avg_duration, hero_ratio, hub_ratio, help_ratio, avg_views)
            
            # Conseils r√©gionaux sp√©cifiques
            regional_advice = []
            if region_key == 'france':
                regional_advice.extend([
                    "üá´üá∑ Mettre en avant les sp√©cificit√©s fran√ßaises : terroir, gastronomie locale",
                    "üèûÔ∏è Valoriser les activit√©s nature et bien-√™tre typiques",
                    "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Cr√©er du contenu famille multig√©n√©rationnel"
                ])
            elif region_key == 'belgium_netherlands':
                regional_advice.extend([
                    "üáßüá™üá≥üá± Capitaliser sur la notori√©t√© historique de Center Parcs",
                    "üå≤ Mettre en avant l'authenticit√© et la proximit√© avec la nature",
                    "üèä‚Äç‚ôÄÔ∏è Valoriser l'Aqua Mundo comme signature unique",
                    "üö≤ Promouvoir les activit√©s cyclables et durables",
                    "üè° Mettre en valeur les cottages comme 'seconde maison'"
                ])
            elif region_key == 'germany':
                regional_advice.extend([
                    "üá©üá™ D√©velopper le contenu en allemand authentique",
                    "üèïÔ∏è Mettre en avant les activit√©s outdoor et sportives",
                    "üéØ Cibler les familles actives et les groupes d'amis",
                    "üå≤ Valoriser la connection avec la nature allemande",
                    "üèÉ‚Äç‚ôÇÔ∏è Promouvoir les activit√©s de fitness et bien-√™tre",
                    "üì± Adapter aux habitudes digitales allemandes"
                ])
            elif region_key == 'international':
                regional_advice.extend([
                    "üåç Cr√©er du contenu universel compr√©hensible internationalement",
                    "üèñÔ∏è Mettre en avant les destinations iconiques",
                    "üéâ Valoriser l'exp√©rience Center Parcs globale"
                ])
            
            # Conseils g√©n√©raux
            general_advice = []
            regional_advice.extend([
                "üì± Optimiser pour mobile : 70% du trafic YouTube",
                "üé¨ Cr√©er des vignettes attractives et coh√©rentes",
                "üìÖ Publier r√©guli√®rement : 2-3 vid√©os par semaine id√©alement"
            ])
            
            channels_insights[region_key] = {
                'name': name,
                'region': region,
                'channel_url': channel_url,
                'thumbnail_url': thumbnail_url,
                'stats': {
                    'subscriber_count': subscriber_count,
                    'video_count': actual_video_count,
                    'total_views': total_views,
                    'avg_views': int(avg_views),
                    'avg_duration_minutes': round(avg_duration / 60, 1) if avg_duration > 0 else 0
                },
                'content_distribution': {
                    'hero_count': hero_count,
                    'hub_count': hub_count,
                    'help_count': help_count,
                    'hero_ratio': round(hero_ratio, 1),
                    'hub_ratio': round(hub_ratio, 1),
                    'help_ratio': round(help_ratio, 1),
                    'total_categorized': total_categorized
                },
                'top_videos': [
                    {
                        'title': video[0],
                        'views': video[1],
                        'category': video[2],
                        'published_at': video[3]
                    }
                    for video in top_videos
                ],
                'advice': what_works + what_doesnt_work + regional_advice + general_advice
            }
        
        return {
            'success': True,
            'channels': channels_insights,
            'total_channels': len(channels_insights),
            'generated_at': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"[CENTER-PARCS-INSIGHTS] ‚ùå Erreur: {e}")
        return {
            'success': False,
            'error': str(e)
        }
    finally:
        conn.close()

def get_sector_benchmarks_for_region(region_key: str) -> Dict:
    """
    R√©cup√®re les benchmarks sectoriels pour une r√©gion donn√©e
    """
    benchmarks = {
        'germany': {
            'hero_optimal_duration': 90,  # 1.5 minutes optimal pour HERO en Allemagne
            'hub_optimal_duration': 240,  # 4 minutes optimal pour HUB en Allemagne
            'help_optimal_duration': 180,  # 3 minutes optimal pour HELP en Allemagne
            'hero_ratio_benchmark': 15,  # 15% optimal pour HERO
            'hub_ratio_benchmark': 60,   # 60% optimal pour HUB
            'help_ratio_benchmark': 25,  # 25% optimal pour HELP
            'avg_views_benchmark': 8000,  # 8K vues moyennes en Allemagne
            'engagement_topics': ['outdoor', 'sport', 'familie', 'natur', 'wellness'],
            'low_engagement_topics': ['hund', 'haustier', 'politik', 'wetter']
        },
        'france': {
            'hero_optimal_duration': 120,  # 2 minutes optimal pour HERO en France
            'hub_optimal_duration': 300,   # 5 minutes optimal pour HUB en France
            'help_optimal_duration': 240,  # 4 minutes optimal pour HELP en France
            'hero_ratio_benchmark': 20,
            'hub_ratio_benchmark': 50,
            'help_ratio_benchmark': 30,
            'avg_views_benchmark': 6000,
            'engagement_topics': ['famille', 'gastronomie', 'nature', 'd√©tente', 'vacances'],
            'low_engagement_topics': ['animaux', 'politique', 'm√©t√©o']
        },
        'belgium_netherlands': {
            'hero_optimal_duration': 150,  # 2.5 minutes optimal pour HERO en Belgique/Pays-Bas
            'hub_optimal_duration': 280,   # 4.7 minutes optimal pour HUB
            'help_optimal_duration': 200,  # 3.3 minutes optimal pour HELP
            'hero_ratio_benchmark': 18,
            'hub_ratio_benchmark': 55,
            'help_ratio_benchmark': 27,
            'avg_views_benchmark': 7000,
            'engagement_topics': ['aqua mundo', 'natuur', 'familie', 'fiets', 'cottage'],
            'low_engagement_topics': ['weer', 'dieren', 'politiek']
        },
        'international': {
            'hero_optimal_duration': 120,
            'hub_optimal_duration': 300,
            'help_optimal_duration': 240,
            'hero_ratio_benchmark': 15,
            'hub_ratio_benchmark': 60,
            'help_ratio_benchmark': 25,
            'avg_views_benchmark': 5000,
            'engagement_topics': ['family', 'nature', 'vacation', 'outdoor', 'wellness'],
            'low_engagement_topics': ['weather', 'pets', 'politics']
        }
    }
    
    return benchmarks.get(region_key, benchmarks['international'])


def analyze_what_works(top_videos: List, benchmarks: Dict, region_key: str, avg_duration: float, hero_ratio: float, hub_ratio: float, help_ratio: float) -> List[str]:
    """
    Analyse ce qui fonctionne bien pour une cha√Æne
    """
    what_works = []
    
    if not top_videos:
        return ["‚ùå Donn√©es insuffisantes pour analyser les performances"]
    
    # Analyse des vid√©os les plus performantes
    avg_top_views = sum(video[1] for video in top_videos) / len(top_videos)
    
    # Analyse des dur√©es des top vid√©os
    top_durations = [video[4] for video in top_videos if video[4] and video[4] > 0]
    if top_durations:
        avg_top_duration = sum(top_durations) / len(top_durations)
        avg_top_minutes = avg_top_duration / 60
        
        # Identifier les cat√©gories qui performent bien
        category_performance = {}
        for video in top_videos:
            category = video[2] or 'non-class√©'
            if category not in category_performance:
                category_performance[category] = []
            category_performance[category].append(video[1])
        
        # Analyser les patterns de dur√©e
        if 2 <= avg_top_minutes <= 6:
            what_works.append(f"‚úÖ **Dur√©e optimale** : Vos top vid√©os font {avg_top_minutes:.1f}min en moyenne, ce qui correspond √† l'engagement optimal pour le tourisme en {region_key.replace('_', ' ').title()}")
        elif avg_top_minutes < 2:
            what_works.append(f"‚úÖ **Format court efficace** : Vos vid√©os de {avg_top_minutes:.1f}min g√©n√®rent {avg_top_views:,.0f} vues en moyenne, format adapt√© aux r√©seaux sociaux")
        
        # Analyser les cat√©gories performantes
        best_category = max(category_performance.keys(), key=lambda cat: sum(category_performance[cat])/len(category_performance[cat]))
        best_category_avg = sum(category_performance[best_category]) / len(category_performance[best_category])
        
        if best_category == 'hub':
            what_works.append(f"‚úÖ **Contenu HUB efficace** : Vos vid√©os HUB g√©n√®rent {best_category_avg:,.0f} vues en moyenne, excellent pour l'industrie du tourisme")
        elif best_category == 'hero':
            what_works.append(f"‚úÖ **Contenu HERO impactant** : Vos √©v√©nements et nouveaut√©s g√©n√®rent {best_category_avg:,.0f} vues, parfait pour cr√©er le buzz")
        elif best_category == 'help':
            what_works.append(f"‚úÖ **Contenu HELP utile** : Vos tutoriels g√©n√®rent {best_category_avg:,.0f} vues, r√©pondant aux besoins de votre audience")
    
    # Analyse des ratios de contenu
    if hero_ratio >= benchmarks['hero_ratio_benchmark']:
        what_works.append(f"‚úÖ **Bon √©quilibre HERO** : {hero_ratio:.1f}% de contenu √©v√©nementiel, au-dessus du benchmark secteur ({benchmarks['hero_ratio_benchmark']}%)")
    
    if hub_ratio >= benchmarks['hub_ratio_benchmark']:
        what_works.append(f"‚úÖ **Excellent contenu HUB** : {hub_ratio:.1f}% de contenu destination, optimal pour l'industrie du tourisme")
    
    if help_ratio >= benchmarks['help_ratio_benchmark']:
        what_works.append(f"‚úÖ **Bon support client** : {help_ratio:.1f}% de contenu d'aide, excellente strat√©gie de service")
    
    # Analyse des titres des top vid√©os
    top_titles = [video[0].lower() for video in top_videos if video[0]]
    engagement_keywords = [keyword for keyword in benchmarks['engagement_topics'] if any(keyword in title for title in top_titles)]
    
    if engagement_keywords:
        what_works.append(f"‚úÖ **Th√©matiques porteuses** : Vos vid√©os sur {', '.join(engagement_keywords[:3])} g√©n√®rent le plus d'engagement dans votre r√©gion")
    
    return what_works

def analyze_what_doesnt_work(bottom_videos: List, benchmarks: Dict, region_key: str, avg_duration: float, hero_ratio: float, hub_ratio: float, help_ratio: float, avg_views: float) -> List[str]:
    """
    Analyse ce qui ne fonctionne pas pour une cha√Æne    """
    what_doesnt_work = []
    
    if not bottom_videos:
        return ["‚ùå Donn√©es insuffisantes pour analyser les sous-performances"]
    
    # Analyse des vid√©os les moins performantes
    avg_bottom_views = sum(video[1] for video in bottom_videos) / len(bottom_videos)
    
    # Analyse des dur√©es des vid√©os peu performantes
    bottom_durations = [video[4] for video in bottom_videos if video[4] and video[4] > 0]
    if bottom_durations:
        avg_bottom_duration = sum(bottom_durations) / len(bottom_durations)
        avg_bottom_minutes = avg_bottom_duration / 60
        
        # Identifier les cat√©gories qui sous-performent
        category_underperformance = {}
        for video in bottom_videos:
            category = video[2] or 'non-class√©'
            if category not in category_underperformance:
                category_underperformance[category] = []
            category_underperformance[category].append(video[1])
        
        # Analyser les patterns de dur√©e probl√©matiques
        if avg_bottom_minutes > 8:
            what_doesnt_work.append(f"‚ùå **Vid√©os trop longues** : Vos vid√©os de {avg_bottom_minutes:.1f}min ne g√©n√®rent que {avg_bottom_views:,.0f} vues. Optimal : 3-6min pour le tourisme")
        elif avg_bottom_minutes < 1:
            what_doesnt_work.append(f"‚ùå **Contenu trop court** : Vos vid√©os de {avg_bottom_minutes:.1f}min manquent de substance ({avg_bottom_views:,.0f} vues). D√©veloppez plus le contenu")
        
        # Analyser les cat√©gories sous-performantes
        if category_underperformance:
            worst_category = min(category_underperformance.keys(), key=lambda cat: sum(category_underperformance[cat])/len(category_underperformance[cat]))
            worst_category_avg = sum(category_underperformance[worst_category]) / len(category_underperformance[worst_category])
            
            if worst_category == 'help' and worst_category_avg < benchmarks['avg_views_benchmark'] * 0.5:
                what_doesnt_work.append(f"‚ùå **Contenu HELP inefficace** : Vos tutoriels ne g√©n√®rent que {worst_category_avg:,.0f} vues. Probl√®me : manque de recherche des besoins clients")
    
    # Analyse des ratios de contenu
    if hero_ratio < benchmarks['hero_ratio_benchmark']:
        what_doesnt_work.append(f"‚ùå **Manque de contenu HERO** : {hero_ratio:.1f}% seulement vs {benchmarks['hero_ratio_benchmark']}% recommand√©. Vos actualit√©s ne cr√©ent pas assez de buzz")
    
    if hub_ratio < benchmarks['hub_ratio_benchmark']:
        what_doesnt_work.append(f"‚ùå **D√©ficit contenu HUB** : {hub_ratio:.1f}% vs {benchmarks['hub_ratio_benchmark']}% optimal. Vos destinations ne sont pas assez mises en avant")
    
    if help_ratio < benchmarks['help_ratio_benchmark']:
        what_doesnt_work.append(f"‚ùå **Support client insuffisant** : {help_ratio:.1f}% de contenu d'aide vs {benchmarks['help_ratio_benchmark']}% recommand√©. Vos clients cherchent plus d'informations pratiques")
    
    # Analyse de la performance globale
    if avg_views < benchmarks['avg_views_benchmark']:
        performance_gap = (benchmarks['avg_views_benchmark'] - avg_views) / benchmarks['avg_views_benchmark'] * 100
        what_doesnt_work.append(f"‚ùå **Sous-performance globale** : {avg_views:,.0f} vues/vid√©o vs {benchmarks['avg_views_benchmark']:,.0f} dans votre secteur (-{performance_gap:.0f}%)")
    
    # Analyse des titres probl√©matiques
    bottom_titles = [video[0].lower() for video in bottom_videos if video[0]]
    problematic_keywords = [keyword for keyword in benchmarks['low_engagement_topics'] if any(keyword in title for title in bottom_titles)]
    
    if problematic_keywords:
        what_doesnt_work.append(f"‚ùå **Th√©matiques peu engageantes** : Vos vid√©os sur {', '.join(problematic_keywords[:3])} g√©n√®rent peu d'engagement dans votre r√©gion")
    
    return what_doesnt_work
