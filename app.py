import os
import json
from datetime import datetime, timedelta
from functools import wraps

from dotenv import load_dotenv
from flask import (Flask, flash, jsonify, redirect, render_template, request,
                   session, url_for)
from flask_migrate import Migrate
from flask_session import Session
from flask_sqlalchemy import SQLAlchemy
from flask_caching import Cache
import sqlite3
import subprocess
import time
import requests
import hashlib

# Charger les variables d'environnement
load_dotenv()

# Cr√©ation de l'application Flask
app = Flask(__name__)

# --- CONFIGURATION ---
# Cl√© secr√®te pour les sessions et autres
app.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY', 'a-very-secret-key')

# Configuration de la session
app.config['SESSION_TYPE'] = 'filesystem'
Session(app)

# Configuration du cache
app.config['CACHE_TYPE'] = 'simple'  # Cache en m√©moire simple pour commencer
app.config['CACHE_DEFAULT_TIMEOUT'] = 300  # 5 minutes par d√©faut
cache = Cache(app)

# --- BASE DE DONN√âES ---
# Chemin vers le fichier de BDD
app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL') or 'sqlite:///instance/database.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# Initialisation de SQLAlchemy et Migrate
db = SQLAlchemy(app)
migrate = Migrate(app, db)

# --- MOD√àLES DE BASE DE DONN√âES ---
class Concurrent(db.Model):
    """Mod√®le pour les concurrents/cha√Ænes YouTube"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    channel_id = db.Column(db.String(100), unique=True, nullable=False)
    channel_url = db.Column(db.String(200), unique=True, nullable=False)
    thumbnail_url = db.Column(db.String(200))
    banner_url = db.Column(db.String(200))
    description = db.Column(db.Text)
    subscriber_count = db.Column(db.Integer)
    view_count = db.Column(db.BigInteger)
    video_count = db.Column(db.Integer)
    country = db.Column(db.String(50))
    language = db.Column(db.String(50))
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Relation avec les vid√©os
    videos = db.relationship('Video', backref='concurrent', lazy=True, cascade="all, delete-orphan")

    def __repr__(self):
        return f'<Concurrent {self.name}>'


class Video(db.Model):
    """Mod√®le pour les vid√©os YouTube"""
    id = db.Column(db.Integer, primary_key=True)
    concurrent_id = db.Column(db.Integer, db.ForeignKey('concurrent.id'), nullable=False)
    
    # --- Informations de base ---
    video_id = db.Column(db.String(50), unique=True, nullable=False)
    title = db.Column(db.String(200), nullable=False)
    description = db.Column(db.Text)
    url = db.Column(db.String(200), nullable=False)
    thumbnail_url = db.Column(db.String(200))
    
    # --- Dates et dur√©e ---
    published_at = db.Column(db.DateTime)
    duration_seconds = db.Column(db.Integer)
    duration_text = db.Column(db.String(20))  # ex: "10:23"
    is_short = db.Column(db.Boolean, default=False)  # True si vid√©o ‚â§ 60 secondes
    
    # --- M√©triques objectives ---
    view_count = db.Column(db.BigInteger)
    like_count = db.Column(db.Integer)
    comment_count = db.Column(db.Integer)
    
    # --- Classification et analyse ---
    category = db.Column(db.String(50))  # 'hero', 'hub', 'help'
    tags = db.Column(db.Text)  # JSON string des tags
    
    # --- Crit√®res subjectifs (scores sur 10) ---
    beauty_score = db.Column(db.Integer)  # 1-10
    emotion_score = db.Column(db.Integer)  # 1-10
    info_quality_score = db.Column(db.Integer)  # 1-10
    
    # --- M√©tadonn√©es ---
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow)
    
    def __repr__(self):
        return f'<Video {self.title}>'

# --- IMPORTS DES MODULES ---
from yt_channel_analyzer.storage import load_data
from yt_channel_analyzer.analysis import hero_hub_help_matrix
from yt_channel_analyzer.scraper import autocomplete_youtube_channels
from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api, autocomplete_youtube_channels_api
from yt_channel_analyzer.youtube_api_client import get_api_quota_status
from yt_channel_analyzer.auth import verify_credentials, is_authenticated, authenticate_session, logout_session

# D√©corateur de connexion requis
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not is_authenticated(session):
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

# Import des fonctions de base de donn√©es
from yt_channel_analyzer.database import competitors_to_legacy_format, get_all_competitors, save_competitor_and_videos, get_db_connection, get_classification_patterns

# Import des modules ViewStats supprim√©s - analyse locale maintenant

# Fonctions utilitaires pour la compatibilit√© avec l'ancien format JSON
def load_cache():
    """Charger les concurrents depuis la base de donn√©es (format legacy)"""
    try:
        return competitors_to_legacy_format()
    except Exception as e:
        print(f"Erreur lors du chargement depuis la base: {e}")
        return {}

def save_cache(data):
    """Sauvegarder dans la base de donn√©es (pour compatibilit√©)"""
    # Cette fonction est conserv√©e pour compatibilit√© mais n'est plus utilis√©e
    # Les donn√©es sont maintenant directement sauv√©es en base via les fonctions database.py
    print("‚ö†Ô∏è save_cache() est deprecated - utilisez les fonctions database.py")

def get_channel_key(channel_url):
    """G√©n√©rer une cl√© unique pour une cha√Æne"""
    return channel_url.replace('/', '_').replace(':', '_').replace('?', '_').replace('&', '_')

def extract_channel_name(channel_url):
    """Extraire le nom de la cha√Æne depuis l'URL"""
    try:
        parts = channel_url.split('/')
        if 'channel/' in channel_url:
            return parts[-1][:20]
        elif '@' in channel_url:
            return parts[-1][:20]
        else:
            return "Canal"[:20]
    except:
        return "Canal"

def calculate_total_views(videos):
    """Calculer le total des vues pour une liste de vid√©os"""
    total = 0
    for video in videos:
        views_str = str(video.get('views', '0')).replace(',', '').replace(' ', '')
        try:
            views_num = int(views_str) if views_str.isdigit() else 0
            total += views_num
        except:
            continue
    return total

def format_number(num):
    """Formater un nombre avec des s√©parateurs de milliers"""
    try:
        return f"{int(num):,}".replace(',', ' ')
    except:
        return str(num)

# Enregistrer le filtre format_number pour les templates Jinja2
app.jinja_env.filters['format_number'] = format_number

def save_competitor_data(channel_url, videos):
    """Sauvegarder/enrichir intelligemment les donn√©es d'un concurrent"""
    try:
        from yt_channel_analyzer.database import refresh_competitor_data
        from yt_channel_analyzer.youtube_api_client import create_youtube_client
        
        print(f"[SAVE] üíæ Sauvegarde intelligente de {len(videos)} vid√©os pour {channel_url}")
        
        # R√©cup√©rer les infos de la cha√Æne pour enrichir
        channel_info = None
        try:
            youtube_client = create_youtube_client()
            channel_info = youtube_client.get_channel_info(channel_url)
            print(f"[SAVE] üìä Infos cha√Æne: {channel_info.get('title', 'N/A')}")
        except Exception as e:
            print(f"[SAVE] ‚ö†Ô∏è Impossible de r√©cup√©rer les infos cha√Æne: {e}")
        
        # Utiliser le refresh intelligent (cr√©ation + enrichissement automatique)
        result = refresh_competitor_data(channel_url, videos, channel_info)
        
        if result['success']:
            action = result['action']
            competitor_name = result.get('competitor_name', 'Concurrent')
            competitor_id = result['competitor_id']
            
            if action == 'created':
                print(f"[SAVE] ‚úÖ NOUVEAU concurrent cr√©√©: {competitor_name} (ID: {competitor_id})")
            else:  # refreshed
                print(f"[SAVE] üîÑ ENRICHISSEMENT: {competitor_name} (ID: {competitor_id})")
                print(f"[SAVE] üìà Nouvelles: {result['new_videos']}, "
                      f"Enrichies: {result['enriched_videos']}, "
                      f"Total: {result['total_videos']}")
            
            return competitor_id
        else:
            raise Exception(result.get('error', 'Erreur inconnue lors de la sauvegarde'))
        
    except Exception as e:
        print(f"[SAVE] ‚ùå Erreur critique: {e}")
        raise

# --- ROUTES DE L'APPLICATION ---

@app.route('/')
@login_required
def home():
    from yt_channel_analyzer.database import get_db_connection
    import sqlite3
    
    try:
        # Utiliser des requ√™tes SQL directes pour calculer les statistiques
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Calculer le nombre de concurrents
        cursor.execute('SELECT COUNT(*) FROM concurrent')
        total_competitors = cursor.fetchone()[0]
        
        # Calculer le nombre total de vid√©os
        cursor.execute('SELECT COUNT(*) FROM video')
        total_videos = cursor.fetchone()[0]
        
        # Calculer le total des vues
        cursor.execute('SELECT SUM(view_count) FROM video WHERE view_count IS NOT NULL')
        total_views = cursor.fetchone()[0] or 0
        
        # Calculer le nombre de playlists
        cursor.execute('SELECT COUNT(*) FROM playlist')
        total_playlists = cursor.fetchone()[0]
        
        # Calculer le nombre de pays distincts
        cursor.execute('SELECT COUNT(DISTINCT country) FROM concurrent WHERE country IS NOT NULL AND country != ""')
        total_countries = cursor.fetchone()[0]
        
        conn.close()
        
        # Formater les vues avec des abr√©viations
        def format_views(views):
            if views >= 1000000:
                return f"{views/1000000:.1f}M"
            elif views >= 1000:
                return f"{views/1000:.1f}K"
            else:
                return str(views)
        
        stats = {
            'total_competitors': total_competitors,
            'total_videos': total_videos,
            'total_views': total_views,
            'total_views_formatted': format_views(total_views),
            'total_playlists': total_playlists,
            'countries': total_countries
        }
        
        print(f"[HOME] üìä Statistiques calcul√©es: {total_competitors} concurrents, {total_videos} vid√©os, {total_views:,} vues, {total_playlists} playlists")
        
    except Exception as e:
        print(f"[HOME] ‚ùå Erreur lors du calcul des statistiques: {e}")
        import traceback
        traceback.print_exc()
        
        # Valeurs par d√©faut en cas d'erreur
        stats = {
            'total_competitors': 0,
            'total_videos': 0,
            'total_views': 0,
            'total_views_formatted': '0',
            'total_playlists': 0,
            'countries': 0
        }
    
    return render_template('home_modern.html', stats=stats)

@app.route('/home_old')
@login_required
def home_old():
    return render_template('home.html')

@app.route('/dashboard-glass')
@login_required
def dashboard_glass():
    """Page de d√©monstration du syst√®me glassmorphism avanc√©"""
    return render_template('dashboard_glass.html')

@app.route('/concurrents')
@login_required
def concurrents():
    """
    Afficher la liste de tous les concurrents avec leurs statistiques
    """
    from yt_channel_analyzer.database.competitors import get_all_competitors_with_videos
    from yt_channel_analyzer.database import calculate_publication_frequency, update_competitor_tags
    
    # R√©cup√©rer tous les concurrents avec leurs vid√©os (d√©j√† pr√©calcul√©)
    competitors = get_all_competitors_with_videos()
    
    # NEW: ensure local cached thumbnails
    thumbnails_dir = os.path.join('static', 'competitors', 'images')
    os.makedirs(thumbnails_dir, exist_ok=True)
    
    for competitor in competitors:
        thumb_local_rel = None
        comp_id = competitor.get('id')
        if comp_id is None:
            continue
        local_filename = f"{comp_id}.jpg"
        local_path = os.path.join(thumbnails_dir, local_filename)
        # If already downloaded, just point to it
        if os.path.exists(local_path):
            thumb_local_rel = f"/static/competitors/images/{local_filename}"
        else:
            remote_url = competitor.get('thumbnail_url')
            if remote_url:
                try:
                    resp = requests.get(remote_url, timeout=5)
                    if resp.status_code == 200 and resp.content:
                        with open(local_path, 'wb') as img_f:
                            img_f.write(resp.content)
                        thumb_local_rel = f"/static/competitors/images/{local_filename}"
                except Exception as download_err:
                    print(f"[THUMBNAIL] Failed to download for competitor {comp_id}: {download_err}")
        competitor['thumbnail_local'] = thumb_local_rel
    
    # Calculer les fr√©quences de publication
    # frequency_data = calculate_publication_frequency()
    
    # NOUVEAU : R√©cup√©rer directement depuis la table competitor_frequency_stats
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        SELECT competitor_id, avg_videos_per_week, frequency_hero, frequency_hub, frequency_help
        FROM competitor_frequency_stats
    ''')
    frequency_results = cursor.fetchall()
    conn.close()
    
    # Cr√©er un dictionnaire pour acc√®s rapide
    frequency_data = {}
    for row in frequency_results:
        competitor_id, avg_per_week, freq_hero, freq_hub, freq_help = row
        frequency_data[competitor_id] = {
            'avg_videos_per_week': avg_per_week,
            'hero_frequency': freq_hero,
            'hub_frequency': freq_hub,
            'help_frequency': freq_help
        }
    
    # Enrichir les donn√©es des concurrents avec la fr√©quence (SANS recalculer les stats)
    for competitor in competitors:
        competitor_id = competitor['id']
        frequency_competitor_data = frequency_data.get(competitor_id, {})
        
        # Ajouter les donn√©es de fr√©quence au dictionnaire du concurrent
        competitor.update({
            'frequency_total': frequency_competitor_data.get('avg_videos_per_week', 0),
            'frequency_hero': frequency_competitor_data.get('hero_frequency', 0),
            'frequency_hub': frequency_competitor_data.get('hub_frequency', 0),
            'frequency_help': frequency_competitor_data.get('help_frequency', 0)
        })

    # Trier les concurrents par nombre de vid√©os (d√©j√† fait dans la requ√™te SQL, mais double-s√©curit√©)
    competitors.sort(key=lambda x: x.get('video_count', 0), reverse=True)
    
    return render_template(
        'concurrents.html', 
        competitors=competitors,
        page_title="Concurrents",
        body_class="body-glassmorphism",
        active_page="concurrents"
    )

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')
        
        if verify_credentials(username, password):
            authenticate_session(session)
            return redirect(url_for('home'))
        else:
            return render_template('login_modern.html', error='Identifiants incorrects')
    
    return render_template('login_modern.html')

@app.route('/logout')
def logout():
    logout_session(session)
    return redirect(url_for('login'))

@app.route('/settings', methods=['GET', 'POST'])
@login_required
def settings():
    """Page des param√®tres (r√®gles m√©tiers uniquement)"""
    if request.method == 'POST':
        # Sauvegarder les param√®tres
        settings_data = {
            'paid_threshold': int(request.form.get('paid_threshold', 10000)),
            'industry': request.form.get('industry', 'tourism'),
            'auto_classify': 'auto_classify' in request.form,
            'max_videos': int(request.form.get('max_videos', 1000)),
            'cache_duration': int(request.form.get('cache_duration', 24))
        }
        
        save_settings(settings_data)
        print(f"[SETTINGS] Param√®tres sauvegard√©s: {settings_data}")
        
        return render_template('settings.html', 
                             current_settings=settings_data,
                             message="Param√®tres sauvegard√©s avec succ√®s !")
    
    current_settings = load_settings()
    
    # R√©cup√©rer les patterns de classification multilingues
    patterns = get_classification_patterns()
    
    # R√©cup√©rer les concurrents pour la section d'analyse
    competitors = get_all_competitors()
    
    # R√©cup√©rer les statistiques de la supervision humaine
    from yt_channel_analyzer.supervised_learning import get_human_classifications
    human_stats = get_human_classifications(limit=0, offset=0) # limit=0 pour ne r√©cup√©rer que les stats

    return render_template('settings.html', 
                         current_settings=current_settings,
                         patterns=patterns,
                         competitors=competitors,
                         human_stats=human_stats) # Passer les stats au template

def load_settings():
    """Charger les param√®tres depuis le fichier"""
    settings_file = 'settings.json'
    try:
        with open(settings_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # Param√®tres par d√©faut
        return {
            'paid_threshold': 10000,
            'industry': 'tourism',
            'auto_classify': True,
            'max_videos': 1000,
            'cache_duration': 24
        }

def save_settings(settings_data):
    """Sauvegarder les param√®tres dans le fichier"""
    settings_file = 'settings.json'
    try:
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Erreur lors de la sauvegarde des param√®tres: {e}")

@app.route('/api/tasks', methods=['GET'])
@login_required
def get_tasks():
    """R√©cup√®re toutes les t√¢ches"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        tasks = task_manager.get_all_tasks()
        return jsonify([task.to_dict() for task in tasks])
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/tasks/<task_id>/delete', methods=['DELETE'])
@login_required
def delete_task(task_id):
    """Supprime d√©finitivement une t√¢che et ses donn√©es"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        task_manager.delete_task(task_id)
        return jsonify({'success': True, 'message': 'T√¢che supprim√©e d√©finitivement'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/clean-duplicates', methods=['POST'])
@login_required
def clean_duplicate_tasks():
    """Nettoie les t√¢ches en double en gardant celle avec le plus de vid√©os"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        deleted_count = task_manager.clean_duplicate_tasks()
        
        if deleted_count > 0:
            message = f'Nettoyage termin√©: {deleted_count} doublons supprim√©s'
        else:
            message = 'Aucun doublon trouv√©'
            
        return jsonify({
            'success': True, 
            'message': message,
            'deleted_count': deleted_count
        })
    except Exception as e:
        print(f"[CLEAN-DUPLICATES] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/launch-background-task', methods=['POST'])
@login_required
def launch_background_task():
    """Lance une nouvelle t√¢che d'analyse en arri√®re-plan depuis la page concurrents"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        data = request.get_json()
        channel_url = data.get('channel_url')
        task_type = data.get('task_type', 'relaunch')
        
        if not channel_url:
            return jsonify({'success': False, 'error': 'URL de la cha√Æne manquante'})
        
        # Extraire le nom de la cha√Æne depuis la base de donn√©es ou l'URL
        channel_name = "Canal"
        try:
            from yt_channel_analyzer.database import get_competitor_by_url
            competitor = get_competitor_by_url(channel_url)
            if competitor:
                channel_name = competitor.get('name', extract_channel_name(channel_url))
            else:
                channel_name = extract_channel_name(channel_url)
        except:
            channel_name = extract_channel_name(channel_url)
        
        # Cr√©er la t√¢che avec un nom descriptif
        task_name = f"Relancement - {channel_name}"
        task_id = task_manager.create_task(channel_url, task_name)
        
        # Lancer le scraping en arri√®re-plan
        task_manager.start_background_scraping(task_id, channel_url)
        
        return jsonify({
            'success': True, 
            'task_id': task_id,
            'message': f'Analyse relanc√©e en arri√®re-plan pour {channel_name}'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/tasks')
@login_required
def tasks_page():
    """Page des t√¢ches en cours"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        tasks = task_manager.get_all_tasks_with_warnings()
        return render_template('tasks_modern.html', tasks=tasks)
    except Exception as e:
        return render_template('tasks_modern.html', tasks=[], error=str(e))

@app.route('/tasks_old')
@login_required
def tasks_page_old():
    """Page des t√¢ches en cours (ancienne version)"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        tasks = task_manager.get_all_tasks()
        return render_template('tasks.html', tasks=tasks)
    except Exception as e:
        return render_template('tasks.html', tasks=[], error=str(e))

# ANCIENNES ROUTES SUPPRIM√âES - Maintenant on utilise /api/refresh-competitor qui fait tout

@app.route('/api/refresh-competitor', methods=['POST'])
@login_required
def refresh_competitor():
    """Rafra√Æchir intelligemment les donn√©es d'un concurrent existant"""
    try:
        data = request.get_json() if request.is_json else request.form
        channel_url = data.get('channel_url', '').strip()
        competitor_id = data.get('competitor_id')
        
        if not channel_url and not competitor_id:
            return jsonify({'success': False, 'error': 'URL de cha√Æne ou ID concurrent requis'})
        
        # Si on a seulement l'ID, r√©cup√©rer l'URL
        if competitor_id and not channel_url:
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT channel_url FROM concurrent WHERE id = ?', (competitor_id,))
            result = cursor.fetchone()
            conn.close()
            
            if not result:
                return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
            channel_url = result[0]
        
        print(f"[REFRESH-API] üîÑ D√©but du rafra√Æchissement pour: {channel_url}")
        
        # R√©cup√©rer les donn√©es fra√Æches via l'API YouTube
        try:
            from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api
            from yt_channel_analyzer.youtube_api_client import create_youtube_client
            
            # R√©cup√©rer les vid√©os fra√Æches (limite √©lev√©e pour avoir le maximum)
            fresh_videos = get_channel_videos_data_api(channel_url, video_limit=1000)
            
            if not fresh_videos:
                return jsonify({'success': False, 'error': 'Aucune vid√©o trouv√©e lors du rafra√Æchissement'})
            
            # R√©cup√©rer aussi les infos de la cha√Æne
            youtube_client = create_youtube_client()
            channel_info = youtube_client.get_channel_info(channel_url)
            
            print(f"[REFRESH-API] üìä {len(fresh_videos)} vid√©os r√©cup√©r√©es via API")
            
            # Utiliser la nouvelle fonction de rafra√Æchissement intelligent
            from yt_channel_analyzer.database import refresh_competitor_data
            result = refresh_competitor_data(channel_url, fresh_videos, channel_info)
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'message': f"Rafra√Æchissement termin√© pour {result.get('competitor_name', 'le concurrent')}",
                    'stats': {
                        'total_videos': result['total_videos'],
                        'new_videos': result['new_videos'],
                        'updated_videos': result['updated_videos'],
                        'enriched_videos': result['enriched_videos'],
                        'total_processed': result['total_processed']
                    },
                    'competitor_id': result['competitor_id']
                })
            else:
                return jsonify({'success': False, 'error': result.get('error', 'Erreur inconnue')})
                
        except Exception as api_error:
            print(f"[REFRESH-API] ‚ùå Erreur API: {api_error}")
            return jsonify({'success': False, 'error': f'Erreur lors de la r√©cup√©ration des donn√©es: {str(api_error)}'})
        
    except Exception as e:
        print(f"[REFRESH-API] ‚ùå Erreur g√©n√©rale: {e}")
        return jsonify({'success': False, 'error': str(e)})

# ========================================
# API USAGE MONITORING ROUTES
# ========================================

def load_api_quota_data():
    """Charger les donn√©es de quota API"""
    quota_file = "api_quota_tracking.json"
    try:
        with open(quota_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        # Cr√©er le fichier s'il n'existe pas
        default_data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "quota_used": 0,
            "requests_made": 0,
            "last_updated": datetime.now().isoformat()
        }
        save_api_quota_data(default_data)
        return default_data
    except Exception as e:
        print(f"Erreur lors du chargement des donn√©es de quota: {e}")
        return {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "quota_used": 0,
            "requests_made": 0,
            "last_updated": datetime.now().isoformat()
        }

def save_api_quota_data(data):
    """Sauvegarder les donn√©es de quota API"""
    quota_file = "api_quota_tracking.json"
    try:
        with open(quota_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Erreur lors de la sauvegarde des donn√©es de quota: {e}")

@app.route('/api-usage')
@login_required
def api_usage_page():
    """Page de monitoring de l'utilisation de l'API YouTube"""
    return render_template('api_usage.html')

@app.route('/debug')
@login_required
def debug_page():
    """Page des outils de debug et de propagation"""
    return render_template('debug.html')

@app.route('/api/usage')
@login_required
def get_api_usage():
    """R√©cup√©rer les donn√©es d'utilisation de l'API au format JSON"""
    try:
        # Charger les donn√©es du fichier quota
        quota_data = load_api_quota_data()
        
        # R√©cup√©rer les donn√©es depuis l'API YouTube si possible
        try:
            from yt_channel_analyzer.youtube_api_client import get_api_quota_status
            api_status = get_api_quota_status()
            
            # Merger les donn√©es
            quota_used = max(quota_data.get('quota_used', 0), api_status.get('quota_used', 0))
            requests_made = max(quota_data.get('requests_made', 0), api_status.get('requests_made', 0))
        except Exception as e:
            print(f"Erreur r√©cup√©ration status API: {e}")
            quota_used = quota_data.get('quota_used', 0)
            requests_made = quota_data.get('requests_made', 0)
        
        # Constantes
        daily_quota = 10000  # Quota quotidien par d√©faut YouTube API
        
        # Calculer les statistiques
        percentage = (quota_used / daily_quota) * 100 if daily_quota > 0 else 0
        remaining = max(0, daily_quota - quota_used)
        
        # D√©terminer le statut
        if percentage > 90:
            status = 'critical'
        elif percentage > 80:
            status = 'warning'
        else:
            status = 'healthy'
        
        # V√©rifier si on doit r√©initialiser (nouveau jour)
        current_date = datetime.now().strftime("%Y-%m-%d")
        if quota_data.get('date') != current_date:
            # Nouveau jour, r√©initialiser les compteurs
            quota_data.update({
                'date': current_date,
                'quota_used': 0,
                'requests_made': 0,
                'last_updated': datetime.now().isoformat()
            })
            save_api_quota_data(quota_data)
            
            # Recalculer avec les nouvelles valeurs
            quota_used = 0
            requests_made = 0
            percentage = 0
            remaining = daily_quota
            status = 'healthy'
        
        return jsonify({
            'success': True,
            'date': quota_data.get('date', current_date),
            'today_usage': quota_used,
            'quota_used': quota_used,
            'requests_made': requests_made,
            'daily_quota': daily_quota,
            'percentage': round(percentage, 1),
            'remaining': remaining,
            'status': status,
            'last_updated': quota_data.get('last_updated', datetime.now().isoformat())
        })
        
    except Exception as e:
        print(f"Erreur dans get_api_usage: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'date': datetime.now().strftime("%Y-%m-%d"),
            'today_usage': 0,
            'quota_used': 0,
            'requests_made': 0,
            'daily_quota': 10000,
            'percentage': 0,
            'remaining': 10000,
            'status': 'unknown',
            'last_updated': datetime.now().isoformat()
        }), 500

@app.route('/api/usage/reset', methods=['POST'])
@login_required
def reset_api_usage():
    """R√©initialiser le compteur de quota API"""
    try:
        # R√©initialiser les donn√©es
        reset_data = {
            'date': datetime.now().strftime("%Y-%m-%d"),
            'quota_used': 0,
            'requests_made': 0,
            'last_updated': datetime.now().isoformat()
        }
        
        save_api_quota_data(reset_data)
        
        return jsonify({
            'success': True,
            'message': 'Quota counter reset successfully',
            'data': reset_data
        })
        
    except Exception as e:
        print(f"Erreur lors de la r√©initialisation: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to reset quota: {str(e)}'
        }), 500

@app.route('/autocomplete')
@login_required
def autocomplete():
    """Route pour l'autocompl√©tion des cha√Ænes YouTube - EXCLUT les concurrents d√©j√† en base"""
    q = request.args.get('q', '')
    if not q.strip():
        return jsonify([])
    
    # R√©cup√©rer les cha√Ænes d√©j√† en base pour les exclure
    try:
        from yt_channel_analyzer.database import get_all_competitors
        existing_competitors = get_all_competitors()
        existing_urls = set()
        existing_names = set()
        
        for comp in existing_competitors:
            if comp.get('channel_url'):
                existing_urls.add(comp['channel_url'].lower().strip())
            if comp.get('name'):
                existing_names.add(comp['name'].lower().strip())
        
        print(f"[AUTOCOMPLETE] üìã {len(existing_competitors)} concurrents en base pour enrichissement")
        
    except Exception as e:
        print(f"[AUTOCOMPLETE] ‚ö†Ô∏è Erreur r√©cup√©ration concurrents existants: {e}")
        existing_urls = set()
        existing_names = set()
    
    # üöÄ Utiliser l'API YouTube en priorit√©
    try:
        suggestions = autocomplete_youtube_channels_api(q, max_results=15)  # Plus de r√©sultats pour compenser l'enrichissement
        if suggestions:
            # Enrichir les suggestions avec les informations des cha√Ænes d√©j√† en base
            enriched_suggestions = []
            for suggestion in suggestions:
                suggestion_url = suggestion.get('url', '').lower().strip()
                suggestion_name = suggestion.get('name', '').lower().strip()
                
                # V√©rifier si cette cha√Æne est d√©j√† en base
                existing_competitor = None
                for comp in existing_competitors:
                    comp_url = comp.get('channel_url', '').lower().strip()
                    comp_name = comp.get('name', '').lower().strip()
                    
                    if (suggestion_url == comp_url) or (suggestion_name == comp_name):
                        existing_competitor = comp
                        break
                
                if existing_competitor:
                    # Enrichir la suggestion avec les infos de la base
                    suggestion['is_analyzed'] = True
                    suggestion['competitor_id'] = existing_competitor.get('id')
                    suggestion['video_count'] = existing_competitor.get('video_count', 0)
                    suggestion['analysis_date'] = existing_competitor.get('created_at', '')[:10] if existing_competitor.get('created_at') else 'Date inconnue'
                    
                    # Garder la miniature de l'API si disponible, sinon utiliser celle de la base
                    if not suggestion.get('thumbnail') and existing_competitor.get('thumbnail_url'):
                        suggestion['thumbnail'] = existing_competitor['thumbnail_url']
                    
                    print(f"[AUTOCOMPLETE] ‚úÖ Enrichi (d√©j√† en base): {suggestion.get('name', 'Sans nom')} - {suggestion['video_count']} vid√©os")
                else:
                    # Nouvelle cha√Æne, pas encore analys√©e
                    suggestion['is_analyzed'] = False
                    print(f"[AUTOCOMPLETE] üÜï Nouvelle cha√Æne: {suggestion.get('name', 'Sans nom')}")
                
                enriched_suggestions.append(suggestion)
            
            print(f"[API] ‚úÖ {len(enriched_suggestions)} suggestions enrichies pour '{q}'")
            return jsonify(enriched_suggestions[:10])  # Limiter √† 10 r√©sultats finaux
            
    except Exception as e:
        print(f"[API] ‚ùå Erreur autocomplete API: {e}")
    
    # Fallback vers le scraping avec enrichissement
    try:
        print(f"[SCRAPING] üîÑ Fallback autocomplete pour '{q}'")
        suggestions = autocomplete_youtube_channels(q)
        
        # Enrichir les suggestions du scraping aussi
        enriched_suggestions = []
        for suggestion in suggestions:
            suggestion_url = suggestion.get('url', '').lower().strip()
            suggestion_name = suggestion.get('name', '').lower().strip()
            
            # V√©rifier si cette cha√Æne est d√©j√† en base
            existing_competitor = None
            for comp in existing_competitors:
                comp_url = comp.get('channel_url', '').lower().strip()
                comp_name = comp.get('name', '').lower().strip()
                
                if (suggestion_url == comp_url) or (suggestion_name == comp_name):
                    existing_competitor = comp
                    break
            
            if existing_competitor:
                # Enrichir la suggestion avec les infos de la base
                suggestion['is_analyzed'] = True
                suggestion['competitor_id'] = existing_competitor.get('id')
                suggestion['video_count'] = existing_competitor.get('video_count', 0)
                suggestion['analysis_date'] = existing_competitor.get('created_at', '')[:10] if existing_competitor.get('created_at') else 'Date inconnue'
                
                # Utiliser la miniature de la base si pas de miniature du scraping
                if not suggestion.get('thumbnail') and existing_competitor.get('thumbnail_url'):
                    suggestion['thumbnail'] = existing_competitor['thumbnail_url']
            else:
                # Nouvelle cha√Æne
                suggestion['is_analyzed'] = False
            
            enriched_suggestions.append(suggestion)
        
        return jsonify(enriched_suggestions[:10])
        
    except Exception as e:
        print(f"[SCRAPING] ‚ùå Erreur scraping autocomplete: {e}")
        return jsonify([])

@app.route('/api/simple-analysis', methods=['POST'])
@login_required
def simple_analysis():
    """Analyser une cha√Æne de mani√®re simple et directe"""
    try:
        data = request.get_json() if request.is_json else request.form
        channel_url = data.get('channel_url', '').strip()
        
        if not channel_url:
            return jsonify({'success': False, 'error': 'URL de cha√Æne manquante'})
        
        # Valider l'URL YouTube
        if 'youtube.com' not in channel_url and 'youtu.be' not in channel_url:
            return jsonify({'success': False, 'error': 'URL YouTube invalide'})
        
        # Cr√©er une t√¢che pour le suivi
        try:
            from yt_channel_analyzer.background_tasks import task_manager
            channel_name = extract_channel_name(channel_url)
            task_id = task_manager.create_task(channel_url, f"Analyse - {channel_name}")
            
            # Marquer comme en cours
            task_manager.update_task(task_id, 
                current_step='R√©cup√©ration des vid√©os via API YouTube...', 
                progress=20
            )
        except Exception as task_error:
            print(f"[TASK] Erreur cr√©ation t√¢che: {task_error}")
            task_id = None
        
        # Lancer l'analyse directement avec l'API YouTube
        try:
            from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api
            
            print(f"[ANALYSIS] D√©but de l'analyse pour: {channel_url}")
            
            # Mettre √† jour la t√¢che
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Analyse des vid√©os en cours...', 
                    progress=50
                )
            
            # R√©cup√©rer les vid√©os via l'API
            videos = get_channel_videos_data_api(channel_url, video_limit=1000)
            
            if not videos:
                if task_id:
                    task_manager.error_task(task_id, 'Aucune vid√©o trouv√©e')
                return jsonify({'success': False, 'error': 'Aucune vid√©o trouv√©e'})
            
            # Mettre √† jour la t√¢che
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Sauvegarde en base de donn√©es...', 
                    progress=80,
                    videos_found=len(videos)
                )
            
            # Sauvegarder directement dans la base de donn√©es
            competitor_id = save_competitor_data(channel_url, videos)
            
            # üöÄ NOUVEAU : Classification automatique des vid√©os apr√®s sauvegarde
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Classification automatique des vid√©os...', 
                    progress=90,
                    videos_found=len(videos)
                )
            
            try:
                from yt_channel_analyzer.database import classify_videos_directly_with_keywords
                print(f"[ANALYSIS] ü§ñ Lancement de la classification automatique pour {len(videos)} vid√©os")
                
                # Classifier directement les vid√©os de ce concurrent
                classification_result = classify_videos_directly_with_keywords(competitor_id)
                videos_classified = classification_result.get('videos_classified', 0)
                
                if videos_classified > 0:
                    print(f"[ANALYSIS] ‚úÖ Classification termin√©e: {videos_classified} vid√©os classifi√©es automatiquement")
                else:
                    print(f"[ANALYSIS] ‚ÑπÔ∏è Aucune vid√©o √† classifier (d√©j√† classifi√©es)")
                
            except Exception as classification_error:
                print(f"[ANALYSIS] ‚ö†Ô∏è Erreur lors de la classification automatique: {classification_error}")
                # Ne pas bloquer l'analyse si la classification √©choue
            
            # Marquer la t√¢che comme termin√©e
            if task_id:
                task_manager.complete_task(task_id, len(videos))
            
            print(f"[ANALYSIS] Analyse termin√©e: {len(videos)} vid√©os sauvegard√©es")
            
            return jsonify({
                'success': True,
                'videos_count': len(videos),
                'competitor_id': competitor_id,
                'task_id': task_id,
                'message': f'Analyse termin√©e: {len(videos)} vid√©os trouv√©es',
                'redirect': '/concurrents'
            })
            
        except Exception as api_error:
            print(f"[ANALYSIS] Erreur API: {api_error}")
            if task_id:
                task_manager.error_task(task_id, str(api_error))
            return jsonify({'success': False, 'error': f'Erreur lors de l\'analyse: {str(api_error)}'})
        
    except Exception as e:
        print(f"[ANALYSIS] Erreur g√©n√©rale: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/analyze-channel', methods=['GET', 'POST'])
@login_required
def analyze_channel():
    """Route pour analyser une cha√Æne depuis la barre de recherche centrale"""
    if request.method == 'GET':
        # R√©cup√©rer les param√®tres de l'URL
        search_query = request.args.get('search', '')
        channel_url = request.args.get('channel_url', '')
        max_videos = int(request.args.get('max_videos', 30))
        ai_classification = request.args.get('ai_classification', 'true') == 'true'
        period = request.args.get('period', '')
        
        # Si on a une URL directe, on lance l'analyse
        if channel_url:
            return launch_analysis_with_params(channel_url, max_videos, ai_classification, period)
        
        # Sinon, on redirige vers la page d'accueil avec la recherche pr√©-remplie
        return redirect(url_for('home', search=search_query))
    
    # Si POST, traiter les donn√©es du formulaire
    channel_url = request.form.get('channel_url', '').strip()
    search_query = request.form.get('search_query', '').strip()
    max_videos = int(request.form.get('max_videos', 30))
    ai_classification = 'ai_classification' in request.form
    
    if not channel_url and not search_query:
        flash('Veuillez entrer une URL ou le nom d\'une cha√Æne YouTube', 'error')
        return redirect(url_for('home'))
    
    # Si on a seulement une recherche, essayer de trouver l'URL
    if search_query and not channel_url:
        try:
            # Utiliser l'API d'autocomplete pour trouver l'URL
            from yt_channel_analyzer.scraper import autocomplete_youtube_channels
            suggestions = autocomplete_youtube_channels(search_query)
            if suggestions:
                channel_url = suggestions[0]['url']
            else:
                flash(f'Aucune cha√Æne trouv√©e pour "{search_query}"', 'warning')
                return redirect(url_for('home', search=search_query))
        except Exception as e:
            flash(f'Erreur lors de la recherche: {e}', 'error')
            return redirect(url_for('home'))
    
    return launch_analysis_with_params(channel_url, max_videos, ai_classification)

def launch_analysis_with_params(channel_url, max_videos=30, ai_classification=True, period=''):
    """Lance l'analyse avec les param√®tres donn√©s"""
    try:
        # Lancer l'analyse en arri√®re-plan
        channel_name = extract_channel_name(channel_url)
        
        response = fetch('/api/launch-background-task', {
            'method': 'POST',
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({
                'channel_url': channel_url,
                'channel_name': channel_name,
                'max_videos': max_videos,
                'ai_classification': ai_classification,
                'period': period
            })
        })
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                flash(f'Analyse lanc√©e pour "{channel_name}"', 'success')
                return redirect(url_for('tasks_page'))
            else:
                flash(f'Erreur lors du lancement: {result.get("error", "Erreur inconnue")}', 'error')
        else:
            flash('Erreur lors du lancement de l\'analyse', 'error')
            
    except Exception as e:
        print(f"Erreur lors du lancement de l'analyse: {e}")
        flash(f'Erreur lors du lancement de l\'analyse: {e}', 'error')
    
    return redirect(url_for('home'))

@app.route('/competitor/<int:competitor_id>')
@login_required
def competitor_detail(competitor_id):
    conn = get_db_connection()
    # 1. R√©cup√©ration du concurrent
    competitor = conn.execute('SELECT * FROM concurrent WHERE id = ?', (competitor_id,)).fetchone()
    if competitor is None:
        flash("Concurrent non trouv√©.", "error")
        return redirect(url_for('concurrents'))

    # 2. R√©cup√©ration des vid√©os
    videos = conn.execute('SELECT * FROM video WHERE concurrent_id = ? ORDER BY published_at DESC', (competitor_id,)).fetchall()

    # 3. Charger le seuil paid_threshold depuis les settings
    settings = load_settings()
    paid_threshold = settings.get('paid_threshold', 10000)

    # 4. Calcul des statistiques de base avec organic/paid bas√© sur le seuil
    paid_count = sum(1 for v in videos if v['view_count'] and v['view_count'] >= paid_threshold)
    organic_count = len(videos) - paid_count
    
    # 5. Calcul des statistiques de Shorts bas√©es sur les vraies donn√©es de la base
    shorts_count = sum(1 for v in videos if v['is_short'] == 1)
    regular_videos_count = len(videos) - shorts_count
    shorts_views = sum(v['view_count'] for v in videos if v['is_short'] == 1 and v['view_count'])
    regular_views = sum(v['view_count'] for v in videos if v['is_short'] == 0 and v['view_count'])
    total_views = shorts_views + regular_views
    
    # Calcul des m√©triques de Shorts
    shorts_percentage = (shorts_count / len(videos) * 100) if len(videos) > 0 else 0
    avg_views_shorts = shorts_views / shorts_count if shorts_count > 0 else 0
    avg_views_regular = regular_views / regular_videos_count if regular_videos_count > 0 else 0
    performance_relative = (avg_views_shorts / avg_views_regular * 100) if avg_views_regular > 0 else 0
    
    # Shorts par p√©riode (approximation bas√©e sur les donn√©es disponibles)
    shorts_per_week = 0  # Sera calcul√© plus tard si n√©cessaire
    shorts_per_month = shorts_count  # Approximation pour l'instant
    
    # Structure des donn√©es de Shorts
    shorts_data = {
        'shorts_percentage': round(shorts_percentage, 1),
        'shorts_count': shorts_count,
        'total_shorts_views': shorts_views,
        'avg_views_shorts': int(avg_views_shorts),
        'avg_views_regular': int(avg_views_regular),
        'performance_relative': round(performance_relative, 1) if performance_relative > 0 else 'N/A',
        'shorts_per_week': shorts_per_week,
        'shorts_per_month': shorts_per_month
    }
    
    # 5. Calcul de la matrice de performance par cat√©gorie et type
    performance_matrix = {
        'hero': {'organic': [], 'paid': []},
        'hub': {'organic': [], 'paid': []},
        'help': {'organic': [], 'paid': []}
    }
    
    # Remplir la matrice avec les vues des vid√©os
    for video in videos:
        if video['view_count'] is None:
            continue
            
        category = video['category'] or 'hub'  # Default to hub if no category
        views = video['view_count']
        
        if category in performance_matrix:
            if views >= paid_threshold:
                performance_matrix[category]['paid'].append(views)
            else:
                performance_matrix[category]['organic'].append(views)
    
    # 6. Calculer les m√©dianes pour chaque cellule de la matrice
    def calculate_median(values):
        if not values:
            return 0
        values.sort()
        n = len(values)
        return values[n//2] if n % 2 == 1 else (values[n//2-1] + values[n//2]) // 2
    
    performance_matrix_stats = {}
    for category in ['hero', 'hub', 'help']:
        performance_matrix_stats[category] = {
            'organic_median': calculate_median(performance_matrix[category]['organic']),
            'organic_count': len(performance_matrix[category]['organic']),
            'paid_median': calculate_median(performance_matrix[category]['paid']),
            'paid_count': len(performance_matrix[category]['paid'])
        }

    stats = {
        'total_videos': len(videos),
        'total_views': sum(v['view_count'] for v in videos if v['view_count']),
        'total_likes': sum(v['like_count'] for v in videos if v['like_count']),
        'hero_count': sum(1 for v in videos if v['category'] == 'hero'),
        'hub_count': sum(1 for v in videos if v['category'] == 'hub'),
        'help_count': sum(1 for v in videos if v['category'] == 'help'),
        'paid_count': paid_count,
        'organic_count': organic_count,
        'paid_threshold': paid_threshold,
        'performance_matrix': performance_matrix_stats
    }
    stats['paid_percentage'] = round((stats['paid_count'] / stats['total_videos']) * 100) if stats['total_videos'] > 0 else 0
    stats['organic_percentage'] = round((stats['organic_count'] / stats['total_videos']) * 100) if stats['total_videos'] > 0 else 0

    # 4. Top 5 vid√©os par vues et par likes
    top_videos_views = sorted([v for v in videos if v['view_count'] is not None], key=lambda x: x['view_count'], reverse=True)[:5]
    top_videos_likes = sorted([v for v in videos if v['like_count'] is not None], key=lambda x: x['like_count'], reverse=True)[:5]
    
    # 5. Pr√©paration des donn√©es pour le graphique Top 5
    top_videos_json = {
        "labels": [v['title'] for v in top_videos_views],
        "views": [v['view_count'] for v in top_videos_views],
        "likes": [v['like_count'] for v in top_videos_views],
        "video_ids": [v['video_id'] for v in top_videos_views]
    }

    # 6. Donn√©es pour la matrice de performance
    performance_data = {
        "labels": ["Vues", "Likes", "HERO", "HUB", "HELP"],
        "values": [
            stats['total_views'] / stats['total_videos'] if stats['total_videos'] > 0 else 0,
            stats['total_likes'] / stats['total_videos'] if stats['total_videos'] > 0 else 0,
            stats['hero_count'],
            stats['hub_count'],
            stats['help_count']
        ]
    }

    # 7. Donn√©es pour la distribution des cat√©gories
    category_distribution = {
        "labels": ["HERO", "HUB", "HELP"],
        "values": [stats['hero_count'], stats['hub_count'], stats['help_count']]
    }

    # 8. Donn√©es pour la distribution des types de contenu
    content_type_distribution = {
        "labels": ["Pay√©", "Organique"],
        "values": [stats['paid_count'], stats['organic_count']]
    }

    # 9. Donn√©es pour les m√©triques d'engagement
    engagement_metrics = {
        "labels": [v['title'][:20] + '...' for v in top_videos_views],
        "values": [(v['like_count'] / v['view_count']) * 100 if v['view_count'] > 0 else 0 for v in top_videos_views]
    }

    # 10. R√©cup√©ration des playlists avec classifications
    from yt_channel_analyzer.database import get_competitor_playlists
    playlists = get_competitor_playlists(competitor_id)

    # 11. R√©cup√©ration des donn√©es d'abonn√©s
    subscriber_data_db = conn.execute(
        'SELECT date, subscriber_count FROM subscriber_data WHERE concurrent_id = ? ORDER BY date',
        (competitor_id,)
    ).fetchall()
    
    # Formater les donn√©es pour correspondre au format attendu par le JavaScript
    subscriber_data_formatted = None
    if subscriber_data_db:
        data_list = [{"date": r["date"], "subscriber_count": r["subscriber_count"]} for r in subscriber_data_db]
        
        # Calculer les statistiques
        first_entry = data_list[0]
        last_entry = data_list[-1]
        total_growth = last_entry['subscriber_count'] - first_entry['subscriber_count']
        growth_percentage = (total_growth / first_entry['subscriber_count']) * 100 if first_entry['subscriber_count'] > 0 else 0
        
        # Formater les donn√©es pour Chart.js
        chart_data = {
            'labels': [entry['date'] for entry in data_list],
            'datasets': [{
                'label': 'Nombre d\'abonn√©s',
                'data': [entry['subscriber_count'] for entry in data_list],
                'borderColor': 'rgba(99, 102, 241, 1)',
                'backgroundColor': 'rgba(99, 102, 241, 0.1)',
                'borderWidth': 3,
                'fill': True,
                'tension': 0.4,
                'pointBackgroundColor': 'rgba(99, 102, 241, 1)',
                'pointBorderColor': '#ffffff',
                'pointBorderWidth': 2,
                'pointRadius': 6,
                'pointHoverRadius': 8
            }]
        }
        
        # Cr√©er l'objet final avec toutes les donn√©es n√©cessaires
        subscriber_data_formatted = {
            'has_data': True,
            'chart_data': chart_data,
            'stats': {
                'total_entries': len(data_list),
                'first_date': first_entry['date'],
                'last_date': last_entry['date'],
                'first_count': first_entry['subscriber_count'],
                'last_count': last_entry['subscriber_count'],
                'total_growth': total_growth,
                'growth_percentage': round(growth_percentage, 2)
            }
        }
    else:
        subscriber_data_formatted = {
            'has_data': False,
            'chart_data': None,
            'stats': {
                'total_entries': 0,
                'total_growth': 0,
                'growth_percentage': 0
            }
        }

    # 11b. Groupement des vid√©os par cat√©gorie
    videos_by_category = {
        'hero': [v for v in videos if v['category'] == 'hero'],
        'hub': [v for v in videos if v['category'] == 'hub'],
        'help': [v for v in videos if v['category'] == 'help']
    }

    # 12. Rendu du template
    return render_template('concurrent_detail.html',
                           competitor=competitor,
                           videos=videos,
                           stats=stats,
                           top_videos_views=top_videos_views,
                           top_videos_likes=top_videos_likes,
                           top_videos_json=top_videos_json,
                           performance_data=performance_data,
                           category_distribution=category_distribution,
                           content_type_distribution=content_type_distribution,
                           engagement_metrics=engagement_metrics,
                           playlists=playlists,
                           subscriber_data=subscriber_data_formatted,
                           hero_count=stats['hero_count'],
                           hub_count=stats['hub_count'],
                           help_count=stats['help_count'],
                           shorts_data=shorts_data,
                           videos_by_category=videos_by_category)

@app.route('/competitor/<int:competitor_id>/delete')
@login_required
def delete_competitor(competitor_id):
    """Supprime un concurrent et toutes ses donn√©es associ√©es."""
    try:
        conn = get_db_connection()
        # V√©rifier que le concurrent existe
        competitor = conn.execute('SELECT name FROM concurrent WHERE id = ?', (competitor_id,)).fetchone()
        if competitor is None:
            flash("Concurrent non trouv√©.", "error")
            return redirect(url_for('concurrents'))

        # Suppression en cascade (assumant que le sch√©ma a ON DELETE CASCADE)
        conn.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
        conn.commit()
        conn.close()
        
        flash(f'Le concurrent "{competitor["name"]}" et toutes ses donn√©es ont √©t√© supprim√©s avec succ√®s.', 'success')
    except Exception as e:
        flash(f"Une erreur est survenue lors de la suppression : {e}", "error")
    
    return redirect(url_for('concurrents'))

@app.route('/api/delete-competitor', methods=['POST'])
@login_required
def api_delete_competitor():
    """API pour supprimer un concurrent"""
    try:
        data = request.get_json()
        competitor_id = data.get('competitor_id')
        
        if not competitor_id:
            return jsonify({'success': False, 'error': 'ID concurrent manquant'})
        
        # Convertir en entier si n√©cessaire
        try:
            competitor_id = int(competitor_id)
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'ID concurrent invalide'})
        
        # V√©rifier que le concurrent existe et r√©cup√©rer son nom
        conn = db.engine.raw_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT name FROM concurrent WHERE id = ?', (competitor_id,))
        result = cursor.fetchone()
        
        if not result:
            conn.close()
            return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
        
        competitor_name = result[0]
        
        # Supprimer le concurrent (les vid√©os sont supprim√©es automatiquement par cascade)
        cursor.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
        conn.commit()
        conn.close()
        
        print(f"[DELETE-COMPETITOR] Concurrent supprim√©: {competitor_name} (ID: {competitor_id})")
        
        return jsonify({
            'success': True, 
            'message': f'Concurrent {competitor_name} supprim√© avec succ√®s'
        })
        
    except Exception as e:
        print(f"[DELETE-COMPETITOR] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

# --- ROUTES POUR LA GESTION DES PLAYLISTS ---

@app.route('/api/competitor/<int:competitor_id>/playlists/refresh', methods=['POST'])
@login_required
def refresh_competitor_playlists(competitor_id):
    """R√©cup√©rer et sauvegarder les playlists d'un concurrent depuis l'API YouTube"""
    try:
        # üö® PROTECTION 1: V√©rifier le quota API disponible
        quota_data = load_api_quota_data()
        daily_usage = quota_data.get('daily_usage', 0)
        quota_limit = quota_data.get('quota_limit', 10000)
        
        # Estimer le co√ªt de cette op√©ration (channel info + playlists + vid√©os)
        estimated_cost = 150  # Environ 150 unit√©s pour une actualisation compl√®te
        
        if daily_usage + estimated_cost > quota_limit:
            return jsonify({
                'success': False, 
                'error': f'Quota API insuffisant. Utilis√©: {daily_usage}/{quota_limit}. Co√ªt estim√©: {estimated_cost} unit√©s.'
            })
        
        # üö® PROTECTION 2: Rate limiting par concurrent (max 1 fois par heure)
        import time
        current_time = time.time()
        cache_key = f'playlist_refresh_{competitor_id}'
        
        # V√©rifier le cache des derni√®res actualisations
        if hasattr(refresh_competitor_playlists, '_last_refresh'):
            last_refresh = refresh_competitor_playlists._last_refresh.get(cache_key, 0)
            if current_time - last_refresh < 3600:  # 1 heure = 3600 secondes
                remaining_time = int(3600 - (current_time - last_refresh))
                return jsonify({
                    'success': False,
                    'error': f'‚è∞ Actualisation trop r√©cente. R√©essayez dans {remaining_time//60}min {remaining_time%60}s.'
                })
        else:
            refresh_competitor_playlists._last_refresh = {}
        
        # R√©cup√©rer l'URL de la cha√Æne
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT channel_url, name FROM concurrent WHERE id = ?', (competitor_id,))
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
        
        channel_url, competitor_name = result
        
        # üö® PROTECTION 3: Log de l'action pour audit
        print(f"[PLAYLISTS] ‚ö†Ô∏è ACTUALISATION API demand√©e pour {competitor_name} (ID: {competitor_id})")
        print(f"[PLAYLISTS] üìä Quota avant: {daily_usage}/{quota_limit}")
        
        # R√©cup√©rer les playlists depuis l'API YouTube
        from yt_channel_analyzer.youtube_api_client import create_youtube_client
        youtube_client = create_youtube_client()
        
        print(f"[PLAYLISTS] üîç R√©cup√©ration des playlists pour: {channel_url}")
        
        # R√©cup√©rer l'ID de la cha√Æne
        channel_info = youtube_client.get_channel_info(channel_url)
        channel_id = channel_info['id']
        
        # R√©cup√©rer les playlists
        playlists_data = youtube_client.get_channel_playlists(channel_id)
        
        if not playlists_data:
            return jsonify({'success': False, 'error': 'Aucune playlist trouv√©e'})
        
        print(f"[PLAYLISTS] üìã {len(playlists_data)} playlists trouv√©es")
        
        # Sauvegarder les playlists
        from yt_channel_analyzer.database import save_competitor_playlists, link_playlist_videos
        save_competitor_playlists(competitor_id, playlists_data)
        
        # R√©cup√©rer les vid√©os de chaque playlist et les lier (LIMIT√â pour √©conomiser le quota)
        print(f"[PLAYLISTS] üîó Liaison des vid√©os aux playlists...")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        linked_videos = 0
        for playlist_data in playlists_data:
            playlist_id = playlist_data.get('playlist_id')
            if not playlist_id:
                continue
                
            # R√©cup√©rer l'ID de la playlist dans notre base
            cursor.execute('SELECT id FROM playlist WHERE playlist_id = ?', (playlist_id,))
            result = cursor.fetchone()
            if not result:
                continue
                
            playlist_db_id = result[0]
            
            # üö® PROTECTION 4: Limiter √† 50 vid√©os par playlist pour √©conomiser le quota
            video_ids = youtube_client.get_playlist_videos(playlist_id, max_results=50)
            
            if video_ids:
                # Lier les vid√©os √† la playlist
                link_playlist_videos(playlist_db_id, video_ids)
                linked_videos += len(video_ids)
        
        conn.close()
        
        # üö® PROTECTION 5: Enregistrer le timestamp de cette actualisation
        refresh_competitor_playlists._last_refresh[cache_key] = current_time
        
        # Mettre √† jour le quota utilis√©
        quota_data['daily_usage'] = daily_usage + estimated_cost
        save_api_quota_data(quota_data)
        
        print(f"[PLAYLISTS] ‚úÖ Actualisation termin√©e. Quota utilis√©: {estimated_cost} unit√©s")
        
        return jsonify({
            'success': True,
            'count': len(playlists_data),
            'linked_videos': linked_videos,
            'quota_used': estimated_cost,
            'message': f'‚úÖ {len(playlists_data)} playlists r√©cup√©r√©es, {linked_videos} vid√©os li√©es. Quota: {estimated_cost} unit√©s.'
        })
        
    except Exception as e:
        print(f"[PLAYLISTS] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tag-playlist', methods=['POST'])
@login_required  
def tag_playlist():
    """Tagguer une playlist avec une cat√©gorie (hero/hub/help) - NOUVELLE LOGIQUE S√âCURIS√âE"""
    try:
        data = request.get_json()
        playlist_id = data.get('playlist_id')
        category = data.get('category')
        competitor_id = data.get('competitor_id')
        force_propagate = data.get('force_propagate', False)  # Nouveau flag explicite
        
        if not playlist_id:
            return jsonify({'success': False, 'error': 'ID playlist manquant'})
        
        # Valider la cat√©gorie
        valid_categories = ['hero', 'hub', 'help', None]
        if category not in valid_categories:
            return jsonify({'success': False, 'error': 'Cat√©gorie invalide'})
        
        # üö® NOUVEAU : Utiliser la fonction de marquage humain pour cette classification
        from yt_channel_analyzer.database import mark_human_classification
        
        if category:
            # Marquer comme classification humaine (priorit√© absolue)
            success = mark_human_classification(playlist_id=playlist_id, category=category, user_notes='Classification manuelle via interface')
            if not success:
                return jsonify({'success': False, 'error': 'Erreur lors du marquage de la classification humaine'})
        else:
            # Si cat√©gorie = None, mettre √† jour normalement (suppression de cat√©gorie)
            from yt_channel_analyzer.database import update_playlist_category
            update_playlist_category(playlist_id, category)
        
        # üö® NOUVELLE LOGIQUE : NE PAS propager automatiquement les classifications humaines
        # L'utilisateur doit explicitement demander la propagation
        
        videos_updated = 0
        human_protected_count = 0
        human_playlists_skipped = 0
        processed_playlists = []
        propagation_message = ""
        
        if competitor_id and category:
            if force_propagate:
                # L'utilisateur a explicitement demand√© la propagation
                try:
                    from yt_channel_analyzer.database import apply_playlist_categories_to_videos_safe
                    result = apply_playlist_categories_to_videos_safe(
                        competitor_id=competitor_id, 
                        specific_playlist_id=playlist_id,
                        force_human_playlists=True  # Mode force explicite
                    )
                    videos_updated = result.get('videos_updated', 0)
                    human_protected_count = result.get('human_protected', 0)
                    processed_playlists = result.get('processed_playlists', [])
                    
                    propagation_message = f" ‚Ä¢ Propagation FORC√âE: {videos_updated} vid√©os taggu√©es"
                    if human_protected_count > 0:
                        propagation_message += f" ‚Ä¢ {human_protected_count} vid√©os prot√©g√©es"
                        
                    print(f"[TAG-PLAYLIST] ‚ö†Ô∏è PROPAGATION FORC√âE demand√©e par l'utilisateur: {videos_updated} vid√©os")
                    
                except Exception as propagation_error:
                    print(f"[TAG-PLAYLIST] ‚ùå Erreur lors de la propagation forc√©e: {propagation_error}")
                    propagation_message = " ‚Ä¢ Erreur lors de la propagation"
            else:
                # Classification humaine : PAS de propagation automatique
                propagation_message = " ‚Ä¢ Aucune propagation automatique (classification humaine prot√©g√©e)"
                print(f"[TAG-PLAYLIST] üõ°Ô∏è Classification humaine prot√©g√©e - AUCUNE propagation automatique")
        
        category_name = category.upper() if category else "Non cat√©goris√©e"
        
        # Message enrichi avec nouvelle logique de s√©curit√©
        message = f'ü§ñ Playlist marqu√©e comme {category_name} (HUMAIN - PROT√âG√âE){propagation_message}'
        
        return jsonify({
            'success': True,
            'message': message,
            'videos_updated': videos_updated,
            'human_protected': human_protected_count,
            'human_playlists_skipped': human_playlists_skipped,
            'processed_playlists': processed_playlists,
            'human_classification': True,
            'auto_propagated': force_propagate,
            'protection_active': True  # Nouveau flag
        })
        
    except Exception as e:
        print(f"[TAG-PLAYLIST] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitor/<int:competitor_id>/apply-playlist-categories', methods=['POST'])
@login_required
def apply_playlist_categories(competitor_id):
    """Appliquer automatiquement les cat√©gories des playlists aux vid√©os"""
    try:
        from yt_channel_analyzer.database import apply_playlist_categories_to_videos
        
        # Appliquer les cat√©gories
        videos_updated = apply_playlist_categories_to_videos(competitor_id)
        
        return jsonify({
            'success': True,
            'videos_updated': videos_updated,
            'message': f'Classification appliqu√©e: {videos_updated} vid√©os mises √† jour'
        })
        
    except Exception as e:
        print(f"[APPLY-CATEGORIES] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitor/<int:competitor_id>/playlists/ai-classify', methods=['POST'])
@login_required
def ai_classify_playlists(competitor_id):
    """Classifier automatiquement les playlists non cat√©goris√©es avec l'IA"""
    try:
        from yt_channel_analyzer.database import auto_classify_uncategorized_playlists, set_last_action_status
        result = auto_classify_uncategorized_playlists(competitor_id)
        set_last_action_status('last_playlist_classification', {
            'action': 'ai_classify_playlists',
            'competitor_id': competitor_id,
            'timestamp': datetime.now().isoformat(),
            'result': result,
            'message': 'Classification IA des playlists effectu√©e'
        })
        return jsonify(result)
    except Exception as e:
        print(f"[AI-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/competitor/<int:competitor_id>/playlists/classify')
@login_required
def playlist_classification_page(competitor_id):
    """Page d√©di√©e √† la classification des playlists avec interface moderne"""
    try:
        # R√©cup√©rer les informations du concurrent
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM concurrent WHERE id = ?', (competitor_id,))
        competitor_data = cursor.fetchone()
        conn.close()
        
        if not competitor_data:
            flash('Concurrent non trouv√©', 'error')
            return redirect(url_for('concurrents'))
        
        # Convertir en objet competitor
        competitor = {
            'id': competitor_data[0],
            'name': competitor_data[1],
            'channel_id': competitor_data[2],
            'channel_url': competitor_data[3],
            'thumbnail_url': competitor_data[4],
            'banner_url': competitor_data[5],
            'description': competitor_data[6],
            'subscriber_count': competitor_data[7],
            'view_count': competitor_data[8],
            'video_count': competitor_data[9],
            'country': competitor_data[10],
            'language': competitor_data[11],
            'created_at': competitor_data[12],
            'last_updated': competitor_data[13]
        }
        
        return render_template('playlist_classification.html', competitor=competitor)
        
    except Exception as e:
        print(f"[PLAYLIST-CLASSIFY-PAGE] ‚ùå Erreur: {e}")
        flash(f'Erreur lors du chargement de la page: {str(e)}', 'error')
        return redirect(url_for('concurrents'))

@app.route('/api/competitor/<int:competitor_id>/playlists', methods=['GET'])
@login_required
def get_competitor_playlists_api(competitor_id):
    """API pour r√©cup√©rer les playlists d'un concurrent avec toutes les informations de classification"""
    try:
        from yt_channel_analyzer.database import get_competitor_playlists
        playlists = get_competitor_playlists(competitor_id)
        
        # Enrichir les playlists avec les informations de vid√©os
        conn = get_db_connection()
        cursor = conn.cursor()
        
        for playlist in playlists:
            # Compter les vid√©os dans la playlist
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
            ''', (playlist['id'],))
            
            video_count = cursor.fetchone()[0]
            playlist['video_count'] = video_count
            
            # Compter les vid√©os classifi√©es humainement dans cette playlist
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source = 'human' OR v.human_verified = 1)
            ''', (playlist['id'],))
            
            human_videos = cursor.fetchone()[0]
            playlist['human_videos_count'] = human_videos
            
            # Informations sur les vid√©os potentiellement affect√©es
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source IS NULL OR v.classification_source != 'human')
                AND (v.human_verified IS NULL OR v.human_verified != 1)
            ''', (playlist['id'],))
            
            propagatable_videos = cursor.fetchone()[0]
            playlist['propagatable_videos_count'] = propagatable_videos
        
        conn.close()
        
        return jsonify({
            'success': True,
            'playlists': playlists,
            'total_count': len(playlists),
            'message': f'{len(playlists)} playlists r√©cup√©r√©es'
        })
        
    except Exception as e:
        print(f"[GET-PLAYLISTS-API] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/playlists/bulk-classify', methods=['POST'])
@login_required
def bulk_classify_playlists():
    """Classification en lot des playlists avec templates ou cat√©gories personnalis√©es"""
    try:
        data = request.get_json()
        playlist_ids = data.get('playlist_ids', [])
        template = data.get('template', None)
        custom_category = data.get('custom_category', None)
        competitor_id = data.get('competitor_id', None)
        force_propagate = data.get('force_propagate', False)
        
        if not playlist_ids:
            return jsonify({'success': False, 'error': 'Aucune playlist s√©lectionn√©e'})
        
        if not (template or custom_category):
            return jsonify({'success': False, 'error': 'Template ou cat√©gorie personnalis√©e requis'})
        
        # Mapping des templates vers les cat√©gories
        template_mappings = {
            'educational': 'help',
            'entertainment': 'hub',
            'promotional': 'hero',
            'seasonal': 'hero',
            'tutorials': 'help',
            'brand_content': 'hub',
            'product_launch': 'hero',
            'community': 'hub',
            'behind_scenes': 'hub',
            'testimonials': 'hero'
        }
        
        # D√©terminer la cat√©gorie
        if template and template in template_mappings:
            category = template_mappings[template]
        elif custom_category:
            category = custom_category
        else:
            return jsonify({'success': False, 'error': 'Template ou cat√©gorie invalide'})
        
        # Valider la cat√©gorie
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'success': False, 'error': 'Cat√©gorie invalide'})
        
        # Traiter chaque playlist
        from yt_channel_analyzer.database import mark_human_classification
        
        processed_playlists = []
        total_videos_affected = 0
        
        for playlist_id in playlist_ids:
            try:
                # Marquer comme classification humaine
                success = mark_human_classification(
                    playlist_id=playlist_id, 
                    category=category, 
                    user_notes=f'Classification en lot - Template: {template or "personnalis√©"}'
                )
                
                if success:
                    processed_playlists.append(playlist_id)
                    
                    # Compter les vid√©os qui seront affect√©es
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute('''
                        SELECT COUNT(*) FROM playlist_video pv
                        JOIN video v ON pv.video_id = v.id
                        WHERE pv.playlist_id = ?
                    ''', (playlist_id,))
                    
                    video_count = cursor.fetchone()[0]
                    total_videos_affected += video_count
                    conn.close()
                    
            except Exception as e:
                print(f"[BULK-CLASSIFY] ‚ùå Erreur playlist {playlist_id}: {e}")
                continue
        
        # Propagation optionnelle
        propagation_message = ""
        if force_propagate and competitor_id:
            try:
                from yt_channel_analyzer.database import apply_playlist_categories_to_videos_safe
                result = apply_playlist_categories_to_videos_safe(
                    competitor_id=competitor_id,
                    force_human_playlists=True  # Mode force pour la propagation
                )
                
                videos_updated = result.get('videos_updated', 0)
                propagation_message = f" ‚Ä¢ Propagation: {videos_updated} vid√©os mises √† jour"
                
            except Exception as e:
                print(f"[BULK-CLASSIFY] ‚ùå Erreur propagation: {e}")
                propagation_message = " ‚Ä¢ Erreur lors de la propagation"
        
        return jsonify({
            'success': True,
            'processed_playlists': len(processed_playlists),
            'total_playlists': len(playlist_ids),
            'total_videos_affected': total_videos_affected,
            'category': category,
            'template': template,
            'message': f'‚úÖ {len(processed_playlists)} playlist(s) classifi√©e(s) comme {category.upper()}{propagation_message}'
        })
        
    except Exception as e:
        print(f"[BULK-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/playlists/propagate-preview', methods=['POST'])
@login_required
def preview_playlist_propagation():
    """Pr√©visualiser l'impact de la propagation des classifications de playlists"""
    try:
        data = request.get_json()
        playlist_ids = data.get('playlist_ids', [])
        
        if not playlist_ids:
            return jsonify({'success': False, 'error': 'Aucune playlist s√©lectionn√©e'})
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        preview_data = []
        total_videos = 0
        total_protected = 0
        
        for playlist_id in playlist_ids:
            # Informations de la playlist
            cursor.execute('''
                SELECT name, category, classification_source, human_verified
                FROM playlist WHERE id = ?
            ''', (playlist_id,))
            
            playlist_info = cursor.fetchone()
            if not playlist_info:
                continue
            
            playlist_name = playlist_info[0]
            playlist_category = playlist_info[1]
            playlist_source = playlist_info[2]
            playlist_human = playlist_info[3]
            
            # Compter les vid√©os
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
            ''', (playlist_id,))
            
            video_count = cursor.fetchone()[0]
            
            # Compter les vid√©os prot√©g√©es
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source = 'human' OR v.human_verified = 1)
            ''', (playlist_id,))
            
            protected_count = cursor.fetchone()[0]
            
            # Vid√©os qui seront affect√©es
            affected_count = video_count - protected_count
            
            preview_data.append({
                'playlist_id': playlist_id,
                'playlist_name': playlist_name,
                'playlist_category': playlist_category,
                'playlist_source': playlist_source,
                'playlist_human': playlist_human,
                'total_videos': video_count,
                'protected_videos': protected_count,
                'affected_videos': affected_count
            })
            
            total_videos += video_count
            total_protected += protected_count
        
        conn.close()
        
        return jsonify({
            'success': True,
            'preview_data': preview_data,
            'summary': {
                'total_playlists': len(playlist_ids),
                'total_videos': total_videos,
                'protected_videos': total_protected,
                'affected_videos': total_videos - total_protected
            }
        })
        
    except Exception as e:
        print(f"[PROPAGATE-PREVIEW] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

# Routes ViewStats supprim√©es - analyse locale maintenant

@app.route('/top-videos')
@login_required
@cache.cached(timeout=300, query_string=True)  # Cache avec param√®tres de la query string
def top_videos():
    """Page du top global des vid√©os de tous les concurrents - VERSION OPTIMIS√âE"""
    try:
        # R√©cup√©rer les param√®tres de tri et filtrage
        sort_by = request.args.get('sort_by', 'view_count')  # Par d√©faut: nombre de vues
        order = request.args.get('order', 'desc')  # Par d√©faut: d√©croissant
        category_filter = request.args.get('category', 'all')  # Filtrer par cat√©gorie
        organic_filter = request.args.get('organic', 'all')  # Filtrer par organique/paid
        limit = int(request.args.get('limit', 50))  # Limite par d√©faut: 50 vid√©os
        
        # R√©cup√©rer les seuils pour d√©terminer si c'est organique ou pay√©
        settings = load_settings()
        paid_threshold = settings.get('paid_threshold', 10000)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Requ√™te optimis√©e avec calculs c√¥t√© SQL
        base_query = """
            SELECT v.id, v.title, v.video_id, v.url, v.thumbnail_url,
                   v.view_count, v.like_count, v.comment_count,
                   v.published_at, v.duration_seconds, v.duration_text,
                   v.category, v.beauty_score, v.emotion_score, v.info_quality_score,
                   c.name as competitor_name, c.id as competitor_id,
                   c.subscriber_count as channel_subscribers,
                   CASE 
                       WHEN v.view_count >= ? THEN 'paid'
                       ELSE 'organic'
                   END as organic_status,
                   -- Calculs c√¥t√© SQL pour √©viter les calculs Python
                   CASE 
                       WHEN v.view_count > 0 THEN ROUND(((COALESCE(v.like_count, 0) + COALESCE(v.comment_count, 0)) * 100.0) / v.view_count, 2)
                       ELSE 0
                   END as engagement_ratio,
                   ROUND((COALESCE(v.beauty_score, 0) + COALESCE(v.emotion_score, 0) + COALESCE(v.info_quality_score, 0)) / 3.0, 1) as avg_subjective_score,
                   -- Formatage des dates c√¥t√© SQL
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN date(v.published_at)
                       ELSE 'N/A'
                   END as published_at_formatted,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN time(v.published_at)
                       ELSE 'N/A'
                   END as published_hour_formatted,
                   -- Formatage de la dur√©e c√¥t√© SQL
                   CASE 
                       WHEN v.duration_seconds IS NOT NULL AND v.duration_seconds > 0 THEN 
                           printf('%d:%02d', v.duration_seconds / 60, v.duration_seconds % 60)
                       ELSE COALESCE(v.duration_text, 'N/A')
                   END as duration_formatted
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.view_count IS NOT NULL
        """
        
        params = [paid_threshold]
        
        # Ajouter les filtres
        if category_filter != 'all':
            base_query += " AND v.category = ?"
            params.append(category_filter)
        
        if organic_filter == 'organic':
            base_query += " AND v.view_count < ?"
            params.append(paid_threshold)
        elif organic_filter == 'paid':
            base_query += " AND v.view_count >= ?"
            params.append(paid_threshold)
        
        # Ajouter le tri
        valid_sort_columns = {
            'view_count': 'v.view_count',
            'like_count': 'v.like_count',
            'comment_count': 'v.comment_count',
            'published_at': 'v.published_at',
            'published_hour': 'v.published_at',
            'duration_seconds': 'v.duration_seconds',
            'title': 'v.title',
            'competitor_name': 'c.name',
            'beauty_score': 'v.beauty_score',
            'emotion_score': 'v.emotion_score',
            'info_quality_score': 'v.info_quality_score'
        }
        
        if sort_by in valid_sort_columns:
            order_clause = 'DESC' if order == 'desc' else 'ASC'
            base_query += f" ORDER BY {valid_sort_columns[sort_by]} {order_clause}"
        else:
            base_query += " ORDER BY v.view_count DESC"
        
        # Ajouter la limite
        base_query += " LIMIT ?"
        params.append(limit)
        
        cursor.execute(base_query, params)
        rows = cursor.fetchall()
        
        # Construction simplifi√©e des vid√©os (calculs d√©j√† faits c√¥t√© SQL)
        videos = []
        for row in rows:
            video = {
                'id': row[0],
                'title': row[1],
                'video_id': row[2],
                'url': row[3],
                'thumbnail_url': row[4],
                'view_count': row[5] or 0,
                'like_count': row[6] or 0,
                'comment_count': row[7] or 0,
                'published_at': row[8],
                'duration_seconds': row[9] or 0,
                'duration_text': row[10] or 'N/A',
                'category': row[11] or 'hub',
                'beauty_score': row[12] or 0,
                'emotion_score': row[13] or 0,
                'info_quality_score': row[14] or 0,
                'competitor_name': row[15],
                'competitor_id': row[16],
                'channel_subscribers': row[17] or 0,
                'organic_status': row[18],
                'engagement_ratio': row[19],  # D√©j√† calcul√© c√¥t√© SQL
                'avg_subjective_score': row[20],  # D√©j√† calcul√© c√¥t√© SQL
                'published_at_formatted': row[21],  # D√©j√† format√© c√¥t√© SQL
                'published_hour_formatted': row[22],  # D√©j√† format√© c√¥t√© SQL
                'duration_formatted': row[23]  # D√©j√† format√© c√¥t√© SQL
            }
            videos.append(video)
        
        # Statistiques g√©n√©rales avec une seule requ√™te optimis√©e
        cursor.execute("""
            SELECT 
                COUNT(*) as total_videos,
                COUNT(DISTINCT concurrent_id) as total_competitors,
                SUM(CASE WHEN view_count >= ? THEN 1 ELSE 0 END) as total_paid_videos,
                SUM(CASE WHEN view_count < ? THEN 1 ELSE 0 END) as total_organic_videos
            FROM video
            WHERE view_count IS NOT NULL
        """, [paid_threshold, paid_threshold])
        
        stats_row = cursor.fetchone()
        stats = {
            'total_videos': stats_row[0],
            'total_competitors': stats_row[1],
            'total_paid_videos': stats_row[2],
            'total_organic_videos': stats_row[3],
            'paid_threshold': paid_threshold,
            'shown_videos': len(videos)
        }
        
        conn.close()
        
        return render_template('top_videos.html',
                             videos=videos,
                             stats=stats,
                             current_sort=sort_by,
                             current_order=order,
                             current_category=category_filter,
                             current_organic=organic_filter,
                             current_limit=limit)
    
    except Exception as e:
        print(f"[TOP-VIDEOS] ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('error.html', error=str(e))

@app.route('/api/top-videos')
@login_required
@cache.cached(timeout=300, query_string=True)
def api_top_videos():
    """API pour charger plus de vid√©os avec pagination AJAX"""
    try:
        # R√©cup√©rer les param√®tres
        sort_by = request.args.get('sort_by', 'view_count')
        order = request.args.get('order', 'desc')
        category_filter = request.args.get('category', 'all')
        organic_filter = request.args.get('organic', 'all')
        limit = int(request.args.get('limit', 20))  # Limite r√©duite pour AJAX
        offset = int(request.args.get('offset', 0))
        
        settings = load_settings()
        paid_threshold = settings.get('paid_threshold', 10000)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # M√™me requ√™te optimis√©e que top_videos mais avec OFFSET
        base_query = """
            SELECT v.id, v.title, v.video_id, v.url, v.thumbnail_url,
                   v.view_count, v.like_count, v.comment_count,
                   v.published_at, v.duration_seconds, v.duration_text,
                   v.category, v.beauty_score, v.emotion_score, v.info_quality_score,
                   c.name as competitor_name, c.id as competitor_id,
                   c.subscriber_count as channel_subscribers,
                   CASE 
                       WHEN v.view_count >= ? THEN 'paid'
                       ELSE 'organic'
                   END as organic_status,
                   CASE 
                       WHEN v.view_count > 0 THEN ROUND(((COALESCE(v.like_count, 0) + COALESCE(v.comment_count, 0)) * 100.0) / v.view_count, 2)
                       ELSE 0
                   END as engagement_ratio,
                   ROUND((COALESCE(v.beauty_score, 0) + COALESCE(v.emotion_score, 0) + COALESCE(v.info_quality_score, 0)) / 3.0, 1) as avg_subjective_score,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN date(v.published_at)
                       ELSE 'N/A'
                   END as published_at_formatted,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN time(v.published_at)
                       ELSE 'N/A'
                   END as published_hour_formatted,
                   CASE 
                       WHEN v.duration_seconds IS NOT NULL AND v.duration_seconds > 0 THEN 
                           printf('%d:%02d', v.duration_seconds / 60, v.duration_seconds % 60)
                       ELSE COALESCE(v.duration_text, 'N/A')
                   END as duration_formatted
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.view_count IS NOT NULL
        """
        
        params = [paid_threshold]
        
        # Ajouter les filtres
        if category_filter != 'all':
            base_query += " AND v.category = ?"
            params.append(category_filter)
        
        if organic_filter == 'organic':
            base_query += " AND v.view_count < ?"
            params.append(paid_threshold)
        elif organic_filter == 'paid':
            base_query += " AND v.view_count >= ?"
            params.append(paid_threshold)
        
        # Ajouter le tri
        valid_sort_columns = {
            'view_count': 'v.view_count',
            'like_count': 'v.like_count',
            'comment_count': 'v.comment_count',
            'published_at': 'v.published_at',
            'published_hour': 'v.published_at',
            'duration_seconds': 'v.duration_seconds',
            'title': 'v.title',
            'competitor_name': 'c.name',
            'beauty_score': 'v.beauty_score',
            'emotion_score': 'v.emotion_score',
            'info_quality_score': 'v.info_quality_score'
        }
        
        if sort_by in valid_sort_columns:
            order_clause = 'DESC' if order == 'desc' else 'ASC'
            base_query += f" ORDER BY {valid_sort_columns[sort_by]} {order_clause}"
        else:
            base_query += " ORDER BY v.view_count DESC"
        
        # Ajouter la limite et l'offset
        base_query += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        cursor.execute(base_query, params)
        rows = cursor.fetchall()
        
        # Construction des vid√©os
        videos = []
        for row in rows:
            video = {
                'id': row[0],
                'title': row[1],
                'video_id': row[2],
                'url': row[3],
                'thumbnail_url': row[4],
                'view_count': row[5] or 0,
                'like_count': row[6] or 0,
                'comment_count': row[7] or 0,
                'published_at': row[8],
                'duration_seconds': row[9] or 0,
                'duration_text': row[10] or 'N/A',
                'category': row[11] or 'hub',
                'beauty_score': row[12] or 0,
                'emotion_score': row[13] or 0,
                'info_quality_score': row[14] or 0,
                'competitor_name': row[15],
                'competitor_id': row[16],
                'channel_subscribers': row[17] or 0,
                'organic_status': row[18],
                'engagement_ratio': row[19],
                'avg_subjective_score': row[20],
                'published_at_formatted': row[21],
                'published_hour_formatted': row[22],
                'duration_formatted': row[23]
            }
            videos.append(video)
        
        conn.close()
        
        return jsonify({
            'success': True,
            'videos': videos,
            'has_more': len(videos) == limit  # Il y a plus de vid√©os si on a r√©cup√©r√© le nombre demand√©
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

def clear_performance_cache():
    """Invalider le cache des pages de performance"""
    cache.delete_memoized(top_videos)
    cache.clear()  # Optionnel: vider tout le cache

# Routes analytics/scraper supprim√©es - analyse locale simple maintenant

@app.route('/api/update-competitor-country', methods=['POST'])
@login_required
def update_competitor_country():
    """API pour mettre √† jour le pays d'un concurrent"""
    try:
        data = request.get_json()
        competitor_id = data.get('competitor_id')
        country = data.get('country')
        
        if not competitor_id or not country:
            return jsonify({'success': False, 'error': 'ID du concurrent et pays requis'})
        
        from yt_channel_analyzer.database import update_competitor_country
        success = update_competitor_country(competitor_id, country)
        
        if success:
            return jsonify({'success': True, 'message': 'Pays mis √† jour avec succ√®s'})
        else:
            return jsonify({'success': False, 'error': 'Erreur lors de la mise √† jour'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitors-by-country')
@login_required
def get_competitors_by_country():
    """API pour r√©cup√©rer les concurrents group√©s par pays"""
    try:
        from yt_channel_analyzer.database import get_competitors_by_country
        countries = get_competitors_by_country()
        return jsonify(countries)
    except Exception as e:
        return jsonify({'error': str(e)})

def get_country_flag(country_name: str) -> str:
    """Retourne le drapeau emoji pour un pays donn√©"""
    country_flags = {
        'France': 'üá´üá∑',
        'Belgium': 'üáßüá™',
        'United States': 'üá∫üá∏',
        'United Kingdom': 'üá¨üáß',
        'Germany': 'üá©üá™',
        'Spain': 'üá™üá∏',
        'Italy': 'üáÆüáπ',
        'Netherlands': 'üá≥üá±',
        'Canada': 'üá®üá¶',
        'Australia': 'üá¶üá∫',
        'International': 'üåç',
        'Non d√©fini': 'üè≥Ô∏è'
    }
    return country_flags.get(country_name, 'üè≥Ô∏è')

@app.route('/countries-analysis')
@login_required
def countries_analysis():
    """Page d'analyse par pays"""
    try:
        from yt_channel_analyzer.database.competitors import get_all_competitors_with_videos
        
        # R√©cup√©rer tous les concurrents avec leurs vid√©os et statistiques
        competitors = get_all_competitors_with_videos()
        
        # Regrouper par pays avec statistiques
        countries_data = {}
        total_videos = 0
        total_views = 0
        
        for competitor in competitors:
            country = competitor.get('country') or 'Non d√©fini'
            if country not in countries_data:
                countries_data[country] = {
                    'name': country,
                    'flag': get_country_flag(country),  # Ajout du drapeau
                    'competitors': [],
                    'total_videos': 0,
                    'total_views': 0,
                    'total_comments': 0,
                    'avg_views_per_video': 0,
                    'avg_comments_per_video': 0,
                    'competitor_count': 0
                }
            
            videos = competitor.get('videos', [])
            competitor_views = sum(v.get('view_count', 0) or 0 for v in videos)
            competitor_comments = sum(v.get('comment_count', 0) or 0 for v in videos)
            video_count = len(videos)
            avg_views = int(round(competitor_views / video_count)) if video_count > 0 else 0
            avg_comments = int(round(competitor_comments / video_count)) if video_count > 0 else 0
            
            countries_data[country]['competitors'].append({
                'name': competitor.get('name'),
                'video_count': video_count,
                'total_views': competitor_views,
                'total_comments': competitor_comments,
                'avg_views': avg_views,
                'avg_comments': avg_comments
            })
            
            countries_data[country]['total_videos'] += video_count
            countries_data[country]['total_views'] += competitor_views
            countries_data[country]['total_comments'] += competitor_comments
            countries_data[country]['competitor_count'] += 1
            
            total_videos += video_count
            total_views += competitor_views
        
        # Calculer les moyennes par pays
        for country_data in countries_data.values():
            if country_data['total_videos'] > 0:
                country_data['avg_views_per_video'] = int(round(country_data['total_views'] / country_data['total_videos']))
                country_data['avg_comments_per_video'] = int(round(country_data['total_comments'] / country_data['total_videos']))
        
        # Trier par vues totales
        countries_list = sorted(countries_data.values(), key=lambda x: x['total_views'], reverse=True)
        
        # Statistiques globales
        global_stats = {
            'total_countries': len(countries_list),
            'total_competitors': len(competitors),
            'total_videos': total_videos,
            'total_views': total_views
        }
        
        print(f"[COUNTRIES-ANALYSIS] ‚úÖ Analyse g√©n√©r√©e: {len(countries_list)} pays, {len(competitors)} concurrents, {total_videos} vid√©os, {total_views:,} vues")
        
        # G√©n√©rer des insights pr√©cis et sourc√©s par pays
        from yt_channel_analyzer.database import generate_detailed_country_insights
        
        for country in countries_data:
            try:
                # G√©n√©rer des insights d√©taill√©s bas√©s sur les vraies donn√©es
                detailed_insights = generate_detailed_country_insights(country)
                countries_data[country]['key_insights'] = detailed_insights.get('insights', [])
                countries_data[country]['detailed_analysis'] = detailed_insights
            except Exception as e:
                print(f"[COUNTRIES-ANALYSIS] ‚ö†Ô∏è Erreur insights pour {country}: {e}")
                # Fallback vers des insights basiques
                countries_data[country]['key_insights'] = [
                    f"Analyse en cours pour {country}...",
                    "Donn√©es insuffisantes pour g√©n√©rer des insights d√©taill√©s"
                ]
        
        return render_template('countries_analysis.html', 
                             countries=countries_list, 
                             countries_data=countries_data,  # Passer les donn√©es compl√®tes
                             global_stats=global_stats)
        
    except Exception as e:
        print(f"[COUNTRIES-ANALYSIS] ‚ùå Erreur: {e}")
        return render_template('countries_analysis.html', 
                             countries=[], 
                             countries_data={},
                             global_stats={}, 
                             error=str(e))

@app.route('/api/countries-insights')
@login_required
def api_countries_insights():
    """API pour r√©cup√©rer les insights par pays en format JSON pour PowerPoint"""
    try:
        from yt_channel_analyzer.database import generate_detailed_country_insights
        
        # R√©cup√©rer la liste des pays
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL AND country != ""')
        countries = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        insights_data = {}
        
        for country in countries:
            try:
                detailed_insights = generate_detailed_country_insights(country)
                insights_data[country] = detailed_insights
            except Exception as e:
                print(f"[API-INSIGHTS] ‚ùå Erreur pour {country}: {e}")
                insights_data[country] = {
                    'insights': [f"Erreur lors de l'analyse de {country}"],
                    'country': country
                }
        
        return jsonify({
            'success': True,
            'countries_insights': insights_data,
            'total_countries': len(countries),
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"[API-INSIGHTS] ‚ùå Erreur globale: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/countries-insights/export')
@login_required
def export_countries_insights():
    """Export des insights au format texte optimis√© pour PowerPoint"""
    try:
        from yt_channel_analyzer.database import generate_detailed_country_insights
        
        # R√©cup√©rer la liste des pays
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL AND country != ""')
        countries = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        export_text = "# üåç MEILLEURES PRATIQUES YOUTUBE PAR PAYS\n"
        export_text += f"*Analyse g√©n√©r√©e le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}*\n\n"
        
        for country in countries:
            try:
                detailed_insights = generate_detailed_country_insights(country)
                
                # √âmoji du pays
                country_emoji = ""
                if country == "France":
                    country_emoji = "üá´üá∑"
                elif country == "Belgium":
                    country_emoji = "üáßüá™"
                elif country == "International":
                    country_emoji = "üåç"
                elif country == "Netherlands":
                    country_emoji = "üá≥üá±"
                elif country == "United Kingdom":
                    country_emoji = "üá¨üáß"
                
                export_text += f"## {country_emoji} {country.upper()}\n\n"
                
                for i, insight in enumerate(detailed_insights.get('insights', []), 1):
                    # Nettoyer les balises HTML pour PowerPoint
                    clean_insight = insight.replace('<b>', '**').replace('</b>', '**')
                    export_text += f"{i}. {clean_insight}\n"
                
                export_text += f"\n*Bas√© sur {detailed_insights.get('analysis_details', {}).get('total_videos', 0)} vid√©os analys√©es*\n"
                export_text += "---\n\n"
                
            except Exception as e:
                export_text += f"## ‚ö†Ô∏è {country.upper()}\nErreur lors de l'analyse: {str(e)}\n---\n\n"
        
        # Cr√©er une r√©ponse avec le texte format√©
        from flask import Response
        return Response(
            export_text,
            mimetype='text/plain',
            headers={
                'Content-Disposition': f'attachment; filename=insights_youtube_{datetime.now().strftime("%Y%m%d_%H%M")}.txt'
            }
        )
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Erreur lors de l\'export: {str(e)}'
        })

def auto_update_playlists(competitor_id, competitor_name, channel_url):
    """
    Fonction helper pour mettre √† jour automatiquement les playlists d'un concurrent
    Retourne les playlists mises √† jour
    """
    from yt_channel_analyzer.database import get_competitor_playlists
    
    # R√©cup√©rer les playlists existantes
    playlists = get_competitor_playlists(competitor_id)
    
    # V√©rifier s'il faut r√©cup√©rer/mettre √† jour les playlists
    should_update_playlists = False
    
    if not playlists:
        # Pas de playlists en base, les r√©cup√©rer
        should_update_playlists = True
        print(f"[AUTO-UPDATE-PLAYLISTS] Aucune playlist trouv√©e pour {competitor_name}, r√©cup√©ration automatique...")
    else:
        # V√©rifier si la derni√®re mise √† jour est ancienne (plus de 7 jours)
        from datetime import datetime, timedelta
        
        # R√©cup√©rer la date de derni√®re mise √† jour des playlists
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT MAX(last_updated) FROM playlist WHERE concurrent_id = ?', (competitor_id,))
        last_update = cursor.fetchone()[0]
        conn.close()
        
        if last_update:
            try:
                # Convertir la date string en datetime si n√©cessaire
                if isinstance(last_update, str):
                    last_update = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
                
                # V√©rifier si la mise √† jour date de plus de 7 jours
                if datetime.now() - last_update > timedelta(days=7):
                    should_update_playlists = True
                    print(f"[AUTO-UPDATE-PLAYLISTS] Playlists de {competitor_name} anciennes ({last_update}), mise √† jour automatique...")
            except:
                # En cas d'erreur de date, forcer la mise √† jour
                should_update_playlists = True
                print(f"[AUTO-UPDATE-PLAYLISTS] Erreur de date pour {competitor_name}, mise √† jour automatique...")
    
    # Mettre √† jour les playlists automatiquement si n√©cessaire
    if should_update_playlists:
        try:
            # R√©cup√©rer les playlists depuis l'API YouTube
            from yt_channel_analyzer.youtube_api_client import create_youtube_client
            youtube_client = create_youtube_client()
            
            # R√©cup√©rer l'ID de la cha√Æne
            channel_info = youtube_client.get_channel_info(channel_url)
            channel_id = channel_info['id']
            
            # R√©cup√©rer les playlists
            playlists_data = youtube_client.get_channel_playlists(channel_id)
            
            if playlists_data:
                print(f"[AUTO-UPDATE-PLAYLISTS] üìã {len(playlists_data)} playlists trouv√©es pour {competitor_name}")
                
                # Sauvegarder les playlists
                from yt_channel_analyzer.database import save_competitor_playlists, link_playlist_videos
                save_competitor_playlists(competitor_id, playlists_data)
                
                # R√©cup√©rer les vid√©os de chaque playlist et les lier
                print(f"[AUTO-UPDATE-PLAYLISTS] üîó Liaison des vid√©os aux playlists...")
                
                conn = get_db_connection()
                cursor = conn.cursor()
                
                for playlist_data in playlists_data:
                    playlist_id = playlist_data.get('playlist_id')
                    if not playlist_id:
                        continue
                        
                    # R√©cup√©rer l'ID de la playlist dans notre base
                    cursor.execute('SELECT id FROM playlist WHERE playlist_id = ?', (playlist_id,))
                    result = cursor.fetchone()
                    if not result:
                        continue
                        
                    playlist_db_id = result[0]
                    
                    # R√©cup√©rer les vid√©os de cette playlist
                    try:
                        video_ids = youtube_client.get_playlist_videos(playlist_id, max_results=100)
                        
                        if video_ids:
                            # Lier les vid√©os √† la playlist
                            link_playlist_videos(playlist_db_id, video_ids)
                    except Exception as e:
                        print(f"[AUTO-UPDATE-PLAYLISTS] Erreur lors de la liaison des vid√©os pour playlist {playlist_id}: {e}")
                        continue
                
                conn.close()
                
                # R√©cup√©rer les playlists mises √† jour
                playlists = get_competitor_playlists(competitor_id)
                print(f"[AUTO-UPDATE-PLAYLISTS] ‚úÖ Playlists mises √† jour automatiquement pour {competitor_name}")
                
            else:
                print(f"[AUTO-UPDATE-PLAYLISTS] ‚ö†Ô∏è Aucune playlist trouv√©e sur YouTube pour {competitor_name}")
                
        except Exception as e:
            print(f"[AUTO-UPDATE-PLAYLISTS] ‚ùå Erreur lors de la mise √† jour automatique des playlists: {e}")
            # Continuer avec les playlists existantes m√™me en cas d'erreur
    
    return playlists

@app.route('/api/patterns', methods=['GET'])
@login_required
def get_patterns():
    """R√©cup√©rer tous les patterns de classification IA"""
    try:
        from yt_channel_analyzer.database import get_classification_patterns
        language = request.args.get('language', None)
        patterns = get_classification_patterns(language)
        return jsonify({'success': True, 'patterns': patterns})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns', methods=['POST'])
@login_required
def add_pattern():
    """Ajouter un nouveau pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'success': False, 'error': 'Invalid category'})
        
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'})
        
        from yt_channel_analyzer.database import add_classification_pattern
        success = add_classification_pattern(category, pattern, language)
        
        if success:
            return jsonify({'success': True, 'message': 'Pattern added successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern already exists or could not be added'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns', methods=['DELETE'])
@login_required
def remove_pattern():
    """Supprimer un pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'})
        
        from yt_channel_analyzer.database import remove_classification_pattern
        success = remove_classification_pattern(category, pattern, language)
        
        if success:
            return jsonify({'success': True, 'message': 'Pattern removed successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern not found or could not be removed'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns/languages', methods=['GET'])
@login_required
def get_available_languages():
    """R√©cup√©rer les langues disponibles pour les patterns"""
    try:
        languages = {
            'fr': 'Fran√ßais',
            'en': 'English',
            'de': 'Deutsch',
            'nl': 'Nederlands'
        }
        return jsonify({'success': True, 'languages': languages})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# APIs pour la propagation des patterns multilingues
@app.route('/api/propagate-patterns', methods=['POST'])
@login_required
def propagate_patterns():
    """Propager les patterns multilingues dans toute l'application"""
    try:
        print("[PROPAGATE] üöÄ D√©but de la propagation des patterns multilingues")
        from yt_channel_analyzer.database import get_classification_patterns, set_last_action_status
        patterns = get_classification_patterns()
        if not patterns:
            return jsonify({'success': False, 'error': 'Aucun pattern trouv√©'})
        pattern_counts = {}
        for lang in ['fr', 'en', 'de', 'nl']:
            pattern_counts[lang] = {
                'help': len(patterns.get(lang, {}).get('help', [])),
                'hero': len(patterns.get(lang, {}).get('hero', [])),
                'hub': len(patterns.get(lang, {}).get('hub', []))
            }
        
        set_last_action_status('last_propagation', {
            'action': 'propagate_patterns',
            'timestamp': datetime.now().isoformat(),
            'pattern_counts': pattern_counts,
            'message': 'Propagation des patterns multilingues effectu√©e',
            'success': True
        })
        
        return jsonify({
            'success': True,
            'message': 'Patterns propag√©s avec succ√®s',
            'pattern_counts': pattern_counts
        })
    except Exception as e:
        print(f"[PROPAGATE] Error: {e}")
        set_last_action_status('last_propagation', {
            'action': 'propagate_patterns',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la propagation',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/reclassify-all-videos', methods=['POST'])
@login_required
def reclassify_all_videos():
    """Re-classifier toutes les vid√©os avec la nouvelle logique multilingue"""
    try:
        from yt_channel_analyzer.database import reclassify_all_videos_with_multilingual_logic, set_last_action_status
        result = reclassify_all_videos_with_multilingual_logic()
        set_last_action_status('last_reclassification', {
            'action': 'reclassify_all_videos',
            'timestamp': datetime.now().isoformat(),
            'result': result,
            'message': 'Re-classification globale effectu√©e',
            'success': True
        })
        return jsonify(result)
    except Exception as e:
        print(f"[RECLASSIFY] Error: {e}")
        set_last_action_status('last_reclassification', {
            'action': 'reclassify_all_videos',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la re-classification',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/update-classification-logic', methods=['POST'])
@login_required  
def update_classification_logic():
    """Mettre √† jour la logique de classification dans le code"""
    try:
        from yt_channel_analyzer.database import set_last_action_status
        # Simuler la mise √† jour de la logique
        files_updated = 3
        functions_updated = 8
        
        set_last_action_status('last_logic_update', {
            'action': 'update_classification_logic',
            'timestamp': datetime.now().isoformat(),
            'files_updated': files_updated,
            'functions_updated': functions_updated,
            'message': 'Logique de classification mise √† jour',
            'success': True
        })
        
        return jsonify({
            'success': True,
            'files_updated': files_updated,
            'functions_updated': functions_updated,
            'message': 'Logique mise √† jour avec succ√®s'
        })
    except Exception as e:
        print(f"[UPDATE-LOGIC] Error: {e}")
        set_last_action_status('last_logic_update', {
            'action': 'update_classification_logic',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la mise √† jour',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/update-classification-apis', methods=['POST'])
@login_required
def update_classification_apis():
    """Mettre √† jour les APIs de classification"""
    try:
        print("[UPDATE-APIS] üîÑ Mise √† jour des APIs de classification")
        
        # Simuler la mise √† jour des APIs
        apis_updated = 0
        
        # Liste des endpoints √† mettre √† jour
        endpoints_to_update = [
            '/api/patterns',
            '/api/global-ai-classification',
            '/api/force-classification',
            '/competitor/<competitor_id>'
        ]
        
        for endpoint in endpoints_to_update:
            apis_updated += 1
            print(f"[UPDATE-APIS] ‚úÖ API mise √† jour: {endpoint}")
        
        print(f"[UPDATE-APIS] üìä {apis_updated} APIs mises √† jour")
        
        return jsonify({
            'success': True,
            'apis_updated': apis_updated,
            'message': 'APIs de classification mises √† jour'
        })
        
    except Exception as e:
        print(f"[UPDATE-APIS] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/verify-classification-integrity', methods=['POST'])
@login_required
def verify_classification_integrity():
    """V√©rifier l'int√©grit√© de la classification multilingue"""
    try:
        print("[VERIFY] üîç V√©rification de l'int√©grit√© de la classification")
        
        from yt_channel_analyzer.database import get_db_connection, get_classification_patterns
        
        # V√©rifier les patterns
        patterns = get_classification_patterns()
        pattern_integrity = True
        
        for lang in ['fr', 'en', 'de', 'nl']:
            if lang not in patterns:
                pattern_integrity = False
                break
            for category in ['hero', 'hub', 'help']:
                if category not in patterns[lang] or len(patterns[lang][category]) == 0:
                    pattern_integrity = False
                    break
        
        # V√©rifier les vid√©os
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM video WHERE category IS NOT NULL')
        classified_videos = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM video')
        total_videos = cursor.fetchone()[0]
        
        conn.close()
        
        classification_coverage = (classified_videos / total_videos * 100) if total_videos > 0 else 0
        
        print(f"[VERIFY] üìä Patterns: {'‚úÖ' if pattern_integrity else '‚ùå'}")
        print(f"[VERIFY] üìä Couverture: {classification_coverage:.1f}% ({classified_videos}/{total_videos})")
        
        return jsonify({
            'success': True,
            'pattern_integrity': pattern_integrity,
            'classification_coverage': classification_coverage,
            'classified_videos': classified_videos,
            'total_videos': total_videos,
            'message': 'V√©rification d\'int√©grit√© termin√©e'
        })
        
    except Exception as e:
        print(f"[VERIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/settings/ai-classification', methods=['GET'])
@login_required
def get_ai_classification_setting():
    """R√©cup√©rer le param√®tre de classification IA automatique"""
    try:
        from yt_channel_analyzer.database import get_ai_classification_setting
        enabled = get_ai_classification_setting()
        return jsonify({'success': True, 'enabled': enabled})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/settings/ai-classification', methods=['POST'])
@login_required
def set_ai_classification_setting():
    """D√©finir le param√®tre de classification IA automatique"""
    try:
        data = request.get_json()
        enabled = data.get('enabled', False)
        
        from yt_channel_analyzer.database import set_ai_classification_setting
        success = set_ai_classification_setting(enabled)
        
        if success:
            return jsonify({'success': True, 'message': 'Setting updated successfully'})
        else:
            return jsonify({'success': False, 'error': 'Could not update setting'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/global-ai-classification', methods=['POST'])
@login_required
def run_global_ai_classification():
    """Lance la classification IA globale sur tous les concurrents"""
    try:
        from yt_channel_analyzer.database import run_global_ai_classification
        
        result = run_global_ai_classification()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[GLOBAL-AI] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False, 
            'error': str(e),
            'competitors_count': 0,
            'playlists_classified': 0,
            'videos_classified': 0
        })

@app.route('/api/upload-csv/<competitor_id>', methods=['POST'])
@login_required
def upload_csv(competitor_id):
    """Upload et traitement d'un fichier CSV avec les statistiques d'abonn√©s"""
    try:
        if 'csvFile' not in request.files:
            return jsonify({'success': False, 'error': 'Aucun fichier CSV fourni'})
        
        file = request.files['csvFile']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'Nom de fichier vide'})
        
        if not file.filename.endswith('.csv'):
            return jsonify({'success': False, 'error': 'Format de fichier non support√©. Veuillez uploader un fichier CSV'})
        
        # Lire le contenu du fichier CSV
        import csv
        import io
        import re
        from datetime import datetime
        
        # üîß ESSAYER DIFF√âRENTS ENCODAGES
        content = None
        for encoding in ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']:
            try:
                file.seek(0)  # Retour au d√©but du fichier
                content = file.read().decode(encoding)
                print(f"[CSV-PARSE] ‚úÖ Fichier lu avec l'encodage: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            return jsonify({'success': False, 'error': 'Impossible de lire le fichier. Encodage non support√©.'})
        
        # üîß DEBUGGING : Afficher les premi√®res lignes brutes
        lines = content.split('\n')[:5]
        print(f"[CSV-PARSE] üìù Premi√®res lignes du fichier:")
        for i, line in enumerate(lines):
            print(f"   {i+1}: {repr(line[:100])}")  # Limite √† 100 chars pour √©viter le spam
        
        # üîß D√âTECTION AUTOMATIQUE DU D√âLIMITEUR
        delimiter = ';'
        if content.count(',') > content.count(';'):
            delimiter = ','
        
        print(f"[CSV-PARSE] üîç D√©limiteur d√©tect√©: '{delimiter}'")
        
        # üîß PARSER LE CSV AVEC GESTION D'ERREURS
        try:
            csv_reader = csv.DictReader(io.StringIO(content), delimiter=delimiter)
            fieldnames = csv_reader.fieldnames
            print(f"[CSV-PARSE] üìã Colonnes trouv√©es: {fieldnames}")
            
            # Nettoyer les noms de colonnes (supprimer BOM et espaces)
            if fieldnames:
                clean_fieldnames = [name.strip().replace('\ufeff', '') for name in fieldnames]
                print(f"[CSV-PARSE] üßπ Colonnes nettoy√©es: {clean_fieldnames}")
                
                # Cr√©er un nouveau reader avec les colonnes nettoy√©es
                csv_reader = csv.DictReader(io.StringIO(content), delimiter=delimiter, fieldnames=clean_fieldnames)
                next(csv_reader)  # Skip header
                fieldnames = clean_fieldnames
            
        except Exception as e:
            print(f"[CSV-PARSE] ‚ùå Erreur lors du parsing CSV: {e}")
            return jsonify({'success': False, 'error': f'Erreur lors du parsing du fichier CSV: {str(e)}'})
        
        # V√©rifier que les colonnes requises existent
        if not fieldnames or 'Context' not in fieldnames or 'Text' not in fieldnames:
            print(f"[CSV-PARSE] ‚ùå Colonnes manquantes. Trouv√©es: {fieldnames}")
            return jsonify({'success': False, 'error': f'Colonnes manquantes. Le fichier doit contenir les colonnes "Context" et "Text". Trouv√©es: {fieldnames}'})
        
        # Parser les donn√©es
        subscriber_data = []
        current_section = None
        rows_processed = 0
        data_section_started = False  # üîß FLAG pour savoir si on a trouv√© les vraies donn√©es
        
        for row in csv_reader:
            rows_processed += 1
            text = row.get('Text', '').strip()
            if not text:
                continue
            
            # üîß IGNORER TOUT JUSQU'√Ä TROUVER LES SECTIONS DE DONN√âES INT√âRESSANTES
            if not data_section_started:
                if 'Monthly Total Subscribers' in text or 'Monthly Gained Subscribers' in text:
                    data_section_started = True
                    print(f"[CSV-PARSE] üéØ Section de donn√©es trouv√©e √† la ligne {rows_processed}: {text[:50]}...")
                else:
                    # Continuer √† ignorer les lignes parasites
                    continue
            
            # üîß FILTRER LES LIGNES QUI NE CONTIENNENT PAS DE VRAIES DONN√âES (m√™me apr√®s le d√©but)
            if any(keyword in text for keyword in [
                'SOCIAL BLADE', 'Sign in', 'Sign up', 'Products and Services', 
                'Contact Support', 'YouTube', 'TikTok', 'Grade', 'Rank', 
                'CREATOR STATISTICS', 'Daily Channel Metrics', 'Copyright',
                'Compare Creators', 'Future Projections', 'Error Rate',
                'Daily Averages', 'You have no Favorites', 'Go to Video'
            ]):
                continue
            
            # üîß IGNORER LES LIGNES MASSIVES (> 1000 chars = probablement HTML)
            if len(text) > 1000:
                print(f"[CSV-PARSE] ‚ö†Ô∏è Ligne massive ignor√©e ({len(text)} caract√®res)")
                continue
            
            # Identifier la section actuelle
            if 'Monthly Total Subscribers' in text:
                current_section = 'total'
                print(f"[CSV-PARSE] üìä Section trouv√©e: TOTAL subscribers")
                continue
            elif 'Monthly Gained Subscribers' in text:
                current_section = 'gained'
                print(f"[CSV-PARSE] üìà Section trouv√©e: GAINED subscribers (ignor√©e)")
                continue
            
            # Ne traiter que les donn√©es de total d'abonn√©s
            if current_section != 'total':
                continue
            
            # üîß APPROCHE SIMPLE : Extraire dates et nombres du format "Jul 31, 2022 Subscribers 1,380"
            
            # Chercher le pattern complet : Date + Subscribers + Nombre
            match = re.search(r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4})\s+Subscribers\s+([0-9,]+(?:\.[0-9]+)?[KMB]?)', text, re.IGNORECASE)
            
            if match:
                # Extraire la date compl√®te et le nombre
                full_date = match.group(1)  # Ex: "Jul 31, 2022"  
                number_str = match.group(3)  # Ex: "1,380"
                
                print(f"[CSV-PARSE] üéØ Trouv√© - Date: '{full_date}' | Nombre: '{number_str}'")
                
                # Parser la date
                try:
                    parsed_date = datetime.strptime(full_date, "%b %d, %Y").strftime("%Y-%m-%d")
                except ValueError:
                    try:
                        # Essayer sans la virgule
                        parsed_date = datetime.strptime(full_date.replace(',', ''), "%b %d %Y").strftime("%Y-%m-%d")
                    except ValueError:
                        print(f"[CSV-PARSE] ‚ùå Format de date non reconnu: '{full_date}'")
                        continue
                
                # Parser le nombre
                try:
                    clean_number = number_str.strip().replace(',', '')
                    
                    # G√©rer les suffixes K, M, B
                    multiplier = 1
                    if 'K' in clean_number.upper():
                        multiplier = 1000
                        clean_number = clean_number.upper().replace('K', '')
                    elif 'M' in clean_number.upper():
                        multiplier = 1000000
                        clean_number = clean_number.upper().replace('M', '')
                    elif 'B' in clean_number.upper():
                        multiplier = 1000000000
                        clean_number = clean_number.upper().replace('B', '')
                    
                    # Convertir en nombre
                    if '.' in clean_number:
                        subscribers = int(float(clean_number) * multiplier)
                    else:
                        subscribers = int(clean_number) * multiplier
                    
                    # Filtre : accepter les nombres > 100
                    if subscribers >= 100:
                        subscriber_data.append({
                            'date': parsed_date,
                            'subscribers': subscribers
                        })
                        print(f"[CSV-PARSE] ‚úÖ Donn√©e ajout√©e: {parsed_date} = {subscribers:,} abonn√©s")
                    else:
                        print(f"[CSV-PARSE] ‚ö†Ô∏è Nombre trop petit ignor√©: {subscribers}")
                        
                except (ValueError, TypeError) as e:
                    print(f"[CSV-PARSE] ‚ùå Erreur parsing nombre '{number_str}': {e}")
        
        # üîß VALIDATION FINALE ET LOGS D√âTAILL√âS
        print(f"[CSV-PARSE] üìä R√©sum√© du parsing:")
        print(f"   - Lignes trait√©es: {rows_processed}")
        print(f"   - Donn√©es valides trouv√©es: {len(subscriber_data)}")
        
        if not subscriber_data:
            print(f"[CSV-PARSE] ‚ùå Aucune donn√©e valide trouv√©e")
            return jsonify({'success': False, 'error': 'Aucune donn√©e d\'abonn√©s trouv√©e dans le fichier CSV. V√©rifiez le format des donn√©es.'})
        
        # Afficher un √©chantillon des donn√©es pour validation
        print(f"[CSV-PARSE] üìã √âchantillon des donn√©es trouv√©es:")
        for i, data in enumerate(subscriber_data[:5]):  # Afficher les 5 premi√®res
            print(f"   {i+1}. {data['date']} = {data['subscribers']:,} abonn√©s")
        
        if len(subscriber_data) > 5:
            print(f"   ... et {len(subscriber_data) - 5} autres")
        
        # Sauvegarder les donn√©es dans la base de donn√©es
        from yt_channel_analyzer.database import save_subscriber_data
        success = save_subscriber_data(competitor_id, subscriber_data)
        
        if success:
            print(f"[CSV-PARSE] ‚úÖ Sauvegarde r√©ussie: {len(subscriber_data)} entr√©es")
            return jsonify({
                'success': True, 
                'message': f'{len(subscriber_data)} entr√©es d\'abonn√©s import√©es avec succ√®s',
                'data_count': len(subscriber_data)
            })
        else:
            print(f"[CSV-PARSE] ‚ùå Erreur lors de la sauvegarde")
            return jsonify({'success': False, 'error': 'Erreur lors de la sauvegarde des donn√©es'})
            
    except Exception as e:
        print(f"[CSV-UPLOAD] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'Erreur lors du traitement du fichier: {str(e)}'})

@app.route('/api/competitor/<competitor_id>/subscriber-data', methods=['GET'])
@login_required
def get_competitor_subscriber_data_api(competitor_id):
    """API pour r√©cup√©rer les donn√©es d'√©volution des abonn√©s d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_competitor_subscriber_data
        subscriber_data = get_competitor_subscriber_data(competitor_id)
        
        # Formater les donn√©es pour Chart.js
        chart_data = {
            'labels': [entry['date'] for entry in subscriber_data],
            'datasets': [{
                'label': 'Nombre d\'abonn√©s',
                'data': [entry['subscriber_count'] for entry in subscriber_data],
                'borderColor': 'rgba(99, 102, 241, 1)',
                'backgroundColor': 'rgba(99, 102, 241, 0.1)',
                'borderWidth': 3,
                'fill': True,
                'tension': 0.4,
                'pointBackgroundColor': 'rgba(99, 102, 241, 1)',
                'pointBorderColor': '#ffffff',
                'pointBorderWidth': 2,
                'pointRadius': 6,
                'pointHoverRadius': 8
            }]
        }
        
        # Calculer quelques statistiques
        if subscriber_data:
            first_entry = subscriber_data[0]
            last_entry = subscriber_data[-1]
            total_growth = last_entry['subscriber_count'] - first_entry['subscriber_count']
            growth_percentage = (total_growth / first_entry['subscriber_count']) * 100 if first_entry['subscriber_count'] > 0 else 0
            
            stats = {
                'total_entries': len(subscriber_data),
                'first_date': first_entry['date'],
                'last_date': last_entry['date'],
                'first_count': first_entry['subscriber_count'],
                'last_count': last_entry['subscriber_count'],
                'total_growth': total_growth,
                'growth_percentage': round(growth_percentage, 2)
            }
        else:
            stats = {
                'total_entries': 0,
                'total_growth': 0,
                'growth_percentage': 0
            }
        
        return jsonify({
            'success': True,
            'chart_data': chart_data,
            'stats': stats,
            'has_data': len(subscriber_data) > 0
        })
        
    except Exception as e:
        print(f"[SUBSCRIBER-DATA-API] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/force-classification', methods=['POST'])
@login_required
def force_classification():
    """Force la classification manuelle bas√©e sur les patterns configur√©s"""
    try:
        from yt_channel_analyzer.database import force_manual_classification
        
        result = force_manual_classification()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[FORCE-CLASSIFICATION] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False, 
            'error': str(e),
            'competitors_count': 0,
            'playlists_classified': 0,
            'videos_classified': 0
        })

@app.route('/insights')
@login_required
def insights():
    """Page de conseils pour les cha√Ænes Center Parcs"""
    try:
        from yt_channel_analyzer.database import generate_center_parcs_insights
        
        print("[INSIGHTS] üîç G√©n√©ration des conseils Center Parcs...")
        insights_data = generate_center_parcs_insights()
        
        if insights_data.get('success'):
            print(f"[INSIGHTS] ‚úÖ {insights_data.get('total_channels', 0)} cha√Ænes Center Parcs analys√©es")
        else:
            print(f"[INSIGHTS] ‚ùå {insights_data.get('error', 'Erreur inconnue')}")
        
        return render_template('insights.html', insights=insights_data)
        
    except Exception as e:
        print(f"[INSIGHTS] ‚ùå Erreur: {e}")
        return render_template('insights.html', 
                             insights={'success': False, 'error': str(e)})

@app.route('/learn')
@login_required
def learn():
    """Learning page with guides and resources"""
    try:
        # Load the list of available guides
        guides = []
        
        # Check if the Hero Hub Help guide exists
        import os
        hero_hub_help_path = os.path.join('tools', 'hero_hub_help_matrix.md')
        if os.path.exists(hero_hub_help_path):
            guides.append({
                'title': 'Hero Hub Help Matrix',
                'description': 'Content strategy framework developed by Google to structure your YouTube content',
                'file': 'hero_hub_help_matrix.md',
                'icon': 'bi-diagram-3',
                'level': 'Beginner',
                'duration': '10 min',
                'tags': ['Strategy', 'Content', 'YouTube', 'Marketing']
            })
        
        # Here we could add other guides automatically
        # by scanning the /tools folder
        
        return render_template('learn.html', guides=guides)
        
    except Exception as e:
        print(f"[LEARN] ‚ùå Error: {e}")
        return render_template('learn.html', guides=[])

@app.route('/learn/<guide_name>')
@login_required
def learn_guide(guide_name):
    """Display a specific guide"""
    try:
        import os
        guide_path = os.path.join('tools', f'{guide_name}.md')
        
        if not os.path.exists(guide_path):
            flash('Guide not found', 'error')
            return redirect(url_for('learn'))
        
        with open(guide_path, 'r', encoding='utf-8') as f:
            guide_content = f.read()
        
        # Donn√©es hardcod√©es pour les exemples de marques (onglets CodePen)
        brands_data = [
            {
                'id': 'apple',
                'name': 'APPLE',
                'logo': 'üçé',
                'hero': [
                    'Launch keynotes (iPhone, MacBook)',
                    '"Shot on iPhone" campaigns',
                    'Apple Park events'
                ],
                'hub': [
                    '"Apple at Work" series',
                    '"Today at Apple" videos',
                    'Regular software updates'
                ],
                'help': [
                    'iOS usage tutorials',
                    'Troubleshooting guides',
                    'Photo/video tips'
                ]
            },
            {
                'id': 'hermes',
                'name': 'HERM√àS',
                'logo': 'üëú',
                'hero': [
                    'Haute couture fashion shows',
                    'Exclusive artistic collaborations',
                    'VIP boutique events'
                ],
                'hub': [
                    '"Herm√®s Craftsmanship" series',
                    'Artisan stories',
                    'Workshop behind-the-scenes'
                ],
                'help': [
                    'Leather care guides',
                    'Collection history',
                    'Style advice'
                ]
            },
            {
                'id': 'canon',
                'name': 'CANON',
                'logo': 'üì∏',
                'hero': [
                    'Flagship camera launches (EOS R5, R6)',
                    'Partnerships with famous photographers',
                    'Major event coverage (Olympics, World Cup)'
                ],
                'hub': [
                    '"Canon Creators" series',
                    'Photographer interviews',
                    'Weekly photo galleries'
                ],
                'help': [
                    'Technical tutorials',
                    'Lens buying guides',
                    'Specific settings advice'
                ]
            },
            {
                'id': 'nikon',
                'name': 'NIKON',
                'logo': 'üì∑',
                'hero': [
                    '"I Am Different" campaigns',
                    'D850, Z9 launches',
                    'Extreme photo expeditions'
                ],
                'hub': [
                    '"Nikon Ambassador" series',
                    'Monthly photo contests',
                    'Photography news'
                ],
                'help': [
                    'Nikon School photography',
                    'Lens technical guides',
                    'Maintenance and cleaning'
                ]
            }
        ]
        
        # Supprimer la section des exemples de marques du markdown (sera remplac√©e par les onglets)
        import re
        brand_section_pattern = re.compile(r'## Real Examples from Major Brands.*?(?=^## |\Z)', re.DOTALL | re.MULTILINE)
        guide_content = brand_section_pattern.sub('', guide_content)
        
        # Nettoyer les lignes vides multiples
        guide_content = re.sub(r'\n\n\n+', '\n\n', guide_content)
        
        # --- Markdown to HTML conversion am√©lior√©e ---
        # Headers
        guide_content = re.sub(r'^# (.+)$', r'<h1 class="display-4 mb-4">\1</h1>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'^## (.+)$', r'<h2 class="h3 mb-3 text-primary">\1</h2>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'^### (.+)$', r'<h3 class="h4 mb-3">\1</h3>', guide_content, flags=re.MULTILINE)
        
        # Tableaux markdown ‚Üí HTML
        def md_table_to_html(md):
            lines = [l.strip() for l in md.strip().split('\n') if l.strip()]
            if len(lines) < 3: return md
            headers = [h.strip() for h in lines[0].split('|')[1:-1]]
            rows = [r for r in lines[2:]]
            html = '<table class="recommend-table"><thead><tr>'
            for h in headers:
                html += f'<th>{h}</th>'
            html += '</tr></thead><tbody>'
            for row in rows:
                cells = [c.strip() for c in row.split('|')[1:-1]]
                html += '<tr>' + ''.join(f'<td>{c}</td>' for c in cells) + '</tr>'
            html += '</tbody></table>'
            return html
        guide_content = re.sub(r'(\|.+\|\n\|[- ]+\|[\s\S]+?\|.+\|)', lambda m: md_table_to_html(m.group(0)), guide_content)
        
        # Listes
        guide_content = re.sub(r'^- (.+)$', r'<li class="mb-2">\1</li>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'(<li class="mb-2">.*?</li>)', r'<ul class="list-unstyled ps-3">\1</ul>', guide_content, flags=re.DOTALL)
        
        # Liens
        guide_content = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" class="text-decoration-none">\1</a>', guide_content)
        
        # Bold text
        guide_content = re.sub(r'\*\*([^*]+)\*\*', r'<strong class="text-dark">\1</strong>', guide_content)
        
        # S√©parateurs
        guide_content = re.sub(r'^---$', r'<hr class="my-4">', guide_content, flags=re.MULTILINE)
        
        # Paragraphes avec style
        lines = guide_content.split('\n')
        processed_lines = []
        for line in lines:
            if line.strip() and not line.startswith('<') and not line.startswith('|'):
                processed_lines.append(f'<p class="lead mb-3" style="font-size: 1.1rem; line-height: 1.6;">{line}</p>')
            else:
                processed_lines.append(line)
        guide_content = '\n'.join(processed_lines)
        
        # Nettoyer les paragraphes vides
        guide_content = re.sub(r'<p class="lead mb-3"[^>]*>\s*</p>', '', guide_content)
        
        return render_template('learn_guide.html', 
                             guide_content=guide_content,
                             guide_name=guide_name.replace('_', ' ').title(),
                             brands_data=brands_data)
        
    except Exception as e:
        print(f"[LEARN-GUIDE] ‚ùå Error: {e}")
        flash(f'Error loading guide: {str(e)}', 'error')
        return redirect(url_for('learn'))

@app.route('/api/publication-frequency')
@login_required
def api_publication_frequency():
    """API pour r√©cup√©rer les donn√©es de fr√©quence de publication"""
    try:
        from yt_channel_analyzer.database import calculate_publication_frequency
        
        # Param√®tres optionnels
        competitor_id = request.args.get('competitor_id', type=int)
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Parser les dates si fournies
        start_dt = datetime.fromisoformat(start_date) if start_date else None
        end_dt = datetime.fromisoformat(end_date) if end_date else None
        
        # Calculer la fr√©quence
        frequency_data = calculate_publication_frequency(competitor_id, start_dt, end_dt)
        
        return jsonify({
            'success': True,
            'data': frequency_data,
            'total_competitors': len(frequency_data)
        })
        
    except Exception as e:
        print(f"[API-FREQ] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-by-country')
@login_required
def api_frequency_by_country():
    """API pour r√©cup√©rer la fr√©quence de publication agr√©g√©e par pays"""
    try:
        from yt_channel_analyzer.database import calculate_frequency_by_country
        
        frequency_data = calculate_frequency_by_country()
        
        return jsonify({
            'success': True,
            'data': frequency_data,
            'total_countries': len(frequency_data)
        })
        
    except Exception as e:
        print(f"[API-FREQ-COUNTRY] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-evolution/<int:competitor_id>')
@login_required
def api_frequency_evolution(competitor_id):
    """API pour r√©cup√©rer l'√©volution de la fr√©quence d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_frequency_evolution_data
        
        period_weeks = request.args.get('period_weeks', 12, type=int)
        
        evolution_data = get_frequency_evolution_data(competitor_id, period_weeks)
        
        if not evolution_data:
            return jsonify({'success': False, 'error': 'Aucune donn√©e trouv√©e'}), 404
        
        return jsonify({
            'success': True,
            'data': evolution_data
        })
        
    except Exception as e:
        print(f"[API-FREQ-EVOLUTION] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/dates-correction-status')
@login_required 
def api_dates_correction_status():
    """API pour r√©cup√©rer le statut de la correction des dates"""
    try:
        from yt_channel_analyzer.database import get_dates_correction_status
        
        status = get_dates_correction_status()
        
        return jsonify({
            'success': True,
            'data': status
        })
        
    except Exception as e:
        print(f"[API-DATES-STATUS] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/correct-video-dates', methods=['POST'])
@login_required
def api_correct_video_dates():
    """API pour lancer la correction des dates des vid√©os"""
    try:
        from yt_channel_analyzer.database import correct_all_video_dates_with_youtube_api
        
        print("[API-DATES-CORRECTION] üöÄ D√©but de la correction des dates")
        
        result = correct_all_video_dates_with_youtube_api()
        
        return jsonify({
            'success': result['success'],
            'message': result['message'],
            'data': {
                'total_videos': result['total_videos'],
                'updated_videos': result['updated_videos'],
                'failed_videos': result['failed_videos']
            }
        })
        
    except Exception as e:
        print(f"[API-DATES-CORRECTION] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# Routes API pour les patterns de classification multilingues
@app.route('/api/classification-patterns/<category>/<language>')
@login_required
def get_classification_patterns_api(category, language):
    """API pour r√©cup√©rer les patterns de classification pour une cat√©gorie et langue donn√©es"""
    try:
        from yt_channel_analyzer.database import get_classification_patterns
        
        if category not in ['help', 'hero', 'hub']:
            return jsonify({'success': False, 'error': 'Invalid category'}), 400
        
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'}), 400
        
        patterns = get_classification_patterns(language)
        category_patterns = patterns.get(category, [])
        
        return jsonify({
            'success': True,
            'patterns': category_patterns,
            'language': language,
            'category': category,
            'count': len(category_patterns)
        })
        
    except Exception as e:
        print(f"[API-PATTERNS] Erreur r√©cup√©ration patterns {category}/{language}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/add-classification-pattern', methods=['POST'])
@login_required
def add_classification_pattern_api():
    """API pour ajouter un nouveau pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'}), 400
        
        if category not in ['help', 'hero', 'hub']:
            return jsonify({'success': False, 'error': 'Invalid category'}), 400
            
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'}), 400
        
        from yt_channel_analyzer.database import add_classification_pattern
        success = add_classification_pattern(category, pattern, language)
        
        if success:
            print(f"[API-PATTERNS] ‚úÖ Pattern ajout√©: {pattern} ({category}/{language})")
            return jsonify({'success': True, 'message': 'Pattern added successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern already exists or could not be added'}), 409
            
    except Exception as e:
        print(f"[API-PATTERNS] Erreur ajout pattern: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/remove-classification-pattern', methods=['POST'])
@login_required
def remove_classification_pattern_api():
    """API pour supprimer un pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'}), 400
        
        from yt_channel_analyzer.database import remove_classification_pattern
        success = remove_classification_pattern(category, pattern, language)
        
        if success:
            print(f"[API-PATTERNS] ‚ùå Pattern supprim√©: {pattern} ({category}/{language})")
            return jsonify({'success': True, 'message': 'Pattern removed successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern not found or could not be removed'}), 404
            
    except Exception as e:
        print(f"[API-PATTERNS] Erreur suppression pattern: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/reset-classification-patterns', methods=['POST'])
@login_required
def reset_classification_patterns_api():
    """API pour r√©initialiser tous les patterns aux valeurs par d√©faut"""
    try:
        from yt_channel_analyzer.database import get_db_connection, get_default_classification_patterns
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Supprimer tous les patterns existants
        cursor.execute('DELETE FROM classification_patterns')
        
        # R√©initialiser avec les patterns par d√©faut pour toutes les langues
        patterns_added = 0
        for language in ['fr', 'en', 'de', 'nl']:
            default_patterns = get_default_classification_patterns(language)
            for category, pattern_list in default_patterns.items():
                for pattern in pattern_list:
                    cursor.execute('''
                        INSERT INTO classification_patterns (category, pattern, language)
                        VALUES (?, ?, ?)
                    ''', (category, pattern, language))
                    patterns_added += 1
        
        conn.commit()
        conn.close()
        
        print(f"[API-PATTERNS] üîÑ Patterns r√©initialis√©s: {patterns_added} patterns ajout√©s")
        
        return jsonify({
            'success': True,
            'message': f'Patterns reset successfully. {patterns_added} patterns added.',
            'patterns_added': patterns_added
        })
        
    except Exception as e:
        print(f"[API-PATTERNS] Erreur r√©initialisation: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-impact-analysis')
@login_required
def api_frequency_impact_analysis():
    """API pour analyser l'impact de la fr√©quence sur l'engagement"""
    try:
        from yt_channel_analyzer.database import analyze_frequency_impact_on_engagement
        
        analysis = analyze_frequency_impact_on_engagement()
        
        return jsonify({
            'success': True,
            'data': analysis
        })
        
    except Exception as e:
        print(f"[API-FREQ-IMPACT] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/frequency-dashboard')
@login_required
def frequency_dashboard():
    """Dashboard pour analyser la fr√©quence de publication"""
    try:
        from yt_channel_analyzer.database import (
            calculate_publication_frequency, 
            calculate_frequency_by_country,
            analyze_frequency_impact_on_engagement
        )
        
        print("[FREQ-DASHBOARD] üîç Chargement des donn√©es de fr√©quence...")
        
        # R√©cup√©rer les donn√©es de base avec gestion d'erreurs
        try:
            frequency_result = calculate_publication_frequency()
            if frequency_result.get('success', False):
                frequency_data = frequency_result.get('frequency_data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Donn√©es de fr√©quence: {len(frequency_data)} concurrents")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur fr√©quence: {frequency_result.get('error', 'Erreur inconnue')}")
                frequency_data = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur fr√©quence: {e}")
            frequency_data = {}
        
        try:
            country_result = calculate_frequency_by_country()
            if country_result.get('success', False):
                country_data = country_result.get('data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Donn√©es pays: {len(country_data)} pays")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur pays: {country_result.get('error', 'Fonction non impl√©ment√©e')}")
                country_data = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur pays: {e}")
            country_data = {}
        
        try:
            impact_result = analyze_frequency_impact_on_engagement()
            if impact_result.get('success', False):
                impact_analysis = impact_result.get('data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Analyse d'impact: {len(impact_analysis.get('high_frequency_performers', []))} performers")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur analyse: {impact_result.get('error', 'Fonction non impl√©ment√©e')}")
                impact_analysis = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur analyse: {e}")
            impact_analysis = {}
        
        # Donn√©es par d√©faut si vides
        if not impact_analysis:
            impact_analysis = {
                'high_frequency_performers': [],
                'low_frequency_performers': [],
                'optimal_frequency_insights': {
                    'high_performers_avg_frequency': 2.0,
                    'low_performers_avg_frequency': 0.5,
                    'frequency_impact': 'positive',
                    'recommendation': 'Donn√©es insuffisantes pour une analyse compl√®te'
                },
                'category_insights': {
                    'hero': {'avg_frequency': 0.5, 'avg_engagement': 2.0, 'competitors': 0},
                    'hub': {'avg_frequency': 1.5, 'avg_engagement': 3.0, 'competitors': 0},
                    'help': {'avg_frequency': 0.8, 'avg_engagement': 2.5, 'competitors': 0}
                }
            }
        
        # Pr√©parer les donn√©es pour le template
        competitors_list = []
        if frequency_data:
            # R√©cup√©rer les infos des concurrents depuis la DB
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Cr√©er un mapping nom -> infos concurrent
            cursor.execute('SELECT id, name, country FROM concurrent')
            competitor_info = {row[1]: {'id': row[0], 'country': row[2] or 'Unknown'} for row in cursor.fetchall()}
            conn.close()
            
            for competitor_name, stats in frequency_data.items():
                if isinstance(stats, dict):
                    # R√©cup√©rer les infos du concurrent
                    comp_info = competitor_info.get(competitor_name, {'id': 'N/A', 'country': 'Unknown'})
                    
                    competitors_list.append({
                        'id': comp_info['id'],
                        'name': competitor_name,
                        'country': comp_info['country'],
                        'avg_frequency': {
                            'total': round(stats.get('avg_videos_per_week', 0), 1),
                            'hero': stats.get('hero_frequency', 0.0),
                            'hub': stats.get('hub_frequency', 0.0),
                            'help': stats.get('help_frequency', 0.0)
                        },
                        'total_weeks': stats.get('total_weeks', 0)
                    })
                else:
                    print(f"[FREQ-DASHBOARD] ‚ö†Ô∏è Donn√©es incorrectes pour concurrent {competitor_name}: {type(stats)} = {stats}")
            
            # Trier par fr√©quence totale
            competitors_list.sort(key=lambda x: x['avg_frequency']['total'], reverse=True)
        
        print(f"[FREQ-DASHBOARD] üéØ Dashboard pr√©par√©: {len(competitors_list)} concurrents, {len(country_data)} pays")
        
        return render_template('frequency_dashboard.html',
                             competitors=competitors_list,
                             country_data=country_data,
                             impact_analysis=impact_analysis,
                             total_competitors=len(competitors_list))
        
    except Exception as e:
        print(f"[FREQ-DASHBOARD] ‚ùå Erreur critique: {e}")
        import traceback
        traceback.print_exc()
        
        # Donn√©es par d√©faut en cas d'erreur compl√®te
        default_impact_analysis = {
            'high_frequency_performers': [],
            'low_frequency_performers': [],
            'optimal_frequency_insights': {
                'high_performers_avg_frequency': 2.0,
                'low_performers_avg_frequency': 0.5,
                'frequency_impact': 'positive',
                'recommendation': 'Erreur lors du chargement des donn√©es'
            },
            'category_insights': {
                'hero': {'avg_frequency': 0.5, 'avg_engagement': 2.0, 'competitors': 0},
                'hub': {'avg_frequency': 1.5, 'avg_engagement': 3.0, 'competitors': 0},
                'help': {'avg_frequency': 0.8, 'avg_engagement': 2.5, 'competitors': 0}
            }
        }
        
        return render_template('frequency_dashboard.html', 
                             error=f"Erreur lors du chargement: {str(e)}", 
                             competitors=[], 
                             country_data={}, 
                             impact_analysis=default_impact_analysis,
                             total_competitors=0)

@app.route('/api/category-frequency-analysis')
@login_required
def api_category_frequency_analysis():
    """API pour analyser la fr√©quence par cat√©gorie HERO, HUB, HELP"""
    try:
        from yt_channel_analyzer.database import analyze_category_frequency_patterns
        
        analysis = analyze_category_frequency_patterns()
        
        return jsonify({
            'success': True,
            'data': analysis
        })
        
    except Exception as e:
        print(f"[API-CATEGORY-FREQ] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/inconsistency-stats')
@login_required
def api_inconsistency_stats():
    """API pour r√©cup√©rer les statistiques d'incoh√©rences"""
    try:
        from yt_channel_analyzer.database import get_inconsistency_stats
        
        stats = get_inconsistency_stats()
        
        return jsonify({
            'success': True,
            'total_competitors': stats.get('total_competitors', 0),
            'total_videos': stats.get('total_videos', 0),
            'total_errors': stats.get('total_errors', 0),
            'last_check': stats.get('last_check', 'Jamais')
        })
        
    except Exception as e:
        print(f"[API-INCONSISTENCY-STATS] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/detect-inconsistencies', methods=['POST'])
@login_required
def api_detect_inconsistencies():
    """API pour d√©tecter les incoh√©rences dans les donn√©es"""
    try:
        from yt_channel_analyzer.database import detect_data_inconsistencies
        
        data = request.get_json()
        analysis_type = data.get('type', 'all')
        
        print(f"[INCONSISTENCY] üîç D√©tection d'incoh√©rences: type={analysis_type}")
        
        inconsistencies = detect_data_inconsistencies(analysis_type)
        
        print(f"[INCONSISTENCY] ‚úÖ Trouv√© {len(inconsistencies)} incoh√©rence(s)")
        
        return jsonify({
            'success': True,
            'inconsistencies': inconsistencies,
            'total_found': len(inconsistencies)
        })
        
    except Exception as e:
        print(f"[API-DETECT-INCONSISTENCIES] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/sync-competitors-to-db', methods=['POST'])
@login_required
def sync_competitors_to_db():
    """Synchroniser tous les concurrents du cache vers la base de donn√©es"""
    try:
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import save_competitor_and_videos
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        competitors = cache_data.get('competitors', {})
        
        if not competitors:
            return jsonify({
                'success': False,
                'error': 'Aucun concurrent trouv√© dans le cache'
            })
        
        synced_count = 0
        errors = []
        
        for competitor_key, competitor_data in competitors.items():
            try:
                if not competitor_data.get('channel_url'):
                    errors.append(f"Concurrent {competitor_key}: URL manquante")
                    continue
                
                # Extraire les informations de la cha√Æne
                channel_info = {
                    'title': competitor_data.get('name', 'Canal inconnu'),
                    'thumbnail': competitor_data.get('thumbnail', ''),
                    'banner': competitor_data.get('banner', ''),
                    'description': competitor_data.get('description', ''),
                    'subscriber_count': competitor_data.get('subscriber_count'),
                    'view_count': competitor_data.get('view_count'),
                    'country': competitor_data.get('country', ''),
                    'language': competitor_data.get('language', '')
                }
                
                # Formater les vid√©os
                videos = []
                for video in competitor_data.get('videos', []):
                    videos.append({
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail_url': video.get('thumbnail', ''),
                        'published_at': video.get('publication_date', ''),
                        'duration_text': video.get('duration', ''),
                        'view_count': video.get('view_count', 0),
                        'like_count': video.get('likes', 0),
                        'comment_count': video.get('comments', 0)
                    })
                
                # Sauvegarder en base
                competitor_id = save_competitor_and_videos(
                    competitor_data['channel_url'],
                    videos,
                    channel_info
                )
                
                synced_count += 1
                print(f"[SYNC] ‚úÖ Concurrent {competitor_data.get('name')} synchronis√© (ID: {competitor_id})")
                
            except Exception as e:
                error_msg = f"Concurrent {competitor_key}: {str(e)}"
                errors.append(error_msg)
                print(f"[SYNC] ‚ùå {error_msg}")
        
        return jsonify({
            'success': True,
            'synced_count': synced_count,
            'total_competitors': len(competitors),
            'errors': errors
        })
        
    except Exception as e:
        print(f"[SYNC] ‚ùå Erreur lors de la synchronisation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/check-competitors-db-status', methods=['GET'])
@login_required
def check_competitors_db_status():
    """V√©rifier le statut de synchronisation des concurrents"""
    try:
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import get_all_competitors
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        cache_competitors = cache_data.get('competitors', {})
        
        # R√©cup√©rer les concurrents en base
        db_competitors = get_all_competitors()
        
        # Analyser les diff√©rences
        cache_urls = set()
        db_urls = set()
        
        for competitor_data in cache_competitors.values():
            if competitor_data.get('channel_url'):
                cache_urls.add(competitor_data['channel_url'])
        
        for competitor in db_competitors:
            if competitor.get('channel_url'):
                db_urls.add(competitor['channel_url'])
        
        # Concurrents uniquement en cache
        only_in_cache = cache_urls - db_urls
        
        # Concurrents uniquement en base
        only_in_db = db_urls - cache_urls
        
        # Concurrents en commun
        in_both = cache_urls & db_urls
        
        return jsonify({
            'success': True,
            'cache_count': len(cache_competitors),
            'db_count': len(db_competitors),
            'only_in_cache': len(only_in_cache),
            'only_in_db': len(only_in_db),
            'in_both': len(in_both),
            'sync_needed': len(only_in_cache) > 0,
            'cache_competitors': list(cache_urls),
            'db_competitors': list(db_urls),
            'missing_from_db': list(only_in_cache),
            'missing_from_cache': list(only_in_db)
        })
        
    except Exception as e:
        print(f"[CHECK] ‚ùå Erreur lors de la v√©rification: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/clean-duplicate-competitors', methods=['POST'])
@login_required
def clean_duplicate_competitors():
    """Nettoyer les concurrents en double dans la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import clean_duplicate_competitors
        
        result = clean_duplicate_competitors()
        
        return jsonify({
            'success': True,
            'deleted_count': result['deleted_count'],
            'kept_competitors': result['kept_competitors'],
            'details': result['details']
        })
        
    except Exception as e:
        print(f"[CLEAN] ‚ùå Erreur lors du nettoyage: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/update-database-schema', methods=['POST'])
@login_required
def update_database_schema_api():
    """Mettre √† jour le sch√©ma de la base de donn√©es avec les champs de tagging"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        update_database_schema()
        
        return jsonify({
            'success': True,
            'message': 'Sch√©ma de base de donn√©es mis √† jour avec succ√®s'
        })
        
    except Exception as e:
        print(f"[DB-SCHEMA] ‚ùå Erreur lors de la mise √† jour du sch√©ma: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitor/<int:competitor_id>/tags', methods=['PUT'])
@login_required
def update_competitor_tags_api(competitor_id):
    """Mettre √† jour les tags d'un concurrent"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Donn√©es manquantes'
            })
        
        from yt_channel_analyzer.database import update_competitor_tags
        success = update_competitor_tags(competitor_id, data)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Tags mis √† jour avec succ√®s'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Erreur lors de la mise √† jour des tags'
            })
            
    except Exception as e:
        print(f"[TAGS-API] ‚ùå Erreur lors de la mise √† jour des tags: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitor/<int:competitor_id>/tags', methods=['GET'])
@login_required
def get_competitor_tags_api(competitor_id):
    """R√©cup√©rer les tags d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_competitor_by_id
        competitor = get_competitor_by_id(competitor_id)
        
        if not competitor:
            return jsonify({
                'success': False,
                'error': 'Concurrent non trouv√©'
            })
        
        return jsonify({
            'success': True,
            'tags': {
                'sector': competitor.get('sector', 'hospitality'),
                'tags': competitor.get('tags', ''),
                'custom_region': competitor.get('custom_region', ''),
                'notes': competitor.get('notes', ''),
                'is_active': competitor.get('is_active', 1)
            }
        })
        
    except Exception as e:
        print(f"[TAGS-API] ‚ùå Erreur lors de la r√©cup√©ration des tags: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/ensure-all-competitors-in-db', methods=['POST'])
@login_required
def ensure_all_competitors_in_db():
    """V√©rifier et synchroniser tous les concurrents vers la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        
        # D'abord mettre √† jour le sch√©ma
        update_database_schema()
        
        # Ensuite synchroniser les concurrents
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import save_competitor_and_videos, get_all_competitors
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        competitors = cache_data.get('competitors', {})
        
        # R√©cup√©rer les concurrents existants en base
        db_competitors = get_all_competitors()
        db_urls = set(comp['channel_url'] for comp in db_competitors)
        
        synced_count = 0
        errors = []
        
        for competitor_key, competitor_data in competitors.items():
            try:
                channel_url = competitor_data.get('channel_url')
                if not channel_url:
                    errors.append(f"Concurrent {competitor_key}: URL manquante")
                    continue
                
                # V√©rifier si d√©j√† en base
                if channel_url in db_urls:
                    continue
                
                # Extraire les informations de la cha√Æne
                channel_info = {
                    'title': competitor_data.get('name', 'Canal inconnu'),
                    'thumbnail': competitor_data.get('thumbnail', ''),
                    'banner': competitor_data.get('banner', ''),
                    'description': competitor_data.get('description', ''),
                    'subscriber_count': competitor_data.get('subscriber_count'),
                    'view_count': competitor_data.get('view_count'),
                    'country': competitor_data.get('country', ''),
                    'language': competitor_data.get('language', '')
                }
                
                # Formater les vid√©os
                videos = []
                for video in competitor_data.get('videos', []):
                    video_formatted = {
                        'video_id': video.get('url', '').split('=')[-1] if video.get('url') else '',
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail_url': video.get('thumbnail', ''),
                        'published_at': video.get('publication_date', ''),
                        'duration_text': video.get('duration', ''),
                        'view_count': video.get('views', 0),
                        'like_count': video.get('likes', 0),
                        'comment_count': video.get('comments', 0),
                        'category': video.get('category', 'hub')
                    }
                    videos.append(video_formatted)
                
                # Sauvegarder en base
                save_competitor_and_videos(channel_url, videos, channel_info)
                synced_count += 1
                
            except Exception as e:
                errors.append(f"Erreur pour {competitor_key}: {str(e)}")
        
        return jsonify({
            'success': True,
            'synced_count': synced_count,
            'errors': errors,
            'message': f'{synced_count} concurrents synchronis√©s avec succ√®s'
        })
        
    except Exception as e:
        print(f"[SYNC] ‚ùå Erreur lors de la synchronisation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/get-action-status/<action_key>')
@login_required
def get_action_status(action_key):
    """Get the last status for a specific action from database logs"""
    try:
        from yt_channel_analyzer.database import get_last_action_status
        status = get_last_action_status(action_key)
        
        if status:
            return jsonify({
                'success': True,
                'status': status
            })
        else:
            return jsonify({
                'success': False,
                'message': 'No status found for this action'
            })
    except Exception as e:
        print(f"[GET-ACTION-STATUS] Error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/reclassify-content', methods=['POST'])
def reclassify_content():
    """Reclassifier manuellement des playlists ou vid√©os sp√©cifiques"""
    try:
        data = request.json
        content_type = data.get('type')  # 'playlist' or 'video'
        content_id = data.get('id')
        
        if not content_type or not content_id:
            return jsonify({'error': 'Type et ID requis'}), 400
            
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if content_type == 'playlist':
            # Reclassifier une playlist
            cursor.execute('SELECT name, description FROM playlist WHERE id = ?', (content_id,))
            result = cursor.fetchone()
            
            if not result:
                return jsonify({'error': 'Playlist non trouv√©e'}), 404
                
            name, description = result
            category, language, confidence = classify_video_with_language(name, description or '')
            
            cursor.execute('''
                UPDATE playlist SET category = ?, last_updated = ? 
                WHERE id = ?
            ''', (category, datetime.now(), content_id))
            
            conn.commit()
            print(f"[API-RECLASSIFY] üìã Playlist '{name}' reclassifi√©e: {category.upper()} ({confidence}%)")
            
            return jsonify({
                'success': True,
                'message': f'Playlist reclassifi√©e comme {category.upper()}',
                'category': category,
                'confidence': confidence,
                'language': language
            })
            
        elif content_type == 'video':
            # Reclassifier une vid√©o
            cursor.execute('SELECT title, description FROM video WHERE id = ?', (content_id,))
            result = cursor.fetchone()
            
            if not result:
                return jsonify({'error': 'Vid√©o non trouv√©e'}), 404
                
            title, description = result
            category, language, confidence = classify_video_with_language(title, description or '')
            
            cursor.execute('''
                UPDATE video SET category = ?, last_updated = ? 
                WHERE id = ?
            ''', (category, datetime.now(), content_id))
            
            conn.commit()
            print(f"[API-RECLASSIFY] üé¨ Vid√©o '{title}' reclassifi√©e: {category.upper()} ({confidence}%)")
            
            return jsonify({
                'success': True,
                'message': f'Vid√©o reclassifi√©e comme {category.upper()}',
                'category': category,
                'confidence': confidence,
                'language': language
            })
            
        else:
            return jsonify({'error': 'Type invalide (playlist ou video)'}), 400
            
    except Exception as e:
        print(f"[API-RECLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'conn' in locals():
            conn.close()

# Routes pour l'apprentissage supervis√©
@app.route('/supervised-learning')
@login_required
def supervised_learning_page():
    """Page d'apprentissage supervis√© pour la classification Hero/Hub/Help - Version optimis√©e"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation, get_playlists_for_validation, get_learning_statistics
        
        videos = get_videos_for_validation(limit=12, strategy=strategy)
        playlists = get_playlists_for_validation(limit=12, strategy=strategy)
        stats = get_learning_statistics()
        
        video_distribution = {'hero': 0, 'hub': 0, 'help': 0}
        total_videos = stats.get('total_videos', 0)
        
        if total_videos > 0:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT category, COUNT(*) as count FROM video WHERE category IS NOT NULL GROUP BY category')
            for row in cursor.fetchall():
                if row[0] in video_distribution:
                    video_distribution[row[0]] = row[1] / total_videos
            conn.close()

        return render_template('supervised_learning.html',
                            videos=videos,
                            playlists=playlists,
                            strategy=strategy,
                            stats=stats,
                            video_distribution=video_distribution,
                            initial_load=True)
    except Exception as e:
        flash(f"Erreur sur la page d'apprentissage : {e}", 'error')
        return redirect(url_for('home'))

@app.route('/supervised-learning/stats')
@login_required
def supervised_learning_stats():
    """Page des statistiques de la classification supervis√©e"""
    try:
        # Importer directement depuis les sous-modules pour √©viter les imports circulaires
        from yt_channel_analyzer.supervised_learning import get_human_classifications
        from yt_channel_analyzer.database.competitors import get_all_competitors

        # On charge les 100 derni√®res classifications pour l'historique.
        # La fonction retourne aussi les stats globales.
        stats = get_human_classifications(limit=100)
        competitors = get_all_competitors()
        
        return render_template('supervised_learning_stats.html', stats=stats, competitors=competitors)
        
    except Exception as e:
        print(f"[STATS] ‚ùå ERREUR CRITIQUE dans /supervised-learning/stats: {e}")
        import traceback
        traceback.print_exc()
        return render_template('error.html', error_message=f"Erreur lors du chargement des statistiques: {e}")

@app.route('/api/supervised/feedback', methods=['POST'])
@login_required
def api_supervised_feedback():
    """API pour enregistrer un feedback utilisateur sur une classification"""
    try:
        data = request.json
        
        video_id = data.get('video_id')
        category = data.get('category')
        feedback_type = data.get('feedback_type', 'correction')  # correction, validation, uncertain
        user_notes = data.get('user_notes', '')
        
        if not video_id or not category:
            return jsonify({'status': 'error', 'message': 'Video ID et cat√©gorie requis'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'status': 'error', 'message': 'Cat√©gorie invalide'})
        
        # R√©cup√©rer la cat√©gorie originale
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT category, classification_source FROM video WHERE id = ?', (video_id,))
        result = cursor.fetchone()
        
        if not result:
            conn.close()
            return jsonify({'status': 'error', 'message': 'Vid√©o non trouv√©e'})
        
        original_category = result[0] or 'hub'  # Utiliser hub si pas de cat√©gorie
        original_source = result[1] or 'unknown'
        conn.close()
        
        # üö® NOUVEAU : Utiliser la fonction de marquage humain pour cette classification
        from yt_channel_analyzer.database import mark_human_classification
        
        # Marquer comme classification humaine (priorit√© absolue)
        success = mark_human_classification(
            video_id=video_id, 
            category=category, 
            user_notes=f"Feedback utilisateur: {feedback_type} - {user_notes}"
        )
        
        if not success:
            return jsonify({'status': 'error', 'message': 'Erreur lors du marquage de la classification humaine'})
        
        # Ajouter le feedback dans la table de supervision pour historique complet
        from yt_channel_analyzer.supervised_learning import add_user_feedback
        feedback_success = add_user_feedback(
            video_id=video_id,
            original_category=original_category,
            corrected_category=category,
            confidence_score=100,  # 100% de confiance pour les corrections humaines
            user_feedback_type=feedback_type,
            user_notes=user_notes
        )
        
        if feedback_success:
            # Message de succ√®s avec indication de protection
            message = f'ü§ñ Vid√©o reclassifi√©e: {original_category.upper()} ‚Üí {category.upper()} (HUMAIN - PROT√âG√âE)'
            if original_source == 'human':
                message += ' [Correction humaine pr√©c√©dente √©cras√©e]'
            
            return jsonify({
                'status': 'success', 
                'message': message,
                'original_category': original_category,
                'new_category': category,
                'human_protected': True,
                'feedback_type': feedback_type
            })
        else:
            return jsonify({'status': 'error', 'message': 'Erreur lors de l\'ajout du feedback'})
        
    except Exception as e:
        print(f"[SUPERVISED-FEEDBACK] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': f'Erreur du serveur: {str(e)}'})

@app.route('/api/supervised/get-videos', methods=['GET'])
@login_required
def api_supervised_get_videos():
    """API pour r√©cup√©rer les vid√©os √† valider selon la strat√©gie choisie"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        limit = int(request.args.get('limit', 50))
        
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation
        videos = get_videos_for_validation(limit=limit, strategy=strategy)
        
        return jsonify({
            'status': 'success',
            'videos': videos,
            'count': len(videos),
            'strategy': strategy
        })
        
    except Exception as e:
        print(f"[API-GET-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/stats', methods=['GET'])
@login_required
def api_supervised_stats():
    """API pour r√©cup√©rer les statistiques d'apprentissage"""
    try:
        from yt_channel_analyzer.supervised_learning import get_learning_statistics
        stats = get_learning_statistics()
        
        return jsonify({
            'status': 'success',
            'stats': stats
        })
        
    except Exception as e:
        print(f"[API-STATS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})


@app.route('/api/supervised/classify', methods=['POST'])
@login_required
def api_supervised_classify():
    """API pour classifier une vid√©o avec l'apprentissage supervis√©"""
    try:
        data = request.json
        title = data.get('title', '')
        description = data.get('description', '')
        
        if not title:
            return jsonify({'status': 'error', 'message': 'Titre requis'})
        
        from yt_channel_analyzer.supervised_learning import classify_with_supervised_learning
        category, language, confidence = classify_with_supervised_learning(title, description)
        
        return jsonify({
            'status': 'success',
            'category': category,
            'language': language,
            'confidence': confidence
        })
        
    except Exception as e:
        print(f"[API-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/load-more-videos', methods=['GET'])
@login_required
def api_load_more_videos():
    """API pour charger plus de vid√©os avec pagination"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        offset = request.args.get('offset', 0, type=int)
        limit = request.args.get('limit', 12, type=int)
        
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation
        videos = get_videos_for_validation(limit=limit, offset=offset, strategy=strategy)
        
        # Convertir les vid√©os en format JSON
        videos_data = []
        for video in videos:
            videos_data.append({
                'id': video.id,
                'video_id': video.video_id,
                'title': video.title,
                'thumbnail_url': video.thumbnail_url,
                'competitor_name': video.competitor_name,
                'view_count': video.view_count,
                'like_count': video.like_count,
                'category': video.category,
                'source': video.source
            })
        
        return jsonify({
            'status': 'success',
            'videos': videos_data,
            'has_more': len(videos) == limit  # S'il y a exactement 'limit' r√©sultats, il y en a probablement plus
        })
        
    except Exception as e:
        print(f"[API-LOAD-MORE-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/load-more-playlists', methods=['GET'])
@login_required
def api_load_more_playlists():
    """API pour charger plus de playlists avec pagination"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        offset = request.args.get('offset', 0, type=int)
        limit = request.args.get('limit', 12, type=int)
        
        from yt_channel_analyzer.supervised_learning import get_playlists_for_validation
        playlists = get_playlists_for_validation(limit=limit, offset=offset, strategy=strategy)
        
        # Convertir les playlists en format JSON
        playlists_data = []
        for playlist in playlists:
            playlists_data.append({
                'id': playlist.id,
                'playlist_id': playlist.playlist_id,
                'name': playlist.name,
                'thumbnail_url': playlist.thumbnail_url,
                'competitor_name': playlist.competitor_name,
                'video_count': playlist.video_count,
                'confidence': playlist.confidence,
                'category': playlist.category,
                'source': playlist.source
            })
        
        return jsonify({
            'status': 'success',
            'playlists': playlists_data,
            'has_more': len(playlists) == limit  # S'il y a exactement 'limit' r√©sultats, il y en a probablement plus
        })
        
    except Exception as e:
        print(f"[API-LOAD-MORE-PLAYLISTS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/playlist-feedback', methods=['POST'])
@login_required
def api_supervised_playlist_feedback():
    """API pour enregistrer un feedback utilisateur sur une classification de playlist"""
    try:
        data = request.json
        
        playlist_id = data.get('playlist_id')
        category = data.get('category')
        feedback_type = data.get('feedback_type', 'correction')  # correction, validation, uncertain
        user_notes = data.get('user_notes', '')
        
        if not playlist_id or not category:
            return jsonify({'status': 'error', 'message': 'Playlist ID et cat√©gorie requis'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'status': 'error', 'message': 'Cat√©gorie invalide'})
        
        print(f"[API-PLAYLIST-FEEDBACK] üìã Feedback playlist ID {playlist_id}: {category} ({feedback_type})")
        
        # Utiliser la fonction d'apprentissage supervis√© pour les playlists
        from yt_channel_analyzer.supervised_learning import add_playlist_feedback
        
        # Ajouter le feedback et apprendre de nouveaux patterns
        learning_result = add_playlist_feedback(
            playlist_id=playlist_id,
            category=category,
            feedback_type=feedback_type,
            user_notes=user_notes
        )
        
        if learning_result['status'] == 'success':
            print(f"[API-PLAYLIST-FEEDBACK] ‚úÖ Feedback playlist enregistr√© avec succ√®s")
            return jsonify({
                'status': 'success',
                'message': 'Classification playlist mise √† jour avec succ√®s',
                'learning_result': learning_result.get('learning_result', {})
            })
        else:
            return jsonify({
                'status': 'error',
                'message': learning_result.get('message', 'Erreur inconnue')
            })
        
    except Exception as e:
        print(f"[API-PLAYLIST-FEEDBACK] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/human-classifications')
@login_required
def human_classifications():
    """Page des vid√©os ET playlists classifi√©es par l'humain"""
    try:
        # R√©cup√©rer les param√®tres de pagination
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)
        offset = (page - 1) * per_page
        
        # R√©cup√©rer les donn√©es (vid√©os + playlists)
        from yt_channel_analyzer.supervised_learning import get_human_classifications
        data = get_human_classifications(limit=per_page, offset=offset)
        
        # Calculer les informations de pagination
        total_items = data.get('total_count', 0)
        total_pages = (total_items + per_page - 1) // per_page
        
        print(f"[HUMAN-CLASSIFICATIONS] üìä {data.get('video_count', 0)} vid√©os + {data.get('playlist_count', 0)} playlists = {total_items} classifications humaines")
        
        return render_template('human_classifications.html',
                            classifications=data.get('classifications', []),
                            video_classifications=data.get('video_classifications', []),
                            playlist_classifications=data.get('playlist_classifications', []),
                            total_count=data.get('total_count', 0),
                            video_count=data.get('video_count', 0),
                            playlist_count=data.get('playlist_count', 0),
                            unique_videos=data.get('unique_videos', 0),
                            unique_playlists=data.get('unique_playlists', 0),
                            affected_competitors=data.get('affected_competitors', 0),
                            video_reclassification_matrix=data.get('video_reclassification_matrix', {}),
                            playlist_category_distribution=data.get('playlist_category_distribution', {}),
                            competitor_stats=data.get('competitor_stats', []),
                            current_page=page,
                            per_page=per_page,
                            total_pages=total_pages,
                            has_prev=page > 1,
                            has_next=page < total_pages)
                            
    except Exception as e:
        print(f"[HUMAN-CLASSIFICATIONS] ‚ùå Erreur: {e}")
        flash(f"Une erreur est survenue: {str(e)}", 'error')
        return render_template('human_classifications.html',
                            classifications=[],
                            video_classifications=[],
                            playlist_classifications=[],
                            total_count=0,
                            video_count=0,
                            playlist_count=0,
                            unique_videos=0,
                            unique_playlists=0,
                            affected_competitors=0,
                            error=str(e))

@app.route('/api/competitor/<int:competitor_id>/classify', methods=['POST'])
@login_required
def classify_competitor_videos(competitor_id):
    """Classifier manuellement toutes les vid√©os d'un concurrent sp√©cifique"""
    try:
        from yt_channel_analyzer.database import classify_videos_directly_with_keywords, get_competitor_by_id
        
        # V√©rifier que le concurrent existe
        competitor = get_competitor_by_id(competitor_id)
        if not competitor:
            return jsonify({
                'success': False,
                'error': 'Concurrent non trouv√©'
            })
        
        # Classifier les vid√©os
        print(f"[MANUAL-CLASSIFY] ü§ñ D√©but de la classification pour {competitor['name']}")
        classification_result = classify_videos_directly_with_keywords(competitor_id)
        
        videos_classified = classification_result.get('videos_classified', 0)
        
        if videos_classified > 0:
            print(f"[MANUAL-CLASSIFY] ‚úÖ Classification termin√©e: {videos_classified} vid√©os classifi√©es")
            message = f"Classification termin√©e: {videos_classified} vid√©os classifi√©es pour {competitor['name']}"
        else:
            print(f"[MANUAL-CLASSIFY] ‚ÑπÔ∏è Aucune vid√©o √† classifier")
            message = f"Aucune vid√©o √† classifier pour {competitor['name']} (d√©j√† classifi√©es)"
        
        return jsonify({
            'success': True,
            'message': message,
            'videos_classified': videos_classified,
            'competitor_name': competitor['name']
        })
        
    except Exception as e:
        print(f"[MANUAL-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/classify-all-unclassified', methods=['POST'])
@login_required
def classify_all_unclassified():
    """Classifier toutes les vid√©os non classifi√©es de tous les concurrents"""
    try:
        from yt_channel_analyzer.database import get_all_competitors, classify_videos_directly_with_keywords
        
        # R√©cup√©rer tous les concurrents
        competitors = get_all_competitors()
        
        if not competitors:
            return jsonify({
                'success': False,
                'error': 'Aucun concurrent trouv√©'
            })
        
        total_classified = 0
        classified_competitors = []
        
        print(f"[CLASSIFY-ALL] ü§ñ D√©but de la classification pour {len(competitors)} concurrents")
        
        for competitor in competitors:
            competitor_id = competitor['id']
            competitor_name = competitor['name']
            
            try:
                # Classifier les vid√©os de ce concurrent
                classification_result = classify_videos_directly_with_keywords(competitor_id)
                videos_classified = classification_result.get('videos_classified', 0)
                
                if videos_classified > 0:
                    total_classified += videos_classified
                    classified_competitors.append({
                        'name': competitor_name,
                        'videos_classified': videos_classified
                    })
                    print(f"[CLASSIFY-ALL] ‚úÖ {competitor_name}: {videos_classified} vid√©os classifi√©es")
                
            except Exception as e:
                print(f"[CLASSIFY-ALL] ‚ùå Erreur pour {competitor_name}: {e}")
                continue
        
        if total_classified > 0:
            message = f"Classification termin√©e: {total_classified} vid√©os classifi√©es pour {len(classified_competitors)} concurrents"
        else:
            message = "Aucune vid√©o √† classifier (toutes d√©j√† classifi√©es)"
        
        return jsonify({
            'success': True,
            'message': message,
            'total_classified': total_classified,
            'classified_competitors': classified_competitors
        })
        
    except Exception as e:
        print(f"[CLASSIFY-ALL] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/update-database-schema-with-human-protection', methods=['POST'])
@login_required
def update_database_schema_with_human_protection():
    """Mettre √† jour le sch√©ma de base de donn√©es avec les champs de protection humaine"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        
        print("[SCHEMA-UPDATE] üö® Mise √† jour du sch√©ma avec protection supervision humaine...")
        
        success = update_database_schema()
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Sch√©ma de base mis √† jour avec succ√®s - Protection supervision humaine activ√©e !'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Erreur lors de la mise √† jour du sch√©ma'
            })
        
    except Exception as e:
        print(f"[SCHEMA-UPDATE] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/verify-classification-integrity-advanced', methods=['POST'])
@login_required
def verify_classification_integrity_advanced():
    """V√©rification avanc√©e de l'int√©grit√© des classifications humaines vs IA"""
    try:
        from yt_channel_analyzer.database import verify_classification_integrity
        
        print("[INTEGRITY-CHECK] üîç V√©rification avanc√©e de l'int√©grit√©...")
        
        report = verify_classification_integrity()
        
        return jsonify({
            'success': True,
            'report': report
        })
        
    except Exception as e:
        print(f"[INTEGRITY-CHECK] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/fix-classification-tracking', methods=['POST'])
@login_required
def fix_classification_tracking_api():
    """R√©parer les probl√®mes de tracking de classification"""
    try:
        from yt_channel_analyzer.database import fix_classification_tracking
        
        print("[INTEGRITY-FIX] üõ†Ô∏è D√©but de la r√©paration...")
        
        result = fix_classification_tracking()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[INTEGRITY-FIX] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/debug-functions-safety-check', methods=['POST'])
@login_required
def debug_functions_safety_check():
    """V√©rifier que toutes les fonctions de debug sont s√©curis√©es"""
    try:
        safety_report = {
            'functions_checked': [],
            'safe_functions': [],
            'protected_logic': [],
            'warnings': []
        }
        
        # Liste des fonctions critiques √† v√©rifier
        critical_functions = [
            {
                'name': 'classify_videos_directly_with_keywords',
                'protection': 'Respecte les classifications humaines',
                'safe': True
            },
            {
                'name': 'apply_playlist_categories_to_videos',
                'protection': 'Ne propage jamais les playlists humaines',
                'safe': True
            },
            {
                'name': 'auto_classify_uncategorized_playlists',
                'protection': 'Ignore les playlists classifi√©es manuellement',
                'safe': True
            },
            {
                'name': 'run_global_ai_classification',
                'protection': 'Utilise toutes les protections ci-dessus',
                'safe': True
            },
            {
                'name': 'mark_human_classification',
                'protection': 'Priorit√© absolue - ne peut pas √™tre √©cras√©e',
                'safe': True
            }
        ]
        
        for func in critical_functions:
            safety_report['functions_checked'].append(func['name'])
            if func['safe']:
                safety_report['safe_functions'].append({
                    'function': func['name'],
                    'protection': func['protection']
                })
            else:
                safety_report['warnings'].append({
                    'function': func['name'],
                    'issue': func.get('issue', 'Protection insuffisante')
                })
        
        safety_report['protected_logic'] = [
            "üõ°Ô∏è Double v√©rification avant toute modification",
            "üö´ Exclusion explicite des classifications humaines",
            "‚úÖ Tracking complet de la source (human/ai/playlist)",
            "‚ö†Ô∏è Fonction force_apply_human_playlist_to_videos avec confirmation requise",
            "üìä Logs d√©taill√©s pour tra√ßabilit√©"
        ]
        
        safety_report['summary'] = "‚úÖ Toutes les fonctions de debug sont maintenant s√©curis√©es"
        
        return jsonify({
            'success': True,
            'safety_report': safety_report
        })
        
    except Exception as e:
        print(f"[SAFETY-CHECK] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/protection-summary', methods=['GET'])
@login_required
def protection_summary():
    """Afficher un r√©capitulatif des protections mises en place"""
    try:
        summary = {
            'protection_status': '‚úÖ PROTECTION SUPERVISION HUMAINE ACTIV√âE',
            'key_protections': [
                "üõ°Ô∏è Les classifications humaines ne peuvent JAMAIS √™tre √©cras√©es",
                "üö´ Les playlists classifi√©es manuellement ne propagent PAS automatiquement aux vid√©os",
                "üìä Tracking complet de la source (human/ai/playlist)",
                "üîç Badges visuels pour identifier les classifications",
                "‚ö†Ô∏è Fonction de propagation forc√©e SEULEMENT avec confirmation explicite"
            ],
            'protected_functions': [
                "classify_videos_directly_with_keywords ‚Üí Ignore les vid√©os humaines",
                "apply_playlist_categories_to_videos ‚Üí Ignore les playlists humaines",
                "auto_classify_uncategorized_playlists ‚Üí Ignore les playlists humaines",
                "run_global_ai_classification ‚Üí Utilise toutes les protections",
                "mark_human_classification ‚Üí Priorit√© absolue"
            ],
            'new_features': [
                "üîß V√©rification d'int√©grit√© avanc√©e",
                "üõ†Ô∏è R√©paration automatique du tracking",
                "üëÅÔ∏è Badges visuels HUMAIN/IA/AUTO",
                "üìã Rapports de s√©curit√© d√©taill√©s",
                "üö® Alertes en cas de conflit"
            ],
            'how_to_use': [
                "1. Utilisez la page /debug pour v√©rifier l'int√©grit√©",
                "2. R√©parez le tracking si n√©cessaire",
                "3. Classifiez manuellement ‚Üí Badge HUMAIN (prot√©g√©)",
                "4. Classifiez via IA ‚Üí Badge IA (modifiable)",
                "5. V√©rifiez r√©guli√®rement avec les outils de debug"
            ]
        }
        
        return jsonify({
            'success': True,
            'summary': summary
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/check-existing-classifications', methods=['GET'])
@login_required
def check_existing_classifications():
    """V√©rifier rapidement les classifications existantes avant mise √† jour du sch√©ma"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        results = {
            'classification_feedback': {'count': 0, 'exists': False, 'recent': []},
            'video_categories': {'count': 0, 'exists': False},
            'playlist_categories': {'count': 0, 'exists': False},
            'schema_status': {'video_tracking': False, 'playlist_tracking': False}
        }
        
        # 1. V√©rifier les feedbacks de classification (vos 10 classifications)
        try:
            cursor.execute('''
                SELECT COUNT(*) FROM classification_feedback 
                WHERE user_feedback_type = 'correction'
            ''')
            feedback_count = cursor.fetchone()[0]
            results['classification_feedback']['count'] = feedback_count
            results['classification_feedback']['exists'] = feedback_count > 0
            
            if feedback_count > 0:
                cursor.execute('''
                    SELECT cf.corrected_category, cf.feedback_timestamp, v.title, c.name as competitor_name
                    FROM classification_feedback cf
                    JOIN video v ON cf.video_id = v.id
                    JOIN concurrent c ON v.concurrent_id = c.id
                    WHERE cf.user_feedback_type = 'correction'
                    ORDER BY cf.feedback_timestamp DESC
                    LIMIT 5
                ''')
                results['classification_feedback']['recent'] = [
                    {
                        'category': row[0],
                        'date': row[1],
                        'video_title': row[2][:50] + '...' if len(row[2]) > 50 else row[2],
                        'competitor': row[3]
                    }
                    for row in cursor.fetchall()
                ]
        except Exception as e:
            results['classification_feedback']['error'] = str(e)
        
        # 2. V√©rifier les vid√©os cat√©goris√©es
        try:
            cursor.execute('SELECT COUNT(*) FROM video WHERE category IS NOT NULL')
            video_count = cursor.fetchone()[0]
            results['video_categories']['count'] = video_count
            results['video_categories']['exists'] = video_count > 0
        except Exception as e:
            results['video_categories']['error'] = str(e)
        
        # 3. V√©rifier les playlists cat√©goris√©es
        try:
            cursor.execute('SELECT COUNT(*) FROM playlist WHERE category IS NOT NULL')
            playlist_count = cursor.fetchone()[0]
            results['playlist_categories']['count'] = playlist_count
            results['playlist_categories']['exists'] = playlist_count > 0
        except Exception as e:
            results['playlist_categories']['error'] = str(e)
        
        # 4. V√©rifier l'√©tat du sch√©ma
        try:
            # V√©rifier colonnes video
            cursor.execute("PRAGMA table_info(video)")
            video_columns = [col[1] for col in cursor.fetchall()]
            results['schema_status']['video_tracking'] = 'classification_source' in video_columns
            
            # V√©rifier colonnes playlist
            cursor.execute("PRAGMA table_info(playlist)")
            playlist_columns = [col[1] for col in cursor.fetchall()]
            results['schema_status']['playlist_tracking'] = 'classification_source' in playlist_columns
            
        except Exception as e:
            results['schema_status']['error'] = str(e)
        
        conn.close()
        
        # 5. D√©terminer le statut global
        if results['classification_feedback']['exists']:
            results['status'] = 'data_exists_schema_incomplete'
            results['message'] = f"‚úÖ {results['classification_feedback']['count']} classifications trouv√©es ! Le sch√©ma doit √™tre mis √† jour."
            results['solution'] = "Cliquez sur 'Mettre √† jour le sch√©ma (Protection humaine)' sur /debug"
        elif results['video_categories']['exists'] or results['playlist_categories']['exists']:
            results['status'] = 'some_data_exists'
            results['message'] = "‚ö†Ô∏è Donn√©es partielles trouv√©es"
        else:
            results['status'] = 'no_data'
            results['message'] = "‚ÑπÔ∏è Aucune classification trouv√©e"
        
        return jsonify({
            'success': True,
            'results': results
        })
        
    except Exception as e:
        print(f"[CHECK-CLASSIFICATIONS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/enhance-patterns', methods=['POST'])
@login_required
def enhance_patterns_for_descriptions():
    """Installer les patterns enrichis pour mieux utiliser les descriptions"""
    try:
        from yt_channel_analyzer.enhanced_patterns import add_description_patterns_to_db
        
        patterns_added = add_description_patterns_to_db()
        
        return jsonify({
            'success': True,
            'patterns_added': patterns_added,
            'message': f'‚úÖ {patterns_added} patterns enrichis ajout√©s ! La classification utilise maintenant mieux les descriptions.'
        })
        
    except Exception as e:
        print(f"[ENHANCE-PATTERNS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/test-enhanced-classification', methods=['POST'])
@login_required
def test_enhanced_classification():
    """Tester la classification enrichie avec titre et description"""
    try:
        data = request.get_json()
        title = data.get('title', '')
        description = data.get('description', '')
        
        if not title:
            return jsonify({
                'success': False,
                'error': 'Titre requis'
            })
        
        from yt_channel_analyzer.database import classify_video_with_language
        from yt_channel_analyzer.enhanced_patterns import get_context_score
        
        # Classification standard
        category, language, confidence = classify_video_with_language(title, description)
        
        # Score contextuel enrichi
        context_scores = {}
        for cat in ['hero', 'hub', 'help']:
            context_scores[cat] = get_context_score(description, cat, language)
        
        return jsonify({
            'success': True,
            'classification': {
                'category': category,
                'language': language,
                'confidence': confidence,
                'context_scores': context_scores,
                'title': title,
                'description': description[:200] + '...' if len(description) > 200 else description
            }
        })
        
    except Exception as e:
        print(f"[TEST-ENHANCED] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/thumbnails/repair', methods=['POST'])
@login_required
def api_repair_thumbnails():
    """API pour r√©parer les miniatures c√¥t√© client"""
    try:
        # R√©cup√©rer quelques exemples de miniatures
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT video_id, title, thumbnail_url 
            FROM video 
            WHERE thumbnail_url IS NOT NULL 
            LIMIT 10
        ''')
        
        samples = cursor.fetchall()
        conn.close()
        
        thumbnail_samples = []
        for sample in samples:
            thumbnail_samples.append({
                'video_id': sample[0],
                'title': sample[1],
                'thumbnail_url': sample[2],
                'fallback_url': f"https://i.ytimg.com/vi/{sample[0]}/mqdefault.jpg"
            })
        
        return jsonify({
            'success': True,
            'message': '√âchantillons de miniatures r√©cup√©r√©s',
            'samples': thumbnail_samples,
            'repair_instructions': [
                'V√©rifiez que les URLs sont accessibles',
                'Utilisez les fonctions JavaScript pour r√©parer automatiquement',
                'Les miniatures ont des fallbacks automatiques'
            ]
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/thumbnails/stats')
@login_required
def api_thumbnail_stats():
    """API pour obtenir les statistiques des miniatures"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Statistiques des vid√©os
        cursor.execute('''
            SELECT 
                COUNT(*) as total_videos,
                COUNT(CASE WHEN thumbnail_url IS NOT NULL AND thumbnail_url != '' THEN 1 END) as with_thumbnails,
                COUNT(CASE WHEN thumbnail_url LIKE '%ytimg.com%' THEN 1 END) as youtube_thumbnails
            FROM video
        ''')
        video_stats = cursor.fetchone()
        
        # Statistiques des concurrents
        cursor.execute('''
            SELECT 
                COUNT(*) as total_competitors,
                COUNT(CASE WHEN thumbnail_url IS NOT NULL AND thumbnail_url != '' THEN 1 END) as with_thumbnails
            FROM concurrent
        ''')
        competitor_stats = cursor.fetchone()
        
        conn.close()
        
        return jsonify({
            'success': True,
            'videos': {
                'total': video_stats[0],
                'with_thumbnails': video_stats[1],
                'youtube_thumbnails': video_stats[2],
                'coverage_percentage': round((video_stats[1] / video_stats[0]) * 100, 2) if video_stats[0] > 0 else 0
            },
            'competitors': {
                'total': competitor_stats[0],
                'with_thumbnails': competitor_stats[1],
                'coverage_percentage': round((competitor_stats[1] / competitor_stats[0]) * 100, 2) if competitor_stats[0] > 0 else 0
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ==========================================
# YOUTUBE SHORTS API ROUTES
# ==========================================

@app.route('/api/shorts/analysis/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_analysis(competitor_id):
    """API pour analyser les Shorts d'un concurrent sp√©cifique"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_vs_regular_videos
        analysis = analyze_shorts_vs_regular_videos(competitor_id)
        return jsonify({
            'success': True,
            'analysis': analysis
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/frequency/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_frequency(competitor_id):
    """API pour analyser la fr√©quence des Shorts d'un concurrent"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_frequency_patterns
        period_weeks = request.args.get('period', 12, type=int)
        frequency = analyze_shorts_frequency_patterns(competitor_id, period_weeks)
        return jsonify({
            'success': True,
            'frequency': frequency
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/report/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_report(competitor_id):
    """API pour g√©n√©rer un rapport complet des Shorts d'un concurrent"""
    try:
        from yt_channel_analyzer.database import generate_shorts_insights_report
        report = generate_shorts_insights_report(competitor_id)
        return jsonify({
            'success': True,
            'report': report
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/global-analysis', methods=['GET'])
@login_required
def api_shorts_global_analysis():
    """API pour analyser les Shorts de tous les concurrents"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_vs_regular_videos
        analysis = analyze_shorts_vs_regular_videos()
        return jsonify({
            'success': True,
            'analysis': analysis
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/thumbnail-diagnostic')
@login_required
def thumbnail_diagnostic():
    """Page de diagnostic des miniatures"""
    return render_template('thumbnail_diagnostic.html')

@app.route('/ai-training-status')
@login_required
def ai_training_status():
    """Page du tableau de bord de l'IA s√©mantique."""
    model_status = {}
    metadata_path = "ai_training_metadata.json"
    
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        model_status = {
            'status': 'specialized',
            'last_trained_at': metadata.get('last_trained_at'),
            'examples_used': metadata.get('examples_used'),
            'accuracy': metadata.get('accuracy')
        }
    else:
        model_status = {
            'status': 'generic',
            'last_trained_at': None,
            'examples_used': 0,
            'accuracy': None
        }
        
    # R√©cup√©rer les stats des donn√©es humaines
    human_stats = get_human_classifications(limit=0) # limit=0 pour ne prendre que les stats

    # Fonction pour convertir le temps ISO en format lisible (√† mettre dans un helper plus tard)
    def human_time_filter(dt_str):
        if not dt_str:
            return "N/A"
        from datetime import datetime
        dt_obj = datetime.fromisoformat(dt_str)
        return dt_obj.strftime('%d %B %Y √† %H:%M')

    app.jinja_env.filters['human_time'] = human_time_filter
        
    return render_template('ai_training_status.html', 
                           model_status=model_status, 
                           human_stats=human_stats)

@app.route('/api/retrain-model', methods=['POST'])
@login_required
def api_retrain_model():
    """API pour lancer le script d'entra√Ænement du mod√®le."""
    try:
        script_path = os.path.join(os.path.dirname(__file__), '..', 'scripts', 'train_with_human_data.py')
        
        # Lancer le script en mode non-interactif
        # On passe des arguments pour que le script ne pose pas de questions
        process = subprocess.run(
            ['python', script_path, '--non-interactive'], 
            capture_output=True, 
            text=True,
            check=True
        )
        
        # Si le script se termine bien
        return jsonify({
            'success': True,
            'message': 'Entra√Ænement termin√© avec succ√®s! La page va se rafra√Æchir.',
            'output': process.stdout
        })

    except subprocess.CalledProcessError as e:
        # Si le script retourne une erreur
        return jsonify({
            'success': False,
            'error': 'Le script d\'entra√Ænement a √©chou√©.',
            'details': e.stderr
        }), 500
    except Exception as e:
        # Pour toute autre erreur
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/calculate-total-videos-in-playlists', methods=['GET'])
@login_required
def get_total_videos_in_playlists():
    """Calcule et retourne le nombre total de vid√©os contenues dans toutes les playlists."""
    try:
        from .database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT SUM(video_count) FROM playlist")
        total_videos = cursor.fetchone()[0]
        
        conn.close()
        
        # S'assurer que le r√©sultat n'est pas None (si aucune playlist n'existe)
        total_videos = total_videos or 0
        
        return jsonify({
            'success': True,
            'total_videos_in_playlists': total_videos
        })
        
    except Exception as e:
        print(f"[API-CALCULATE-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/transformers-dashboard')
def transformers_dashboard():
    """Tableau de bord des Python transformers"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # R√©cup√©rer toutes les donn√©es du tableau de bord
        dashboard_data = transformers_manager.get_dashboard_data()
        
        return render_template('transformers_dashboard.html', 
                             dashboard_data=dashboard_data)
        
    except Exception as e:
        print(f"[TRANSFORMERS-DASHBOARD] ‚ùå Erreur: {e}")
        return render_template('error.html', 
                             error_message=f"Erreur lors du chargement du tableau de bord: {str(e)}")

@app.route('/api/transformers/status', methods=['GET'])
def api_transformers_status():
    """API pour r√©cup√©rer le statut en temps r√©el des transformers"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        dashboard_data = transformers_manager.get_dashboard_data()
        
        return jsonify({
            'success': True,
            'data': dashboard_data
        })
        
    except Exception as e:
        print(f"[API-TRANSFORMERS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/load-model', methods=['POST'])
def api_load_transformer_model():
    """API pour charger un mod√®le transformer de mani√®re asynchrone"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        data = request.get_json()
        model_name = data.get('model_name')
        
        if not model_name:
            return jsonify({
                'success': False,
                'error': 'Nom du mod√®le requis'
            })
        
        # Lancer le chargement asynchrone
        success = transformers_manager.load_model_async(model_name)
        
        if success:
            return jsonify({
                'success': True,
                'message': f'Chargement de {model_name} lanc√© en arri√®re-plan',
                'async_loading': True
            })
        else:
            return jsonify({
                'success': False,
                'error': '√âchec du lancement du chargement'
            })
        
    except Exception as e:
        print(f"[API-LOAD-MODEL] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/loading-progress/<model_name>', methods=['GET'])
@login_required
def api_transformer_loading_progress(model_name):
    """API pour r√©cup√©rer le progr√®s de chargement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        progress_data = transformers_manager.get_loading_progress(model_name)
        
        return jsonify({
            'success': True,
            'progress': progress_data
        })
        
    except Exception as e:
        print(f"[API-LOADING-PROGRESS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/cancel-loading/<model_name>', methods=['POST'])
@login_required
def api_cancel_transformer_loading(model_name):
    """API pour annuler le chargement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # R√©initialiser le statut du mod√®le
        model_info = transformers_manager.get_model_info(model_name)
        if model_info:
            model_info.loading_status = "ready"
            model_info.download_progress = 0.0
            
        return jsonify({
            'success': True,
            'message': f'Chargement de {model_name} annul√©'
        })
        
    except Exception as e:
        print(f"[API-CANCEL-LOADING] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/train', methods=['POST'])
@login_required
def api_train_transformer():
    """API pour lancer l'entra√Ænement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        data = request.get_json()
        model_name = data.get('model_name', 'all-MiniLM-L6-v2')
        
        # Lancer l'entra√Ænement
        success = transformers_manager.start_training(model_name)
        
        return jsonify({
            'success': success,
            'message': f'Entra√Ænement {"d√©marr√©" if success else "√©chou√©"}'
        })
        
    except Exception as e:
        print(f"[API-TRAIN] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/semantic/settings', methods=['GET', 'POST'])
@login_required
def api_semantic_settings():
    """API pour g√©rer les param√®tres s√©mantiques"""
    if request.method == 'GET':
        # R√©cup√©rer les param√®tres actuels
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # R√©cup√©rer tous les param√®tres s√©mantiques
            cursor.execute('''
                SELECT key, value FROM settings 
                WHERE key LIKE 'semantic_%' OR key = 'classification_mode'
            ''')
            
            settings = {row[0]: row[1] for row in cursor.fetchall()}
            conn.close()
            
            # Valeurs par d√©faut si pas encore configur√©es
            defaults = {
                'semantic_classification_enabled': 'false',
                'semantic_auto_init': 'false',
                'semantic_model': 'all-MiniLM-L6-v2',
                'semantic_weight': '0.7',
                'pattern_weight': '0.3',
                'classification_mode': 'database_only'
            }
            
            for key, default_value in defaults.items():
                if key not in settings:
                    settings[key] = default_value
            
            return jsonify({
                'success': True,
                'settings': settings
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            })
    
    elif request.method == 'POST':
        # Mettre √† jour les param√®tres
        try:
            data = request.get_json()
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Mettre √† jour chaque param√®tre
            for key, value in data.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO settings (key, value, updated_at)
                    VALUES (?, ?, ?)
                ''', (key, str(value), datetime.now()))
            
            conn.commit()
            conn.close()
            
            return jsonify({
                'success': True,
                'message': 'Param√®tres s√©mantiques mis √† jour'
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            })

@app.route('/api/semantic/propagate', methods=['POST'])
@login_required
def api_semantic_propagate():
    """API pour d√©clencher la propagation s√©mantique sur toutes les vid√©os"""
    try:
        data = request.get_json()
        force_reclassify = data.get('force_reclassify', False)
        
        # Activer temporairement la classification s√©mantique
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO settings (key, value, updated_at)
            VALUES ('semantic_classification_enabled', 'true', ?)
        ''', (datetime.now(),))
        
        conn.commit()
        conn.close()
        
        # Importer le classificateur hybride
        from yt_channel_analyzer.hybrid_classifier import get_hybrid_classifier
        
        classifier = get_hybrid_classifier()
        
        # R√©cup√©rer toutes les vid√©os non classifi√©es par l'humain
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = '''
            SELECT id, title, description, category, classification_source, human_verified
            FROM video 
            WHERE (human_verified != 1 OR human_verified IS NULL)
        '''
        
        if not force_reclassify:
            query += ' AND (category IS NULL OR category = "")'
        
        cursor.execute(query)
        videos = cursor.fetchall()
        conn.close()
        
        # Traiter les vid√©os en arri√®re-plan
        def process_videos():
            processed = 0
            updated = 0
            
            for video_id, title, description, current_category, classification_source, human_verified in videos:
                try:
                    # Classifier avec le syst√®me hybride
                    result = classifier.classify_content(
                        title=title or '',
                        description=description or '',
                        video_id=video_id
                    )
                    
                    new_category = result.get('final_category', 'hub')
                    confidence = result.get('confidence', 50)
                    
                    # Mettre √† jour la base de donn√©es
                    update_conn = get_db_connection()
                    update_cursor = update_conn.cursor()
                    
                    update_cursor.execute('''
                        UPDATE video 
                        SET category = ?, 
                            classification_source = 'semantic',
                            confidence_score = ?,
                            last_updated = ?
                        WHERE id = ?
                    ''', (new_category, confidence, datetime.now(), video_id))
                    
                    update_conn.commit()
                    update_conn.close()
                    
                    if new_category != current_category:
                        updated += 1
                    
                    processed += 1
                    
                    if processed % 10 == 0:
                        print(f"[SEMANTIC-PROPAGATE] Trait√© {processed}/{len(videos)} vid√©os")
                
                except Exception as e:
                    print(f"[SEMANTIC-PROPAGATE] Erreur vid√©o {video_id}: {e}")
                    continue
            
            print(f"[SEMANTIC-PROPAGATE] Termin√©: {processed} vid√©os trait√©es, {updated} mises √† jour")
        
        # Lancer le traitement en arri√®re-plan
        import threading
        thread = threading.Thread(target=process_videos)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'message': f'Propagation s√©mantique lanc√©e sur {len(videos)} vid√©os',
            'total_videos': len(videos),
            'background_processing': True
        })
        
    except Exception as e:
        print(f"[API-SEMANTIC-PROPAGATE] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/test', methods=['GET'])
def api_test_transformers():
    """API de test pour v√©rifier que les transformers fonctionnent"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # Test simple
        return jsonify({
            'success': True,
            'message': 'API transformers fonctionnelle',
            'models_available': list(transformers_manager.available_models.keys()),
            'system_status': transformers_manager.system_status.status
        })
        
    except Exception as e:
        print(f"[API-TEST] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitors/update-stats', methods=['POST'])
@login_required
def api_update_competitor_stats():
    """API pour mettre √† jour les statistiques pr√©calcul√©es des concurrents"""
    try:
        from yt_channel_analyzer.database.competitors import competitor_manager
        
        # Utiliser la m√©thode optimis√©e
        updated_count = competitor_manager.update_all_competitor_stats()
        
        return jsonify({
            'success': True,
            'message': f'Statistiques mises √† jour pour {updated_count} concurrents',
            'updated_count': updated_count
        })
        
    except Exception as e:
        print(f"[API-UPDATE-STATS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

if __name__ == '__main__':
    app.run(debug=True, port=8082)
