import os
import json
from datetime import datetime, timedelta
from functools import wraps

from dotenv import load_dotenv
from flask import (Flask, flash, jsonify, redirect, render_template, request,
                   session, url_for)
from flask_migrate import Migrate
from flask_session import Session
from flask_sqlalchemy import SQLAlchemy
from flask_caching import Cache
import sqlite3
import subprocess
import time
import requests
import hashlib

# Charger les variables d'environnement
load_dotenv()

# Cr√©ation de l'application Flask
app = Flask(__name__)

# --- CONFIGURATION ---
# Cl√© secr√®te pour les sessions et autres
app.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY', 'a-very-secret-key')

# Configuration de la session
app.config['SESSION_TYPE'] = 'filesystem'
Session(app)

# Configuration du cache
app.config['CACHE_TYPE'] = 'simple'  # Cache en m√©moire simple pour commencer
app.config['CACHE_DEFAULT_TIMEOUT'] = 300  # 5 minutes par d√©faut
cache = Cache(app)

# Configuration du mode d√©veloppeur
app.config['DEV_MODE'] = os.getenv('DEV_MODE', 'true').lower() == 'true'

# Filtre Jinja2 pour obtenir la miniature d'un concurrent
@app.template_filter('competitor_thumbnail')
def competitor_thumbnail_filter(competitor_id):
    """Filtre pour obtenir la miniature locale d'un concurrent"""
    from yt_channel_analyzer.utils.thumbnails import get_competitor_thumbnail
    return get_competitor_thumbnail(competitor_id)

# Charger la cl√© API YouTube depuis la base de donn√©es au d√©marrage
def load_youtube_api_key():
    """Charger la cl√© API YouTube depuis la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # V√©rifier si la table existe
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='app_config'
        """)
        
        if cursor.fetchone():
            # R√©cup√©rer la cl√© API
            cursor.execute("""
                SELECT value FROM app_config 
                WHERE key = 'youtube_api_key'
            """)
            result = cursor.fetchone()
            if result and result[0]:
                app.config['YOUTUBE_API_KEY'] = result[0]
                os.environ['YOUTUBE_API_KEY'] = result[0]
                print(f"[STARTUP] ‚úÖ Cl√© API YouTube charg√©e depuis la base de donn√©es")
                return True
        
        # Essayer de charger depuis .env
        env_key = os.getenv('YOUTUBE_API_KEY')
        if env_key:
            app.config['YOUTUBE_API_KEY'] = env_key
            print(f"[STARTUP] ‚úÖ Cl√© API YouTube charg√©e depuis .env")
            
            # Sauvegarder en base pour la prochaine fois
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS app_config (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('''
                INSERT OR REPLACE INTO app_config (key, value, updated_at)
                VALUES ('youtube_api_key', ?, datetime('now'))
            ''', (env_key,))
            conn.commit()
            print(f"[STARTUP] üíæ Cl√© API sauvegard√©e en base de donn√©es")
            
        conn.close()
        return True
    except Exception as e:
        print(f"[STARTUP] ‚ö†Ô∏è Erreur lors du chargement de la cl√© API: {e}")
        # Fallback sur .env
        env_key = os.getenv('YOUTUBE_API_KEY')
        if env_key:
            app.config['YOUTUBE_API_KEY'] = env_key
            return True
        return False

# Charger la cl√© API au d√©marrage
load_youtube_api_key()

# --- BASE DE DONN√âES ---
# Chemin vers le fichier de BDD
app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL') or 'sqlite:///instance/database.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# Initialisation de SQLAlchemy et Migrate
db = SQLAlchemy(app)
migrate = Migrate(app, db)

# --- MOD√àLES DE BASE DE DONN√âES ---
class Concurrent(db.Model):
    """Mod√®le pour les concurrents/cha√Ænes YouTube"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    channel_id = db.Column(db.String(100), unique=True, nullable=False)
    channel_url = db.Column(db.String(200), unique=True, nullable=False)
    thumbnail_url = db.Column(db.String(200))
    banner_url = db.Column(db.String(200))
    description = db.Column(db.Text)
    subscriber_count = db.Column(db.Integer)
    view_count = db.Column(db.BigInteger)
    video_count = db.Column(db.Integer)
    country = db.Column(db.String(50))
    language = db.Column(db.String(50))
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow)
    
    # Relation avec les vid√©os
    videos = db.relationship('Video', backref='concurrent', lazy=True, cascade="all, delete-orphan")

    def __repr__(self):
        return f'<Concurrent {self.name}>'

class Video(db.Model):
    """Mod√®le pour les vid√©os YouTube"""
    id = db.Column(db.Integer, primary_key=True)
    concurrent_id = db.Column(db.Integer, db.ForeignKey('concurrent.id'), nullable=False)
    
    # --- Informations de base ---
    video_id = db.Column(db.String(50), unique=True, nullable=False)
    title = db.Column(db.String(200), nullable=False)
    description = db.Column(db.Text)
    url = db.Column(db.String(200), nullable=False)
    thumbnail_url = db.Column(db.String(200))
    
    # --- Dates et dur√©e ---
    published_at = db.Column(db.DateTime)
    duration_seconds = db.Column(db.Integer)
    duration_text = db.Column(db.String(20))  # ex: "10:23"
    is_short = db.Column(db.Boolean, default=False)  # True si vid√©o ‚â§ 60 secondes
    
    # --- M√©triques objectives ---
    view_count = db.Column(db.BigInteger)
    like_count = db.Column(db.Integer)
    comment_count = db.Column(db.Integer)
    
    # --- Classification et analyse ---
    category = db.Column(db.String(50))  # 'hero', 'hub', 'help'
    tags = db.Column(db.Text)  # JSON string des tags
    
    # --- Crit√®res subjectifs (scores sur 10) ---
    beauty_score = db.Column(db.Integer)  # 1-10
    emotion_score = db.Column(db.Integer)  # 1-10
    info_quality_score = db.Column(db.Integer)  # 1-10
    
    # --- M√©tadonn√©es ---
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_updated = db.Column(db.DateTime, default=datetime.utcnow)
    
    def __repr__(self):
        return f'<Video {self.title}>'

# --- IMPORTS DES MODULES ---
from yt_channel_analyzer.storage import load_data
from yt_channel_analyzer.analysis import hero_hub_help_matrix
from yt_channel_analyzer.scraper import autocomplete_youtube_channels
from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api, autocomplete_youtube_channels_api
from yt_channel_analyzer.youtube_api_client import get_api_quota_status
from yt_channel_analyzer.auth import verify_credentials, is_authenticated, authenticate_session, logout_session

# D√©corateur de connexion requis
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not is_authenticated(session):
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def dev_mode_required(f):
    """D√©corateur pour prot√©ger les routes en mode d√©veloppeur"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not is_authenticated(session):
            return redirect(url_for('login'))
        if not session.get('dev_mode', app.config.get('DEV_MODE', True)):
            flash('Acc√®s refus√©. Mode d√©veloppeur requis.', 'error')
            return redirect(url_for('home'))
        return f(*args, **kwargs)
    return decorated_function

# Import des fonctions de base de donn√©es
from yt_channel_analyzer.database import competitors_to_legacy_format, get_all_competitors, save_competitor_and_videos, get_db_connection, get_classification_patterns

# Filtre Jinja pour formater les nombres
@app.template_filter('format_number')
def format_number(value):
    """Format numbers with K/M/B notation to avoid comma confusion"""
    try:
        num = float(value)
        if num >= 1000000000:
            return f"{num/1000000000:.1f}B"
        elif num >= 1000000:
            return f"{num/1000000:.1f}M"
        elif num >= 1000:
            return f"{num/1000:.1f}K"
        else:
            return f"{num:.0f}"
    except (ValueError, TypeError):
        return str(value)

# Contexte global pour les templates
@app.context_processor
def inject_dev_mode():
    """Injecte le mode d√©veloppeur dans tous les templates"""
    try:
        # Initialiser dev_mode s'il n'existe pas
        if 'dev_mode' not in session:
            session['dev_mode'] = app.config.get('DEV_MODE', True)
            print(f"[CONTEXT] Initializing dev_mode to: {session['dev_mode']}")
        
        dev_mode_value = session.get('dev_mode', True)
        is_auth = is_authenticated(session)
        
        print(f"[CONTEXT] dev_mode: {dev_mode_value}, is_authenticated: {is_auth}, session_keys: {list(session.keys())}")
        
        return dict(
            dev_mode=dev_mode_value,
            is_authenticated=is_auth
        )
    except Exception as e:
        print(f"[CONTEXT] Erreur inject_dev_mode: {e}")
        import traceback
        traceback.print_exc()
        return dict(
            dev_mode=True,
            is_authenticated=False
        )

# Import des modules ViewStats supprim√©s - analyse locale maintenant

def generate_what_works_insights(cursor, countries_data):
    """
    G√©n√®re 5 insights cl√©s simples et utiles pour chaque pays
    """
    countries_insights = {}
    
    for country_name, country_data in countries_data.items():
        insights = {
            'key_metrics': []
        }
        
        try:
            print(f"[INSIGHTS] Analyzing country: '{country_name}'")
            
            # R√©cup√©rer les vid√©os top pour ce pays
            cursor.execute("""
                SELECT 
                    v.title,
                    COALESCE(v.view_count, 0) as view_count,
                    COALESCE(v.like_count, 0) as like_count,
                    COALESCE(v.comment_count, 0) as comment_count,
                    COALESCE(v.duration_seconds, 0) as duration_seconds,
                    COALESCE(v.is_short, 0) as is_short,
                    v.published_at,
                    c.name as channel_name
                FROM video v
                JOIN concurrent c ON v.concurrent_id = c.id
                WHERE c.country = ?
                AND v.view_count > 0
                ORDER BY v.view_count DESC
                LIMIT 20
            """, (country_name,))
            
            top_videos = cursor.fetchall()
            print(f"[INSIGHTS] {country_name}: Found {len(top_videos)} top videos")
            
            # Debug: check if we have any videos for this country at all
            if len(top_videos) == 0:
                cursor.execute("""
                    SELECT COUNT(*) FROM video v
                    JOIN concurrent c ON v.concurrent_id = c.id
                    WHERE c.country = ?
                """, (country_name,))
                total_videos_for_country = cursor.fetchone()[0]
                print(f"[INSIGHTS] {country_name}: Total videos in DB: {total_videos_for_country}")
                
                # Check available countries
                cursor.execute("SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL")
                available_countries = [row[0] for row in cursor.fetchall()]
                print(f"[INSIGHTS] Available countries: {available_countries}")
            
            if top_videos and len(top_videos) > 0:
                # 1. Shorts vs Long Form Performance (check this first)
                shorts_count = sum(1 for v in top_videos if v[5])
                long_form_count = len(top_videos) - shorts_count
                
                if shorts_count > long_form_count:
                    insights['key_metrics'].append(f"üì± Shorts dominate: {shorts_count}/{len(top_videos)} top videos are Shorts")
                    # If shorts dominate, calculate average SHORT duration
                    shorts_videos = [v for v in top_videos if v[5] and v[4] and v[4] > 0]
                    if shorts_videos:
                        avg_duration = sum(v[4] for v in shorts_videos) / len(shorts_videos)
                        insights['key_metrics'].append(f"‚è±Ô∏è Optimal duration: {avg_duration:.0f} seconds (Shorts)")
                else:
                    insights['key_metrics'].append(f"üì∫ Long-form wins: {long_form_count}/{len(top_videos)} top videos are long-form")
                    # If long-form wins, calculate average LONG-FORM duration
                    long_form_videos = [v for v in top_videos if not v[5] and v[4] and v[4] > 0]
                    if long_form_videos:
                        avg_duration = sum(v[4] for v in long_form_videos) / len(long_form_videos)
                        optimal_duration = avg_duration / 60
                        insights['key_metrics'].append(f"‚è±Ô∏è Optimal duration: {optimal_duration:.1f} minutes (Long-form)")
                
                # 3. Most Engaging Theme (highest engagement video)
                best_engagement_video = None
                best_engagement_rate = 0
                
                for v in top_videos:
                    if v[1] and v[1] > 0 and v[2] and v[3]:  # views, likes, comments
                        engagement_rate = ((v[2] + v[3]) / v[1]) * 100
                        if engagement_rate > best_engagement_rate:
                            best_engagement_rate = engagement_rate
                            best_engagement_video = v
                
                if best_engagement_video:
                    title = best_engagement_video[0][:40] + "..." if len(best_engagement_video[0]) > 40 else best_engagement_video[0]
                    insights['key_metrics'].append(f"üéØ Most engaging theme: \"{title}\" ({best_engagement_rate:.1f}% engagement)")
                
                # 4. Average Engagement Rate
                total_views = sum(v[1] for v in top_videos if v[1])
                total_likes = sum(v[2] for v in top_videos if v[2])
                total_comments = sum(v[3] for v in top_videos if v[3])
                
                if total_views > 0:
                    avg_engagement = ((total_likes + total_comments) / total_views) * 100
                    insights['key_metrics'].append(f"üíØ Average engagement: {avg_engagement:.2f}%")
                
                # 5. Comment vs Like Preference
                if total_comments > 0 and total_likes > 0:
                    comment_ratio = (total_comments / (total_comments + total_likes)) * 100
                    if comment_ratio > 15:  # Comments are significant
                        insights['key_metrics'].append(f"üí¨ High discussion: {comment_ratio:.0f}% comments vs likes")
                    else:
                        insights['key_metrics'].append(f"üëç Like-focused: {100-comment_ratio:.0f}% likes vs comments")
                
                # 6. Top 3 Topics Analysis
                top_topics = analyze_top_topics(cursor, country_name, top_videos)
                if top_topics:
                    insights['key_metrics'].append(f"üé§ Top topics: {', '.join(top_topics)}")
                
                # 7. Growth-driving topics
                growth_topics = analyze_growth_driving_topics(cursor, country_name)
                if growth_topics:
                    insights['key_metrics'].append(f"üìà Growth drivers: {', '.join(growth_topics)}")
            else:
                insights['key_metrics'].append("üìä Insufficient video data for analysis")
        
        except Exception as e:
            print(f"[INSIGHTS] Erreur analyse pour {country_name}: {e}")
            import traceback
            traceback.print_exc()
            insights['engagement_leaders'].append(f"‚ö†Ô∏è Analysis error: {str(e)[:100]}")
            # Provide basic fallback insights
            insights['content_patterns'].append("üìä Limited data available for pattern analysis")
            insights['format_preferences'].append("üì± Analysis requires more video data")
        
        countries_insights[country_name] = insights
    
    return countries_insights

def analyze_top_topics(cursor, country_name, top_videos):
    """Analyze the top 3 topics/themes from video titles"""
    if not top_videos:
        return []
    
    # Extract keywords from titles
    import re
    from collections import Counter
    
    # Common stop words to ignore
    stop_words = {'de', 'het', 'een', 'en', 'van', 'voor', 'in', 'op', 'met', 'aan', 'bij', 'la', 'le', 'les', 'et', 'de', 'du', 'des', 'pour', 'dans', 'avec', 'sur', 'the', 'and', 'or', 'of', 'to', 'in', 'on', 'at', 'by', 'for', 'with', 'from'}
    
    all_words = []
    for video in top_videos:
        title = video[0] if video[0] else ''
        # Clean and split title
        words = re.findall(r'\b\w+\b', title.lower())
        # Filter out stop words and short words
        filtered_words = [word for word in words if len(word) > 2 and word not in stop_words]
        all_words.extend(filtered_words)
    
    # Count word frequency
    word_counts = Counter(all_words)
    
    # Get top 3 most common words
    top_3_words = [word for word, count in word_counts.most_common(3)]
    
    return top_3_words

def analyze_growth_driving_topics(cursor, country_name):
    """Analyze which topics drive subscriber growth"""
    try:
        # Get videos with their publication dates and growth correlation
        cursor.execute("""
            SELECT 
                v.title,
                v.youtube_published_at,
                v.view_count,
                c.id as concurrent_id
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE c.country = ?
            AND v.youtube_published_at IS NOT NULL
            AND v.view_count > 10000
            ORDER BY v.view_count DESC
            LIMIT 10
        """, (country_name,))
        
        growth_videos = cursor.fetchall()
        
        if not growth_videos:
            return []
        
        # For each video, try to correlate with subscriber growth
        growth_topics = []
        for video in growth_videos:
            title = video[0]
            pub_date = video[1]
            concurrent_id = video[3]
            
            # Extract month from publication date
            if pub_date:
                pub_month = pub_date[:7]  # YYYY-MM format
                
                # Check subscriber growth for that month
                cursor.execute("""
                    SELECT 
                        s1.subscriber_count,
                        s2.subscriber_count,
                        (s2.subscriber_count - s1.subscriber_count) as growth
                    FROM subscriber_data s1
                    JOIN subscriber_data s2 ON s1.concurrent_id = s2.concurrent_id
                    WHERE s1.concurrent_id = ?
                    AND s1.date LIKE ?
                    AND s2.date > s1.date
                    ORDER BY s2.date
                    LIMIT 1
                """, (concurrent_id, pub_month + '%'))
                
                growth_data = cursor.fetchone()
                if growth_data and growth_data[2] > 0:  # Positive growth
                    # Extract key topic from title
                    import re
                    words = re.findall(r'\b\w+\b', title.lower())
                    # Get meaningful words (not stop words)
                    stop_words = {'de', 'het', 'een', 'en', 'van', 'voor', 'in', 'op', 'met', 'aan', 'bij', 'la', 'le', 'les', 'et', 'de', 'du', 'des', 'pour', 'dans', 'avec', 'sur', 'the', 'and', 'or', 'of', 'to', 'in', 'on', 'at', 'by', 'for', 'with', 'from'}
                    meaningful_words = [word for word in words if len(word) > 3 and word not in stop_words]
                    
                    if meaningful_words:
                        # Take the first meaningful word as the topic
                        topic = meaningful_words[0]
                        if topic not in growth_topics:
                            growth_topics.append(topic)
        
        return growth_topics[:3]  # Return top 3 growth-driving topics
        
    except Exception as e:
        print(f"[GROWTH] Error analyzing growth topics for {country_name}: {e}")
        return []

# Fonctions utilitaires pour la compatibilit√© avec l'ancien format JSON
def load_cache():
    """Charger les concurrents depuis la base de donn√©es (format legacy)"""
    try:
        return competitors_to_legacy_format()
    except Exception as e:
        print(f"Erreur lors du chargement depuis la base: {e}")
        return {}

def save_cache(data):
    """Sauvegarder dans la base de donn√©es (pour compatibilit√©)"""
    # Cette fonction est conserv√©e pour compatibilit√© mais n'est plus utilis√©e
    # Les donn√©es sont maintenant directement sauv√©es en base via les fonctions database.py
    print("‚ö†Ô∏è save_cache() est deprecated - utilisez les fonctions database.py")

def get_channel_key(channel_url):
    """G√©n√©rer une cl√© unique pour une cha√Æne"""
    return channel_url.replace('/', '_').replace(':', '_').replace('?', '_').replace('&', '_')

def extract_channel_name(channel_url):
    """Extraire le nom de la cha√Æne depuis l'URL"""
    try:
        parts = channel_url.split('/')
        if 'channel/' in channel_url:
            return parts[-1][:20]
        elif '@' in channel_url:
            return parts[-1][:20]
        else:
            return "Canal"[:20]
    except:
        return "Canal"

def calculate_total_views(videos):
    """Calculer le total des vues pour une liste de vid√©os"""
    total = 0
    for video in videos:
        views_str = str(video.get('views', '0')).replace(',', '').replace(' ', '')
        try:
            views_num = int(views_str) if views_str.isdigit() else 0
            total += views_num
        except:
            continue
    return total

def format_number(num):
    """Formater un nombre avec des s√©parateurs de milliers"""
    try:
        return f"{int(num):,}".replace(',', ' ')
    except:
        return str(num)

# Enregistrer le filtre format_number pour les templates Jinja2
app.jinja_env.filters['format_number'] = format_number

def save_competitor_data(channel_url, videos):
    """Sauvegarder/enrichir intelligemment les donn√©es d'un concurrent"""
    try:
        from yt_channel_analyzer.database import refresh_competitor_data
        from yt_channel_analyzer.youtube_api_client import create_youtube_client
        
        print(f"[SAVE] üíæ Sauvegarde intelligente de {len(videos)} vid√©os pour {channel_url}")
        
        # R√©cup√©rer les infos de la cha√Æne pour enrichir
        channel_info = None
        try:
            youtube_client = create_youtube_client()
            channel_info = youtube_client.get_channel_info(channel_url)
            print(f"[SAVE] üìä Infos cha√Æne: {channel_info.get('title', 'N/A')}")
        except Exception as e:
            print(f"[SAVE] ‚ö†Ô∏è Impossible de r√©cup√©rer les infos cha√Æne: {e}")
        
        # Utiliser le refresh intelligent (cr√©ation + enrichissement automatique)
        result = refresh_competitor_data(channel_url, videos, channel_info)
        
        if result['success']:
            action = result['action']
            competitor_name = result.get('competitor_name', 'Concurrent')
            competitor_id = result['competitor_id']
            
            if action == 'created':
                print(f"[SAVE] ‚úÖ NOUVEAU concurrent cr√©√©: {competitor_name} (ID: {competitor_id})")
            else:  # refreshed
                print(f"[SAVE] üîÑ ENRICHISSEMENT: {competitor_name} (ID: {competitor_id})")
                print(f"[SAVE] üìà Nouvelles: {result['new_videos']}, "
                      f"Enrichies: {result['enriched_videos']}, "
                      f"Total: {result['total_videos']}")
            
            return competitor_id
        else:
            raise Exception(result.get('error', 'Erreur inconnue lors de la sauvegarde'))
        
    except Exception as e:
        print(f"[SAVE] ‚ùå Erreur critique: {e}")
        raise

# --- ROUTES DE L'APPLICATION ---

@app.route('/sneat-test')
@login_required
def sneat_test():
    """Route de test pour le template Sneat exact"""
    return render_template('home_sneat_exact.html')

@app.route('/sneat-clean')
@login_required
def sneat_clean():
    """Redirection vers la page des t√¢ches"""
    return redirect(url_for('tasks_page'))

@app.route('/')
@login_required
def home():
    from yt_channel_analyzer.database import get_db_connection
    import sqlite3
    
    # Initialiser le mode d√©veloppeur si pas d√©j√† fait
    if 'dev_mode' not in session:
        session['dev_mode'] = app.config.get('DEV_MODE', True)
        session.permanent = True
        session.modified = True
    
    try:
        # Utiliser des requ√™tes SQL directes pour calculer les statistiques
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Calculer le nombre de concurrents
        cursor.execute('SELECT COUNT(*) FROM concurrent')
        total_competitors = cursor.fetchone()[0]
        
        # Calculer le nombre total de vid√©os
        cursor.execute('SELECT COUNT(*) FROM video')
        total_videos = cursor.fetchone()[0]
        
        # Calculer le total des vues
        cursor.execute('SELECT SUM(view_count) FROM video WHERE view_count IS NOT NULL')
        total_views = cursor.fetchone()[0] or 0
        
        # Calculer le nombre de playlists
        cursor.execute('SELECT COUNT(*) FROM playlist')
        total_playlists = cursor.fetchone()[0]
        
        # Calculer le nombre de pays distincts
        cursor.execute('SELECT COUNT(DISTINCT country) FROM concurrent WHERE country IS NOT NULL AND country != ""')
        total_countries = cursor.fetchone()[0]
        
        conn.close()
        
        # Formater les vues avec des abr√©viations
        def format_views(views):
            if views >= 1000000:
                return f"{views/1000000:.1f}M"
            elif views >= 1000:
                return f"{views/1000:.1f}K"
            else:
                return str(views)
        
        stats = {
            'total_competitors': total_competitors,
            'total_videos': total_videos,
            'total_views': total_views,
            'total_views_formatted': format_views(total_views),
            'total_playlists': total_playlists,
            'countries': total_countries
        }
        
        print(f"[HOME] üìä Statistiques calcul√©es: {total_competitors} concurrents, {total_videos} vid√©os, {total_views:,} vues, {total_playlists} playlists")
        
    except Exception as e:
        print(f"[HOME] ‚ùå Erreur lors du calcul des statistiques: {e}")
        import traceback
        traceback.print_exc()
        
        # Valeurs par d√©faut en cas d'erreur
        stats = {
            'total_competitors': 0,
            'total_videos': 0,
            'total_views': 0,
            'total_views_formatted': '0',
            'total_playlists': 0,
            'countries': 0
        }
    
    return render_template('sneat_base_clean.html', stats=stats, dev_mode=session.get('dev_mode', False))

@app.route('/home_old')
@login_required
def home_old():
    return render_template('home.html')

@app.route('/dashboard-glass')
@login_required
def dashboard_glass():
    """Page de d√©monstration du syst√®me glassmorphism avanc√©"""
    return render_template('dashboard_glass.html')

@app.route('/concurrents')
@login_required
def concurrents():
    """
    Afficher la liste de tous les concurrents avec leurs statistiques
    """
    from yt_channel_analyzer.database.competitors import get_all_competitors_with_videos
    from yt_channel_analyzer.database import calculate_publication_frequency, update_competitor_tags
    
    # Initialiser le mode d√©veloppeur si pas d√©j√† fait
    if 'dev_mode' not in session:
        session['dev_mode'] = app.config.get('DEV_MODE', True)
        session.permanent = True
        session.modified = True
    
    # R√©cup√©rer tous les concurrents avec leurs vid√©os (d√©j√† pr√©calcul√©)
    competitors = get_all_competitors_with_videos()
    
    # NEW: ensure local cached thumbnails
    thumbnails_dir = os.path.join('static', 'competitors', 'images')
    os.makedirs(thumbnails_dir, exist_ok=True)
    
    for competitor in competitors:
        thumb_local_rel = None
        comp_id = competitor.get('id')
        if comp_id is None:
            continue
        local_filename = f"{comp_id}.jpg"
        local_path = os.path.join(thumbnails_dir, local_filename)
        # If already downloaded, just point to it
        if os.path.exists(local_path):
            thumb_local_rel = f"/static/competitors/images/{local_filename}"
        else:
            remote_url = competitor.get('thumbnail_url')
            if remote_url:
                try:
                    resp = requests.get(remote_url, timeout=5)
                    if resp.status_code == 200 and resp.content:
                        with open(local_path, 'wb') as img_f:
                            img_f.write(resp.content)
                        thumb_local_rel = f"/static/competitors/images/{local_filename}"
                except Exception as download_err:
                    print(f"[THUMBNAIL] Failed to download for competitor {comp_id}: {download_err}")
        competitor['thumbnail_local'] = thumb_local_rel
    
    # Calculer les fr√©quences de publication
    # frequency_data = calculate_publication_frequency()
    
    # NOUVEAU : R√©cup√©rer directement depuis la table competitor_frequency_stats
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        SELECT competitor_id, avg_videos_per_week, frequency_hero, frequency_hub, frequency_help
        FROM competitor_frequency_stats
    ''')
    frequency_results = cursor.fetchall()
    conn.close()
    
    # Cr√©er un dictionnaire pour acc√®s rapide
    frequency_data = {}
    for row in frequency_results:
        competitor_id, avg_per_week, freq_hero, freq_hub, freq_help = row
        frequency_data[competitor_id] = {
            'avg_videos_per_week': avg_per_week,
            'hero_frequency': freq_hero,
            'hub_frequency': freq_hub,
            'help_frequency': freq_help
        }
    
    # Enrichir les donn√©es des concurrents avec la fr√©quence (SANS recalculer les stats)
    for competitor in competitors:
        competitor_id = competitor['id']
        frequency_competitor_data = frequency_data.get(competitor_id, {})
        
        # Ajouter les donn√©es de fr√©quence au dictionnaire du concurrent
        competitor.update({
            'frequency_total': frequency_competitor_data.get('avg_videos_per_week', 0),
            'frequency_hero': frequency_competitor_data.get('hero_frequency', 0),
            'frequency_hub': frequency_competitor_data.get('hub_frequency', 0),
            'frequency_help': frequency_competitor_data.get('help_frequency', 0)
        })

    # Trier les concurrents par nombre de vid√©os (d√©j√† fait dans la requ√™te SQL, mais double-s√©curit√©)
    competitors.sort(key=lambda x: x.get('video_count', 0), reverse=True)
    
    # Get unique countries for filter
    countries = sorted(list(set(comp.get('country') for comp in competitors if comp.get('country'))))
    
    return render_template(
        'concurrents_sneat_clean.html', 
        competitors=competitors,
        countries=countries,
        dev_mode=session.get('dev_mode', False)
    )

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')
        
        if verify_credentials(username, password):
            authenticate_session(session)
            return redirect(url_for('home'))
        else:
            return render_template('sneat/auth/login.html', error='Identifiants incorrects', demo_mode=app.config.get('DEMO_MODE', False))
    
    return render_template('sneat/auth/login.html', demo_mode=app.config.get('DEMO_MODE', False))

@app.route('/logout')
def logout():
    logout_session(session)
    return redirect(url_for('login'))

@app.route('/toggle-dev-mode', methods=['POST'])
@login_required
def toggle_dev_mode():
    """Basculer le mode d√©veloppeur"""
    current_mode = session.get('dev_mode', app.config.get('DEV_MODE', True))
    new_mode = not current_mode
    session['dev_mode'] = new_mode
    session.permanent = True  # Forcer la persistence
    session.modified = True   # Marquer la session comme modifi√©e
    
    print(f"[TOGGLE-DEV-MODE] Mode d√©veloppeur: {current_mode} -> {new_mode}")
    
    status = "activ√©" if new_mode else "d√©sactiv√©"
    flash(f'Mode d√©veloppeur {status}', 'success')
    
    return redirect(request.referrer or url_for('home'))

@app.route('/toggle-performance-mode', methods=['POST'])
@login_required
def toggle_performance_mode():
    """Activer/d√©sactiver le mode performance"""
    session['performance_mode'] = not session.get('performance_mode', True)
    flash(f"Mode performance {'activ√©' if session['performance_mode'] else 'd√©sactiv√©'}", 'info')
    return redirect(request.referrer or url_for('home'))

# ===== ROUTES PERFORMANCE ET OPTIMISATION =====

@app.route('/performance-dashboard')
@login_required
@dev_mode_required
def performance_dashboard():
    """Dashboard de performance pour monitoring en temps r√©el"""
    import sys
    return render_template('sneat/admin/performance.html', 
                         python_version=f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")

@app.route('/api/performance-metrics')
@login_required
def get_performance_metrics():
    """API pour r√©cup√©rer les m√©triques de performance"""
    try:
        from yt_channel_analyzer.performance_manager import get_optimization_status
        from yt_channel_analyzer.cache_manager import get_cache_stats
        
        optimization_status = get_optimization_status()
        cache_stats = get_cache_stats()
        
        return jsonify({
            **optimization_status,
            'redis_stats': cache_stats
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/performance-actions/clear-cache', methods=['POST'])
@login_required
@dev_mode_required
def clear_cache_action():
    """Action pour vider le cache"""
    try:
        from yt_channel_analyzer.cache_manager import clear_all_cache
        cleared_keys = clear_all_cache()
        return jsonify({
            'success': True,
            'cleared_keys': cleared_keys,
            'message': f'Cache vid√©: {cleared_keys} cl√©s supprim√©es'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/performance-actions/optimize-database', methods=['POST'])
@login_required
@dev_mode_required
def optimize_database_action():
    """Action pour optimiser la base de donn√©es"""
    try:
        from yt_channel_analyzer.performance_manager import thread_pool_manager
        
        # Lancer l'optimisation en arri√®re-plan
        task_id = thread_pool_manager.submit_task(
            _run_database_optimization,
            task_id="db_optimization"
        )
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'Optimisation de la base de donn√©es lanc√©e'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/performance-actions/preload-caches', methods=['POST'])
@login_required
@dev_mode_required
def preload_caches_action():
    """Action pour pr√©charger les caches"""
    try:
        from yt_channel_analyzer.performance_manager import _preload_critical_caches
        _preload_critical_caches()
        return jsonify({
            'success': True,
            'message': 'Pr√©chargement des caches lanc√©'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/performance-metrics/export')
@login_required
@dev_mode_required
def export_performance_metrics():
    """Exporte les m√©triques de performance"""
    try:
        from yt_channel_analyzer.performance_manager import performance_monitor
        import json
        from datetime import datetime
        
        metrics_data = {
            'export_timestamp': datetime.now().isoformat(),
            'metrics_history': [m.__dict__ for m in performance_monitor.metrics_history],
            'current_status': get_optimization_status()
        }
        
        response = make_response(json.dumps(metrics_data, indent=2, default=str))
        response.headers['Content-Type'] = 'application/json'
        response.headers['Content-Disposition'] = f'attachment; filename=performance_metrics_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        return response
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def _run_database_optimization():
    """Fonction d'optimisation de la base de donn√©es"""
    try:
        # R√©ex√©cuter le script d'optimisation
        import subprocess
        import os
        
        script_path = os.path.join(os.getcwd(), 'optimize_database.py')
        result = subprocess.run(['python', script_path], 
                              capture_output=True, 
                              text=True,
                              cwd=os.getcwd())
        
        return {
            'success': result.returncode == 0,
            'output': result.stdout,
            'error': result.stderr if result.returncode != 0 else None
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}

@app.route('/business', methods=['GET', 'POST'])
@login_required
def business():
    """Page des param√®tres m√©tiers (seuil paid/organic, etc.)"""
    if request.method == 'POST':
        # Sauvegarder les param√®tres m√©tiers
        settings_data = {
            'paid_threshold': int(request.form.get('paid_threshold', 10000)),
            'industry': request.form.get('industry', 'tourism'),
            'auto_classify': 'auto_classify' in request.form,
            'max_videos': int(request.form.get('max_videos', 1000)),
            'cache_duration': int(request.form.get('cache_duration', 24))
        }
        
        save_settings(settings_data)
        print(f"[BUSINESS] Param√®tres m√©tiers sauvegard√©s: {settings_data}")
        
        return render_template('business_sneat_clean.html', 
                             current_settings=settings_data,
                             message="Param√®tres m√©tiers sauvegard√©s avec succ√®s !")
    
    current_settings = load_settings()
    
    # R√©cup√©rer les statistiques business (paid vs organic)
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Statistiques organiques
        cursor.execute("""
            SELECT COUNT(*) as count,
                   AVG(CASE WHEN like_count > 0 AND view_count > 0 
                       THEN (like_count + comment_count) * 100.0 / view_count 
                       ELSE 0 END) as engagement
            FROM video 
            WHERE organic_status = 'organic'
        """)
        organic_result = cursor.fetchone()
        organic_stats = {
            'count': organic_result[0] if organic_result else 0,
            'engagement': round(organic_result[1], 2) if organic_result and organic_result[1] else 0
        }
        
        # Statistiques payantes  
        cursor.execute("""
            SELECT COUNT(*) as count,
                   AVG(CASE WHEN like_count > 0 AND view_count > 0 
                       THEN (like_count + comment_count) * 100.0 / view_count 
                       ELSE 0 END) as engagement
            FROM video 
            WHERE organic_status = 'paid'
        """)
        paid_result = cursor.fetchone()
        paid_stats = {
            'count': paid_result[0] if paid_result else 0,
            'engagement': round(paid_result[1], 2) if paid_result and paid_result[1] else 0
        }
        
        conn.close()
        
        business_stats = {
            'organic': organic_stats,
            'paid': paid_stats,
            'threshold': current_settings.get('paid_threshold', 10000)
        }
        
    except Exception as e:
        print(f"[ERROR] Erreur lors de la r√©cup√©ration des stats business: {e}")
        business_stats = {
            'organic': {'count': 0, 'engagement': 0},
            'paid': {'count': 0, 'engagement': 0},
            'threshold': current_settings.get('paid_threshold', 10000)
        }
    
    return render_template('business_sneat_clean.html', 
                         current_settings=current_settings,
                         business_stats=business_stats)

@app.route('/api/recalculate-organic-status', methods=['POST'])
@login_required
def api_recalculate_organic_status():
    """API pour recalculer les statuts organic/paid selon le nouveau seuil"""
    try:
        data = request.get_json()
        new_threshold = data.get('threshold', 10000)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Recalculer les statuts selon le nouveau seuil
        cursor.execute("""
            UPDATE video 
            SET organic_status = CASE 
                WHEN view_count > ? THEN 'paid'
                ELSE 'organic'
            END
        """, (new_threshold,))
        
        updated_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        print(f"[BUSINESS] Statuts organic/paid recalcul√©s pour {updated_count} vid√©os avec seuil {new_threshold}")
        
        return jsonify({
            'success': True, 
            'updated_count': updated_count,
            'threshold': new_threshold
        })
        
    except Exception as e:
        print(f"[ERROR] Erreur lors du recalcul des statuts: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/settings', methods=['GET', 'POST'])
@dev_mode_required
def settings():
    """Page des param√®tres (r√®gles m√©tiers uniquement)"""
    if request.method == 'POST':
        # Sauvegarder la cl√© API YouTube si fournie
        youtube_api_key = request.form.get('youtube_api_key', '').strip()
        if youtube_api_key:
            # Sauvegarder dans la base de donn√©es
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Cr√©er la table si elle n'existe pas
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS app_config (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ins√©rer ou mettre √† jour la cl√©
            cursor.execute('''
                INSERT OR REPLACE INTO app_config (key, value, updated_at)
                VALUES ('youtube_api_key', ?, datetime('now'))
            ''', (youtube_api_key,))
            
            conn.commit()
            conn.close()
            
            # Mettre √† jour la config de l'app
            app.config['YOUTUBE_API_KEY'] = youtube_api_key
            os.environ['YOUTUBE_API_KEY'] = youtube_api_key
            
            print(f"[SETTINGS] Cl√© API YouTube sauvegard√©e en base de donn√©es")
            flash('Cl√© API YouTube sauvegard√©e avec succ√®s !', 'success')
        
        # Suite du code existant
        # Sauvegarder les param√®tres
        settings_data = {
            'paid_threshold': int(request.form.get('paid_threshold', 10000)),
            'industry': request.form.get('industry', 'tourism'),
            'auto_classify': 'auto_classify' in request.form,
            'max_videos': int(request.form.get('max_videos', 1000)),
            'cache_duration': int(request.form.get('cache_duration', 24))
        }
        
        save_settings(settings_data)
        print(f"[SETTINGS] Param√®tres sauvegard√©s: {settings_data}")
        
        return render_template('settings_sneat_clean.html', 
                             current_settings=settings_data,
                             message="Param√®tres sauvegard√©s avec succ√®s !")
    
    current_settings = load_settings()
    
    # R√©cup√©rer les patterns de classification multilingues
    patterns = get_classification_patterns()
    hero_patterns = patterns.get('hero', [])
    hub_patterns = patterns.get('hub', [])
    help_patterns = patterns.get('help', [])
    
    # R√©cup√©rer les statistiques business (paid vs organic)
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©rer le seuil paid_threshold
        paid_threshold = current_settings.get('paid_threshold', 10000)
        
        # Statistiques organiques (< seuil)
        cursor.execute("""
            SELECT COUNT(*) as count,
                   AVG(CASE WHEN likes > 0 AND views > 0 
                       THEN (likes + comments) * 100.0 / views 
                       ELSE 0 END) as engagement
            FROM video 
            WHERE view_count < ? AND view_count > 0
        """, (paid_threshold,))
        organic_result = cursor.fetchone()
        organic_stats = {
            'count': organic_result[0] if organic_result else 0,
            'engagement': organic_result[1] if organic_result and organic_result[1] else 0
        }
        
        # Statistiques payantes (>= seuil)
        cursor.execute("""
            SELECT COUNT(*) as count,
                   AVG(CASE WHEN likes > 0 AND views > 0 
                       THEN (likes + comments) * 100.0 / views 
                       ELSE 0 END) as engagement
            FROM video 
            WHERE view_count >= ?
        """, (paid_threshold,))
        paid_result = cursor.fetchone()
        paid_stats = {
            'count': paid_result[0] if paid_result else 0,
            'engagement': paid_result[1] if paid_result and paid_result[1] else 0
        }
        
        # Pas de mots-cl√©s, c'est bas√© sur le seuil de vues
        paid_keywords = []
        
        conn.close()
        
    except Exception as e:
        print(f"[ERROR] Erreur lors de la r√©cup√©ration des stats business: {e}")
        organic_stats = {'count': 0, 'engagement': 0}
        paid_stats = {'count': 0, 'engagement': 0}
        paid_keywords = []
    
    # Informations cron job (simulation)
    last_cron_run = "2025-01-19 14:30"

    # R√©cup√©rer la cl√© API actuelle (masqu√©e pour la s√©curit√©)
    current_api_key = app.config.get('YOUTUBE_API_KEY', '')
    if current_api_key:
        # Masquer la cl√© en ne montrant que les premiers et derniers caract√®res
        masked_key = f"{current_api_key[:8]}...{current_api_key[-4:]}" if len(current_api_key) > 12 else "Configur√©e"
    else:
        masked_key = "Non configur√©e"
    
    return render_template('settings_sneat_clean.html', 
                         current_settings=current_settings,
                         current_api_key=masked_key,
                         hero_patterns=hero_patterns,
                         hub_patterns=hub_patterns,
                         help_patterns=help_patterns,
                         organic_stats=organic_stats,
                         paid_stats=paid_stats,
                         paid_keywords=paid_keywords,
                         last_cron_run=last_cron_run)

def load_settings():
    """Charger les param√®tres depuis le fichier"""
    settings_file = 'settings.json'
    try:
        with open(settings_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # Param√®tres par d√©faut
        return {
            'paid_threshold': 10000,
            'industry': 'tourism',
            'auto_classify': True,
            'max_videos': 1000,
            'cache_duration': 24
        }

def save_settings(settings_data):
    """Sauvegarder les param√®tres dans le fichier"""
    settings_file = 'settings.json'
    try:
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Erreur lors de la sauvegarde des param√®tres: {e}")

@app.route('/api/tasks/status', methods=['GET'])
@login_required  
def get_tasks_status():
    """R√©cup√®re le statut de toutes les t√¢ches pour l'actualisation AJAX"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        from yt_channel_analyzer.database import get_db_connection
        
        tasks = task_manager.get_all_tasks_with_warnings()
        
        # üîÑ AUTO-CONSOLIDATION DES CONCURRENTS MANQUANTS
        competitors_added = 0
        competitors_updated = 0
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©rer les URLs existantes en base
        cursor.execute("SELECT channel_url FROM concurrent")
        existing_urls = {row[0] for row in cursor.fetchall()}
        
        for task in tasks:
            if task.channel_url and task.channel_url not in existing_urls:
                try:
                    # D√©terminer le nom et le pays du concurrent
                    channel_name = task.channel_name or task.channel_url.split('/')[-1]
                    
                    # Intelligence pour d√©terminer le pays
                    country = 'International'  # Par d√©faut
                    name_upper = channel_name.upper()
                    if any(word in name_upper for word in ['ARD', 'REISEN', 'DEUTSCHLAND', 'GERMAN', 'TUI DEUTSCHLAND']):
                        country = 'Germany'
                    elif any(word in name_upper for word in ['FRANCE', 'FRAN√áAIS', 'PIERRE', 'VACANCES', 'CLUB MED']):
                        country = 'France'
                    elif any(word in name_upper for word in ['NETHERLANDS', 'DUTCH', 'LANDAL', 'ROOMPOT']):
                        country = 'Netherlands'
                    elif any(word in name_upper for word in ['SWITZERLAND', 'SUISSE', 'SCHWEIZ']):
                        country = 'Switzerland'
                    elif any(word in name_upper for word in ['HILTON', 'MARRIOTT', 'HYATT', 'EXPEDIA', 'BOOKING']):
                        country = 'International'
                    
                    # Ins√©rer le nouveau concurrent
                    cursor.execute('''
                        INSERT OR IGNORE INTO concurrent 
                        (name, channel_url, country, created_at) 
                        VALUES (?, ?, ?, datetime('now'))
                    ''', (channel_name, task.channel_url, country))
                    
                    if cursor.rowcount > 0:
                        competitors_added += 1
                        existing_urls.add(task.channel_url)
                        print(f"[TASKS-REFRESH] ‚úÖ Concurrent ajout√©: {channel_name} ({country})")
                        
                        # Marquer la t√¢che comme ayant un concurrent ajout√©
                        if hasattr(task, 'warning'):
                            task.warning = f"‚úÖ Concurrent ajout√© automatiquement ({country})"
                    
                except Exception as e:
                    print(f"[TASKS-REFRESH] ‚ùå Erreur ajout concurrent {task.channel_name}: {e}")
            
            elif task.channel_url in existing_urls:
                # Consolider les donn√©es existantes (sans toucher aux classifications humaines)
                try:
                    # R√©cup√©rer l'ID du concurrent
                    cursor.execute("SELECT id FROM concurrent WHERE channel_url = ?", (task.channel_url,))
                    competitor_result = cursor.fetchone()
                    if competitor_result:
                        competitor_id = competitor_result[0]
                        
                        # Mettre √† jour les statistiques de base (nombre de vid√©os, abonn√©s)
                        if task.videos_found and task.videos_found > 0:
                            cursor.execute('''
                                UPDATE concurrent 
                                SET video_count = COALESCE(video_count, 0) + ?, 
                                    last_updated = datetime('now')
                                WHERE id = ? AND video_count < ?
                            ''', (task.videos_found, competitor_id, task.videos_found))
                            
                            if cursor.rowcount > 0:
                                competitors_updated += 1
                
                except Exception as e:
                    print(f"[TASKS-REFRESH] ‚ö†Ô∏è Erreur consolidation {task.channel_name}: {e}")
        
        conn.commit()
        conn.close()
        
        # Formater les t√¢ches pour l'API
        tasks_data = []
        for task in tasks:
            task_data = {
                'id': task.id,
                'status': task.status,
                'progress': task.progress,
                'current_step': task.current_step,
                'videos_found': task.videos_found,
                'videos_processed': task.videos_processed,
                'channel_name': task.channel_name,
                'warning': getattr(task, 'warning', None)
            }
            tasks_data.append(task_data)
        
        response_data = {
            'success': True,
            'tasks': tasks_data,
            'timestamp': datetime.now().isoformat()
        }
        
        # Ajouter les statistiques de consolidation si pertinentes
        if competitors_added > 0 or competitors_updated > 0:
            response_data['consolidation'] = {
                'competitors_added': competitors_added,
                'competitors_updated': competitors_updated
            }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks', methods=['GET'])
@login_required
def get_tasks():
    """R√©cup√®re toutes les t√¢ches"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        tasks = task_manager.get_all_tasks()
        return jsonify([task.to_dict() for task in tasks])
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/tasks/<task_id>/delete', methods=['DELETE'])
@login_required
def delete_task(task_id):
    """Supprime d√©finitivement une t√¢che et ses donn√©es"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        task_manager.delete_task(task_id)
        return jsonify({'success': True, 'message': 'T√¢che supprim√©e d√©finitivement'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/<task_id>/cancel', methods=['POST'])
@login_required
def cancel_task_api(task_id):
    """Annule une t√¢che en cours"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        task_manager.cancel_task(task_id)
        return jsonify({'success': True, 'message': 'T√¢che annul√©e avec succ√®s'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/<task_id>/resume', methods=['POST'])
@login_required
def resume_task_api(task_id):
    """Reprend une t√¢che interrompue"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        task_manager.resume_task(task_id)
        return jsonify({'success': True, 'message': 'T√¢che reprise avec succ√®s'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/clean-duplicates', methods=['POST'])
@login_required
def clean_duplicate_tasks():
    """Nettoie les t√¢ches en double en gardant celle avec le plus de vid√©os"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        deleted_count = task_manager.clean_duplicate_tasks()
        
        if deleted_count > 0:
            message = f'Nettoyage termin√©: {deleted_count} doublons supprim√©s'
        else:
            message = 'Aucun doublon trouv√©'
            
        return jsonify({
            'success': True, 
            'message': message,
            'deleted_count': deleted_count
        })
    except Exception as e:
        print(f"[CLEAN-DUPLICATES] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/migrate-to-database', methods=['POST'])
@login_required
def migrate_tasks_to_database():
    """Migre les t√¢ches du fichier JSON vers la base de donn√©es"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        from yt_channel_analyzer.database import get_db_connection
        
        # R√©cup√©rer toutes les t√¢ches depuis le JSON
        tasks = task_manager.get_all_tasks()
        
        if not tasks:
            return jsonify({'success': True, 'message': 'Aucune t√¢che √† migrer', 'migrated_count': 0})
        
        migrated_count = 0
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            for task in tasks:
                try:
                    # V√©rifier si la t√¢che existe d√©j√† en base
                    cursor.execute('SELECT id FROM background_tasks WHERE id = ?', (task.id,))
                    if cursor.fetchone():
                        continue  # T√¢che d√©j√† migr√©e
                    
                    # Ins√©rer la t√¢che en base
                    cursor.execute('''
                        INSERT INTO background_tasks (
                            id, channel_url, channel_name, status, progress, current_step,
                            videos_found, videos_processed, total_estimated, start_time,
                            end_time, error_message, channel_thumbnail
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        task.id, task.channel_url, task.channel_name, task.status,
                        task.progress, task.current_step, task.videos_found,
                        task.videos_processed, task.total_estimated, task.start_time,
                        task.end_time, task.error_message, task.channel_thumbnail
                    ))
                    migrated_count += 1
                    
                except Exception as e:
                    print(f"Erreur lors de la migration de la t√¢che {task.id}: {e}")
            
            conn.commit()
        
        return jsonify({
            'success': True,
            'message': f'{migrated_count} t√¢ches migr√©es en base de donn√©es',
            'migrated_count': migrated_count,
            'total_tasks': len(tasks)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/launch-background-task', methods=['POST'])
@login_required
def launch_background_task():
    """Lance une nouvelle t√¢che d'analyse en arri√®re-plan depuis la page concurrents"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        data = request.get_json()
        channel_url = data.get('channel_url')
        task_type = data.get('task_type', 'relaunch')
        
        if not channel_url:
            return jsonify({'success': False, 'error': 'URL de la cha√Æne manquante'})
        
        # Extraire le nom de la cha√Æne depuis la base de donn√©es ou l'URL
        channel_name = "Canal"
        try:
            from yt_channel_analyzer.database import get_competitor_by_url
            competitor = get_competitor_by_url(channel_url)
            if competitor:
                channel_name = competitor.get('name', extract_channel_name(channel_url))
            else:
                channel_name = extract_channel_name(channel_url)
        except:
            channel_name = extract_channel_name(channel_url)
        
        # Cr√©er la t√¢che avec un nom descriptif
        task_name = f"Relancement - {channel_name}"
        task_id = task_manager.create_task(channel_url, task_name)
        
        # Lancer le scraping en arri√®re-plan
        task_manager.start_background_scraping(task_id, channel_url)
        
        return jsonify({
            'success': True, 
            'task_id': task_id,
            'message': f'Analyse relanc√©e en arri√®re-plan pour {channel_name}'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/tasks')
@login_required
def tasks_page():
    """Page des t√¢ches en cours avec organisation par pays"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        from yt_channel_analyzer.database import get_db_connection
        
        tasks = task_manager.get_all_tasks_with_warnings()
        
        # √âliminer les doublons bas√©s sur l'ID de la t√¢che
        seen_ids = set()
        unique_tasks = []
        for task in tasks:
            if task.id not in seen_ids:
                seen_ids.add(task.id)
                unique_tasks.append(task)
        
        tasks = unique_tasks
        print(f"[TASKS] {len(tasks)} t√¢ches uniques trouv√©es")
        
        # Organiser les t√¢ches par pays
        tasks_by_country = {
            'FR': [],
            'DE': [],
            'BE': [],
            'NL': [],
            'International': [],
            'Unknown': []
        }
        
        crashed_tasks = []
        
        # R√©cup√©rer les pays et IDs des concurrents pour associer aux t√¢ches
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT id, channel_url, country FROM concurrent")
        url_data = {}
        for row in cursor.fetchall():
            url_data[row[1]] = {'id': row[0], 'country': row[2]}
        conn.close()
        
        # Importer la fonction pour obtenir les miniatures
        from yt_channel_analyzer.utils.thumbnails import get_competitor_thumbnail
        
        for task in tasks:
            # D√©terminer le pays de la t√¢che et ajouter la miniature locale
            country = 'Unknown'
            if hasattr(task, 'channel_url') and task.channel_url:
                data = url_data.get(task.channel_url, {})
                country = data.get('country', 'Unknown')
                
                # Utiliser la miniature locale stock√©e sur le serveur
                if data.get('id'):
                    task.channel_thumbnail = get_competitor_thumbnail(data['id'])
            
            # S√©parer les t√¢ches en erreur
            if task.status == 'error':
                crashed_tasks.append(task)
            else:
                if country in tasks_by_country:
                    tasks_by_country[country].append(task)
                else:
                    tasks_by_country['Unknown'].append(task)
        
        return render_template('tasks_sneat_clean.html', 
                             tasks=tasks, 
                             tasks_by_country=tasks_by_country,
                             crashed_tasks=crashed_tasks)
    except Exception as e:
        return render_template('tasks_sneat_clean.html', 
                             tasks=[], 
                             tasks_by_country={},
                             crashed_tasks=[],
                             error=str(e))

@app.route('/tasks_old')
@login_required
def tasks_page_old():
    """Page des t√¢ches en cours (ancienne version)"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        tasks = task_manager.get_all_tasks()
        return render_template('tasks.html', tasks=tasks)
    except Exception as e:
        return render_template('tasks.html', tasks=[], error=str(e))

# ANCIENNES ROUTES SUPPRIM√âES - Maintenant on utilise /api/refresh-competitor qui fait tout

@app.route('/api/refresh-competitor', methods=['POST'])
@login_required
def refresh_competitor():
    """Rafra√Æchir intelligemment les donn√©es d'un concurrent existant"""
    try:
        data = request.get_json() if request.is_json else request.form
        channel_url = data.get('channel_url', '').strip()
        competitor_id = data.get('competitor_id')
        
        if not channel_url and not competitor_id:
            return jsonify({'success': False, 'error': 'URL de cha√Æne ou ID concurrent requis'})
        
        # Si on a seulement l'ID, r√©cup√©rer l'URL
        if competitor_id and not channel_url:
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT channel_url FROM concurrent WHERE id = ?', (competitor_id,))
            result = cursor.fetchone()
            conn.close()
            
            if not result:
                return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
            channel_url = result[0]
        
        print(f"[REFRESH-API] üîÑ D√©but du rafra√Æchissement pour: {channel_url}")
        
        # R√©cup√©rer les donn√©es fra√Æches via l'API YouTube
        try:
            from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api
            from yt_channel_analyzer.youtube_api_client import create_youtube_client
            
            print(f"[REFRESH-API] üì° Tentative de r√©cup√©ration via API YouTube...")
            
            # V√©rifier si l'API est configur√©e
            if not app.config.get('YOUTUBE_API_KEY'):
                print("[REFRESH-API] ‚ùå Cl√© API YouTube non configur√©e")
                return jsonify({
                    'success': False, 
                    'error': 'API YouTube non configur√©e. Veuillez configurer votre cl√© API dans les param√®tres.'
                })
            
            # R√©cup√©rer les vid√©os fra√Æches (limite √©lev√©e pour avoir le maximum)
            fresh_videos = get_channel_videos_data_api(channel_url, video_limit=1000)
            
            if not fresh_videos:
                print("[REFRESH-API] ‚ö†Ô∏è Aucune vid√©o trouv√©e")
                return jsonify({'success': False, 'error': 'Aucune vid√©o trouv√©e lors du rafra√Æchissement'})
            
            # R√©cup√©rer aussi les infos de la cha√Æne
            youtube_client = create_youtube_client()
            channel_info = youtube_client.get_channel_info(channel_url)
            
            print(f"[REFRESH-API] üìä {len(fresh_videos)} vid√©os r√©cup√©r√©es via API")
            
            # Utiliser la nouvelle fonction de rafra√Æchissement intelligent
            from yt_channel_analyzer.database import refresh_competitor_data
            result = refresh_competitor_data(channel_url, fresh_videos, channel_info)
            
            if result['success']:
                return jsonify({
                    'success': True,
                    'message': f"Rafra√Æchissement termin√© pour {result.get('competitor_name', 'le concurrent')}",
                    'stats': {
                        'total_videos': result['total_videos'],
                        'new_videos': result['new_videos'],
                        'updated_videos': result['updated_videos'],
                        'enriched_videos': result['enriched_videos'],
                        'total_processed': result['total_processed']
                    },
                    'competitor_id': result['competitor_id']
                })
            else:
                return jsonify({'success': False, 'error': result.get('error', 'Erreur inconnue')})
                
        except Exception as api_error:
            print(f"[REFRESH-API] ‚ùå Erreur API: {api_error}")
            import traceback
            traceback.print_exc()
            return jsonify({'success': False, 'error': f'Erreur lors de la r√©cup√©ration des donn√©es: {str(api_error)}'})
        
    except Exception as e:
        print(f"[REFRESH-API] ‚ùå Erreur g√©n√©rale: {e}")
        return jsonify({'success': False, 'error': str(e)})

# ========================================
# API USAGE MONITORING ROUTES
# ========================================

def load_api_quota_data():
    """Charger les donn√©es de quota API"""
    quota_file = "api_quota_tracking.json"
    try:
        with open(quota_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        # Cr√©er le fichier s'il n'existe pas
        default_data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "quota_used": 0,
            "requests_made": 0,
            "last_updated": datetime.now().isoformat()
        }
        save_api_quota_data(default_data)
        return default_data
    except Exception as e:
        print(f"Erreur lors du chargement des donn√©es de quota: {e}")
        return {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "quota_used": 0,
            "requests_made": 0,
            "last_updated": datetime.now().isoformat()
        }

def save_api_quota_data(data):
    """Sauvegarder les donn√©es de quota API"""
    quota_file = "api_quota_tracking.json"
    try:
        with open(quota_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Erreur lors de la sauvegarde des donn√©es de quota: {e}")

@app.route('/api-usage')
@login_required
def api_usage_page():
    """Page de monitoring de l'utilisation de l'API YouTube"""
    try:
        # Charger les donn√©es de quota
        quota_file_data = load_api_quota_data()
        
        # Donn√©es de quota par d√©faut
        quota_data = {
            'daily_usage': quota_file_data.get('quota_used', 0),
            'quota_limit': 10000,
            'percentage': min(100, (quota_file_data.get('quota_used', 0) / 10000) * 100),
            'reset_time': '12:00 AM Pacific'
        }
        
        # Statistiques API par d√©faut
        api_stats = {
            'videos_fetched': quota_file_data.get('requests_made', 0) * 50,  # Estimation
            'channels_analyzed': max(1, quota_file_data.get('requests_made', 0) // 10),
            'search_requests': quota_file_data.get('requests_made', 0),
            'errors': 0
        }
        
        # Configuration API
        api_config = {
            'key_configured': True,  # Assumption
            'key_suffix': '***ABC123',
            'rate_limit': 100
        }
        
        # Appels r√©cents (donn√©es d'exemple)
        recent_calls = [
            {
                'timestamp': datetime.now(),
                'endpoint': 'search',
                'cost': 100,
                'status': 'success',
                'resource': 'channel search'
            }
        ] if quota_file_data.get('requests_made', 0) > 0 else []
        
        return render_template('api_usage_sneat_clean.html', 
                             quota_data=quota_data,
                             api_stats=api_stats,
                             api_config=api_config,
                             recent_calls=recent_calls,
                             dev_mode=session.get('dev_mode', False))
    
    except Exception as e:
        print(f"[API-USAGE] Erreur: {e}")
        # Donn√©es par d√©faut en cas d'erreur
        return render_template('api_usage_sneat_clean.html', 
                             quota_data={'daily_usage': 0, 'quota_limit': 10000, 'percentage': 0, 'reset_time': '12:00 AM Pacific'},
                             api_stats={'videos_fetched': 0, 'channels_analyzed': 0, 'search_requests': 0, 'errors': 0},
                             api_config={'key_configured': False, 'key_suffix': '', 'rate_limit': 100},
                             recent_calls=[],
                             dev_mode=session.get('dev_mode', False))

@app.route('/debug')
@dev_mode_required
def debug_page():
    """Page des outils de debug et de propagation"""
    return render_template('debug.html')

@app.route('/fix-problems')
@login_required
def fix_problems():
    """Page unifi√©e pour corriger TOUS les probl√®mes"""
    try:
        # √âtat de sant√© du syst√®me
        system_health = {
            'database': 'healthy',
            'api': 'healthy', 
            'classification': 'healthy',
            'cache': 'healthy'
        }
        
        # Probl√®mes d√©tect√©s (vide par d√©faut)
        detected_issues = []
        
        # Informations syst√®me
        system_info = {
            'db_size': '25.6 MB',
            'total_videos': 1250,
            'total_competitors': 45,
            'cache_size': '12.3 MB',
            'last_backup': 'Never',
            'last_check': datetime.now().strftime('%Y-%m-%d %H:%M')
        }
        
        # Log de maintenance (vide par d√©faut)
        maintenance_log = []
        
        # Statistiques compl√®tes du syst√®me
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Compter les vid√©os totales
            cursor.execute("SELECT COUNT(*) FROM video")
            total_videos = cursor.fetchone()[0]
            
            # Compter les concurrents
            cursor.execute("SELECT COUNT(*) FROM concurrent")
            total_competitors = cursor.fetchone()[0]
            
            # Compter les playlists
            cursor.execute("SELECT COUNT(*) FROM playlist")
            total_playlists = cursor.fetchone()[0]
            
            # Compter les vid√©os classifi√©es
            cursor.execute("SELECT COUNT(*) FROM video WHERE category IS NOT NULL AND category != 'uncategorized'")
            classified_videos = cursor.fetchone()[0]
            
            # Calculer les pourcentages pour chaque m√©trique
            
            # 1. Dates YouTube
            cursor.execute("""
                SELECT COUNT(*) FROM video 
                WHERE published_time IS NULL 
                OR published_time = '' 
                OR published_time = 'Unknown'
                OR published_time LIKE '%/%'
            """)
            dates_to_fix = cursor.fetchone()[0]
            youtube_dates_percentage = ((total_videos - dates_to_fix) / total_videos * 100) if total_videos > 0 else 100
            
            # 2. Classifications
            classification_percentage = (classified_videos / total_videos * 100) if total_videos > 0 else 100
            
            # 3. Doublons (supposons qu'ils sont nettoy√©s)
            duplicates_percentage = 100
            
            # 4. Statistiques (v√©rifier si les competitor_stats sont √† jour)
            cursor.execute("SELECT COUNT(*) FROM competitor_stats")
            stats_count = cursor.fetchone()[0]
            stats_percentage = (stats_count / total_competitors * 100) if total_competitors > 0 else 100
            
            # 5. Int√©grit√© base (supposons que c'est OK)
            integrity_percentage = 100
            
            # 6. Cache (supposons que c'est optimis√©)
            cache_percentage = 100
            
            system_metrics = {
                'total_videos': total_videos,
                'total_competitors': total_competitors,
                'total_playlists': total_playlists,
                'classified_videos': classified_videos,
                'youtube_dates_percentage': round(youtube_dates_percentage, 1),
                'classification_percentage': round(classification_percentage, 1),
                'duplicates_percentage': duplicates_percentage,
                'stats_percentage': round(stats_percentage, 1),
                'integrity_percentage': integrity_percentage,
                'cache_percentage': cache_percentage
            }
            
            conn.close()
            
        except Exception as e:
            print(f"[ERROR] Erreur lors du calcul des m√©triques syst√®me: {e}")
            # Donn√©es par d√©faut en cas d'erreur
            system_metrics = {
                'total_videos': 7438,
                'total_competitors': 45,
                'total_playlists': 355,
                'classified_videos': 6890,
                'youtube_dates_percentage': 80.0,
                'classification_percentage': 93.0,
                'duplicates_percentage': 100,
                'stats_percentage': 87.0,
                'integrity_percentage': 100,
                'cache_percentage': 100
            }
        
        return render_template('fix_problems_sneat_clean.html', 
                             system_health=system_health,
                             detected_issues=detected_issues,
                             system_info=system_info,
                             maintenance_log=maintenance_log,
                             system_metrics=system_metrics,
                             dev_mode=session.get('dev_mode', False))
    
    except Exception as e:
        print(f"[FIX-PROBLEMS] Erreur: {e}")
        # Donn√©es par d√©faut en cas d'erreur
        return render_template('fix_problems_sneat_clean.html', 
                             system_health={'database': 'unknown', 'api': 'unknown', 'classification': 'unknown', 'cache': 'unknown'},
                             detected_issues=[],
                             system_info={'db_size': 'Unknown', 'total_videos': 0, 'total_competitors': 0, 'cache_size': 'Unknown', 'last_backup': 'Unknown', 'last_check': 'Unknown'},
                             maintenance_log=[],
                             dev_mode=session.get('dev_mode', False))

@app.route('/api/usage')
@login_required
def get_api_usage():
    """R√©cup√©rer les donn√©es d'utilisation de l'API au format JSON"""
    try:
        # Charger les donn√©es du fichier quota
        quota_data = load_api_quota_data()
        
        # R√©cup√©rer les donn√©es depuis l'API YouTube si possible
        try:
            from yt_channel_analyzer.youtube_api_client import get_api_quota_status
            api_status = get_api_quota_status()
            
            # Merger les donn√©es
            quota_used = max(quota_data.get('quota_used', 0), api_status.get('quota_used', 0))
            requests_made = max(quota_data.get('requests_made', 0), api_status.get('requests_made', 0))
        except Exception as e:
            print(f"Erreur r√©cup√©ration status API: {e}")
            quota_used = quota_data.get('quota_used', 0)
            requests_made = quota_data.get('requests_made', 0)
        
        # Constantes
        daily_quota = 10000  # Quota quotidien par d√©faut YouTube API
        
        # Calculer les statistiques
        percentage = (quota_used / daily_quota) * 100 if daily_quota > 0 else 0
        remaining = max(0, daily_quota - quota_used)
        
        # D√©terminer le statut
        if percentage > 90:
            status = 'critical'
        elif percentage > 80:
            status = 'warning'
        else:
            status = 'healthy'
        
        # V√©rifier si on doit r√©initialiser (nouveau jour)
        current_date = datetime.now().strftime("%Y-%m-%d")
        if quota_data.get('date') != current_date:
            # Nouveau jour, r√©initialiser les compteurs
            quota_data.update({
                'date': current_date,
                'quota_used': 0,
                'requests_made': 0,
                'last_updated': datetime.now().isoformat()
            })
            save_api_quota_data(quota_data)
            
            # Recalculer avec les nouvelles valeurs
            quota_used = 0
            requests_made = 0
            percentage = 0
            remaining = daily_quota
            status = 'healthy'
        
        return jsonify({
            'success': True,
            'date': quota_data.get('date', current_date),
            'today_usage': quota_used,
            'quota_used': quota_used,
            'requests_made': requests_made,
            'daily_quota': daily_quota,
            'percentage': round(percentage, 1),
            'remaining': remaining,
            'status': status,
            'last_updated': quota_data.get('last_updated', datetime.now().isoformat())
        })
        
    except Exception as e:
        print(f"Erreur dans get_api_usage: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'date': datetime.now().strftime("%Y-%m-%d"),
            'today_usage': 0,
            'quota_used': 0,
            'requests_made': 0,
            'daily_quota': 10000,
            'percentage': 0,
            'remaining': 10000,
            'status': 'unknown',
            'last_updated': datetime.now().isoformat()
        }), 500

@app.route('/api/usage/reset', methods=['POST'])
@login_required
def reset_api_usage():
    """R√©initialiser le compteur de quota API"""
    try:
        # R√©initialiser les donn√©es
        reset_data = {
            'date': datetime.now().strftime("%Y-%m-%d"),
            'quota_used': 0,
            'requests_made': 0,
            'last_updated': datetime.now().isoformat()
        }
        
        save_api_quota_data(reset_data)
        
        return jsonify({
            'success': True,
            'message': 'Quota counter reset successfully',
            'data': reset_data
        })
        
    except Exception as e:
        print(f"Erreur lors de la r√©initialisation: {e}")
        return jsonify({
            'success': False,
            'message': f'Failed to reset quota: {str(e)}'
        }), 500

@app.route('/autocomplete')
@login_required
def autocomplete():
    """Route pour l'autocompl√©tion des cha√Ænes YouTube - EXCLUT les concurrents d√©j√† en base"""
    q = request.args.get('q', '')
    if not q.strip():
        return jsonify([])
    
    # R√©cup√©rer les cha√Ænes d√©j√† en base pour les exclure
    try:
        from yt_channel_analyzer.database.competitors import CompetitorManager
        competitor_manager = CompetitorManager()
        existing_competitors = competitor_manager.get_all_competitors()
        existing_urls = set()
        existing_names = set()
        
        for comp in existing_competitors:
            if comp.get('channel_url'):
                existing_urls.add(comp['channel_url'].lower().strip())
            if comp.get('name'):
                existing_names.add(comp['name'].lower().strip())
        
        print(f"[AUTOCOMPLETE] üìã {len(existing_competitors)} concurrents en base pour enrichissement")
        
    except Exception as e:
        print(f"[AUTOCOMPLETE] ‚ö†Ô∏è Erreur r√©cup√©ration concurrents existants: {e}")
        existing_urls = set()
        existing_names = set()
    
    # üöÄ Utiliser l'API YouTube en priorit√©
    try:
        suggestions = autocomplete_youtube_channels_api(q, max_results=15)  # Plus de r√©sultats pour compenser l'enrichissement
        if suggestions:
            # Filtrer les suggestions pour exclure celles d√©j√† en base
            filtered_suggestions = []
            for suggestion in suggestions:
                suggestion_url = suggestion.get('url', '').lower().strip()
                suggestion_name = suggestion.get('name', '').lower().strip()
                
                # V√©rifier si cette cha√Æne est d√©j√† en base
                is_already_analyzed = False
                for comp in existing_competitors:
                    comp_url = comp.get('channel_url', '').lower().strip()
                    comp_name = comp.get('name', '').lower().strip()
                    
                    if (suggestion_url == comp_url) or (suggestion_name == comp_name):
                        is_already_analyzed = True
                        print(f"[AUTOCOMPLETE] ‚è≠Ô∏è Ignor√© (d√©j√† en base): {suggestion.get('name', 'Sans nom')}")
                        break
                
                if not is_already_analyzed:
                    # Nouvelle cha√Æne, pas encore analys√©e - on la garde
                    suggestion['is_analyzed'] = False
                    filtered_suggestions.append(suggestion)
                    print(f"[AUTOCOMPLETE] üÜï Nouvelle cha√Æne: {suggestion.get('name', 'Sans nom')}")
            
            enriched_suggestions = filtered_suggestions
            
            print(f"[API] ‚úÖ {len(enriched_suggestions)} suggestions enrichies pour '{q}'")
            return jsonify(enriched_suggestions[:10])  # Limiter √† 10 r√©sultats finaux
            
    except Exception as e:
        print(f"[API] ‚ùå Erreur autocomplete API: {e}")
    
    # Fallback vers le scraping avec enrichissement
    try:
        print(f"[SCRAPING] üîÑ Fallback autocomplete pour '{q}'")
        suggestions = autocomplete_youtube_channels(q)
        
        # Filtrer les suggestions pour exclure celles d√©j√† en base
        filtered_suggestions = []
        for suggestion in suggestions:
            suggestion_url = suggestion.get('url', '').lower().strip()
            suggestion_name = suggestion.get('name', '').lower().strip()
            
            # V√©rifier si cette cha√Æne est d√©j√† en base
            is_already_analyzed = False
            for comp in existing_competitors:
                comp_url = comp.get('channel_url', '').lower().strip()
                comp_name = comp.get('name', '').lower().strip()
                
                if (suggestion_url == comp_url) or (suggestion_name == comp_name):
                    is_already_analyzed = True
                    print(f"[AUTOCOMPLETE] ‚è≠Ô∏è Ignor√© (d√©j√† en base): {suggestion.get('name', 'Sans nom')}")
                    break
            
            if not is_already_analyzed:
                # Nouvelle cha√Æne, pas encore analys√©e - on la garde
                suggestion['is_analyzed'] = False
                filtered_suggestions.append(suggestion)
                print(f"[AUTOCOMPLETE] üÜï Nouvelle cha√Æne: {suggestion.get('name', 'Sans nom')}")
        
        enriched_suggestions = filtered_suggestions
        
        return jsonify(enriched_suggestions[:10])
        
    except Exception as e:
        print(f"[SCRAPING] ‚ùå Erreur scraping autocomplete: {e}")
        return jsonify([])

@app.route('/api/simple-analysis', methods=['POST'])
@login_required
def simple_analysis():
    """Analyser une cha√Æne de mani√®re simple et directe"""
    try:
        data = request.get_json() if request.is_json else request.form
        channel_url = data.get('channel_url', '').strip()
        
        if not channel_url:
            return jsonify({'success': False, 'error': 'URL de cha√Æne manquante'})
        
        # Valider l'URL YouTube
        if 'youtube.com' not in channel_url and 'youtu.be' not in channel_url:
            return jsonify({'success': False, 'error': 'URL YouTube invalide'})
        
        # Cr√©er une t√¢che pour le suivi
        try:
            from yt_channel_analyzer.background_tasks import task_manager
            channel_name = extract_channel_name(channel_url)
            task_id = task_manager.create_task(channel_url, f"Analyse - {channel_name}")
            
            # Marquer comme en cours
            task_manager.update_task(task_id, 
                current_step='R√©cup√©ration des vid√©os via API YouTube...', 
                progress=20
            )
        except Exception as task_error:
            print(f"[TASK] Erreur cr√©ation t√¢che: {task_error}")
            task_id = None
        
        # Lancer l'analyse directement avec l'API YouTube
        try:
            from yt_channel_analyzer.youtube_adapter import get_channel_videos_data_api
            
            print(f"[ANALYSIS] D√©but de l'analyse pour: {channel_url}")
            
            # Mettre √† jour la t√¢che
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Analyse des vid√©os en cours...', 
                    progress=50
                )
            
            # R√©cup√©rer les vid√©os via l'API
            videos = get_channel_videos_data_api(channel_url, video_limit=1000)
            
            if not videos:
                if task_id:
                    task_manager.error_task(task_id, 'Aucune vid√©o trouv√©e')
                return jsonify({'success': False, 'error': 'Aucune vid√©o trouv√©e'})
            
            # Mettre √† jour la t√¢che
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Sauvegarde en base de donn√©es...', 
                    progress=80,
                    videos_found=len(videos)
                )
            
            # Sauvegarder directement dans la base de donn√©es
            competitor_id = save_competitor_data(channel_url, videos)
            
            # üöÄ NOUVEAU : Classification automatique des vid√©os apr√®s sauvegarde
            if task_id:
                task_manager.update_task(task_id, 
                    current_step='Classification automatique des vid√©os...', 
                    progress=90,
                    videos_found=len(videos)
                )
            
            try:
                from yt_channel_analyzer.database import classify_videos_directly_with_keywords
                print(f"[ANALYSIS] ü§ñ Lancement de la classification automatique pour {len(videos)} vid√©os")
                
                # Classifier directement les vid√©os de ce concurrent
                classification_result = classify_videos_directly_with_keywords(competitor_id)
                videos_classified = classification_result.get('videos_classified', 0)
                
                if videos_classified > 0:
                    print(f"[ANALYSIS] ‚úÖ Classification termin√©e: {videos_classified} vid√©os classifi√©es automatiquement")
                else:
                    print(f"[ANALYSIS] ‚ÑπÔ∏è Aucune vid√©o √† classifier (d√©j√† classifi√©es)")
                
            except Exception as classification_error:
                print(f"[ANALYSIS] ‚ö†Ô∏è Erreur lors de la classification automatique: {classification_error}")
                # Ne pas bloquer l'analyse si la classification √©choue
            
            # Marquer la t√¢che comme termin√©e
            if task_id:
                task_manager.complete_task(task_id, len(videos))
            
            print(f"[ANALYSIS] Analyse termin√©e: {len(videos)} vid√©os sauvegard√©es")
            
            return jsonify({
                'success': True,
                'videos_count': len(videos),
                'competitor_id': competitor_id,
                'task_id': task_id,
                'message': f'Analyse termin√©e: {len(videos)} vid√©os trouv√©es',
                'redirect': '/concurrents'
            })
            
        except Exception as api_error:
            print(f"[ANALYSIS] Erreur API: {api_error}")
            if task_id:
                task_manager.error_task(task_id, str(api_error))
            return jsonify({'success': False, 'error': f'Erreur lors de l\'analyse: {str(api_error)}'})
        
    except Exception as e:
        print(f"[ANALYSIS] Erreur g√©n√©rale: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/analyze-channel', methods=['GET', 'POST'])
@login_required
def analyze_channel():
    """Route pour analyser une cha√Æne depuis la barre de recherche centrale"""
    if request.method == 'GET':
        # R√©cup√©rer les param√®tres de l'URL
        search_query = request.args.get('search', '')
        channel_url = request.args.get('channel_url', '')
        max_videos = int(request.args.get('max_videos', 30))
        ai_classification = request.args.get('ai_classification', 'true') == 'true'
        period = request.args.get('period', '')
        
        # Si on a une URL directe, on lance l'analyse
        if channel_url:
            return launch_analysis_with_params(channel_url, max_videos, ai_classification, period)
        
        # Sinon, on redirige vers la page d'accueil avec la recherche pr√©-remplie
        return redirect(url_for('home', search=search_query))
    
    # Si POST, traiter les donn√©es du formulaire
    channel_url = request.form.get('channel_url', '').strip()
    search_query = request.form.get('search_query', '').strip()
    max_videos = int(request.form.get('max_videos', 30))
    ai_classification = 'ai_classification' in request.form
    
    if not channel_url and not search_query:
        flash('Veuillez entrer une URL ou le nom d\'une cha√Æne YouTube', 'error')
        return redirect(url_for('home'))
    
    # Si on a seulement une recherche, essayer de trouver l'URL
    if search_query and not channel_url:
        try:
            # Utiliser l'API d'autocomplete pour trouver l'URL
            from yt_channel_analyzer.scraper import autocomplete_youtube_channels
            suggestions = autocomplete_youtube_channels(search_query)
            if suggestions:
                channel_url = suggestions[0]['url']
            else:
                flash(f'Aucune cha√Æne trouv√©e pour "{search_query}"', 'warning')
                return redirect(url_for('home', search=search_query))
        except Exception as e:
            flash(f'Erreur lors de la recherche: {e}', 'error')
            return redirect(url_for('home'))
    
    return launch_analysis_with_params(channel_url, max_videos, ai_classification)

def launch_analysis_with_params(channel_url, max_videos=30, ai_classification=True, period=''):
    """Lance l'analyse avec les param√®tres donn√©s"""
    try:
        # Importer le gestionnaire de t√¢ches
        from yt_channel_analyzer.background_tasks import task_manager
        
        # Extraire le nom de la cha√Æne
        channel_name = extract_channel_name(channel_url)
        
        # Cr√©er la t√¢che
        task_id = task_manager.create_task(channel_url, f"Analyse - {channel_name}")
        
        # Lancer le scraping en arri√®re-plan 
        task_manager.start_background_scraping(task_id, channel_url)
        
        flash(f'Analyse lanc√©e pour "{channel_name}"', 'success')
        return redirect(url_for('tasks'))
            
    except Exception as e:
        print(f"Erreur lors du lancement de l'analyse: {e}")
        flash(f'Erreur lors du lancement de l\'analyse: {e}', 'error')
    
    return redirect(url_for('home'))

@app.route('/competitor/<int:competitor_id>')
@login_required
def competitor_detail(competitor_id):
    conn = get_db_connection()
    # 1. R√©cup√©ration du concurrent
    competitor = conn.execute('SELECT * FROM concurrent WHERE id = ?', (competitor_id,)).fetchone()
    if competitor is None:
        flash("Concurrent non trouv√©.", "error")
        return redirect(url_for('concurrents'))

    # 2. R√©cup√©ration des vid√©os
    videos = conn.execute('SELECT * FROM video WHERE concurrent_id = ? ORDER BY published_at DESC', (competitor_id,)).fetchall()

    # 3. Charger le seuil paid_threshold depuis les settings
    settings = load_settings()
    paid_threshold = settings.get('paid_threshold', 10000)

    # 4. Calcul des statistiques de base avec organic/paid bas√© sur le seuil
    paid_count = sum(1 for v in videos if v['view_count'] and v['view_count'] >= paid_threshold)
    organic_count = len(videos) - paid_count
    
    # 5. Calcul des statistiques de Shorts bas√©es sur les vraies donn√©es de la base
    shorts_count = sum(1 for v in videos if v['is_short'] == 1)
    regular_videos_count = len(videos) - shorts_count
    shorts_views = sum(v['view_count'] for v in videos if v['is_short'] == 1 and v['view_count'])
    regular_views = sum(v['view_count'] for v in videos if v['is_short'] == 0 and v['view_count'])
    total_views = shorts_views + regular_views
    
    # Calcul des m√©triques de Shorts
    shorts_percentage = (shorts_count / len(videos) * 100) if len(videos) > 0 else 0
    avg_views_shorts = shorts_views / shorts_count if shorts_count > 0 else 0
    avg_views_regular = regular_views / regular_videos_count if regular_videos_count > 0 else 0
    performance_relative = (avg_views_shorts / avg_views_regular * 100) if avg_views_regular > 0 else 0
    
    # Calculer les statistiques temporelles r√©elles si possible
    cursor = conn.cursor()
    cursor.execute("""
        SELECT 
            MIN(youtube_published_at) as first_date,
            MAX(youtube_published_at) as last_date,
            COUNT(CASE WHEN is_short = 1 THEN 1 END) as shorts_count_db
        FROM video 
        WHERE concurrent_id = ? 
        AND youtube_published_at IS NOT NULL 
        AND youtube_published_at != ''
    """, (competitor_id,))
    
    temporal_data = cursor.fetchone()
    
    # Si nous avons de vraies dates YouTube, calculer les vrais ratios
    if temporal_data and temporal_data[0] and temporal_data[1]:
        try:
            from datetime import datetime
            first_date = datetime.fromisoformat(temporal_data[0].replace('Z', '+00:00'))
            last_date = datetime.fromisoformat(temporal_data[1].replace('Z', '+00:00'))
            days_active = (last_date - first_date).days + 1
            
            if days_active > 0:
                shorts_per_week = round(shorts_count * 7 / days_active, 1)
                shorts_per_month = round(shorts_count * 30.5 / days_active, 1)
            else:
                shorts_per_week = 0
                shorts_per_month = shorts_count
        except:
            # Fallback si parsing √©choue
            shorts_per_week = 0
            shorts_per_month = shorts_count
    else:
        # Pas de vraies dates, utiliser des estimations raisonnables
        # Supposons une activit√© sur 2 ans (valeur typique)
        estimated_days = 730  # 2 ans
        shorts_per_week = round(shorts_count * 7 / estimated_days, 1) if shorts_count > 0 else 0
        shorts_per_month = round(shorts_count * 30.5 / estimated_days, 1) if shorts_count > 0 else 0
    
    # Structure des donn√©es de Shorts
    shorts_data = {
        'shorts_percentage': round(shorts_percentage, 1),
        'shorts_count': shorts_count,
        'total_shorts_views': shorts_views,
        'avg_views_shorts': int(avg_views_shorts),
        'avg_views_regular': int(avg_views_regular),
        'performance_relative': round(performance_relative, 1) if performance_relative > 0 else 'N/A',
        'shorts_per_week': shorts_per_week,
        'shorts_per_month': shorts_per_month
    }
    
    # 5. Calcul de la matrice de performance par cat√©gorie et type
    performance_matrix = {
        'hero': {'organic': [], 'paid': []},
        'hub': {'organic': [], 'paid': []},
        'help': {'organic': [], 'paid': []}
    }
    
    # Remplir la matrice avec les vues des vid√©os
    for video in videos:
        if video['view_count'] is None:
            continue
            
        category = video['category'] or 'hub'  # Default to hub if no category
        views = video['view_count']
        
        if category in performance_matrix:
            if views >= paid_threshold:
                performance_matrix[category]['paid'].append(views)
            else:
                performance_matrix[category]['organic'].append(views)
    
    # 6. Calculer les m√©dianes pour chaque cellule de la matrice
    def calculate_median(values):
        if not values:
            return 0
        values.sort()
        n = len(values)
        return values[n//2] if n % 2 == 1 else (values[n//2-1] + values[n//2]) // 2
    
    performance_matrix_stats = {}
    for category in ['hero', 'hub', 'help']:
        performance_matrix_stats[category] = {
            'organic_median': calculate_median(performance_matrix[category]['organic']),
            'organic_count': len(performance_matrix[category]['organic']),
            'paid_median': calculate_median(performance_matrix[category]['paid']),
            'paid_count': len(performance_matrix[category]['paid'])
        }

    stats = {
        'total_videos': len(videos),
        'total_views': sum(v['view_count'] for v in videos if v['view_count']),
        'total_likes': sum(v['like_count'] for v in videos if v['like_count']),
        'hero_count': sum(1 for v in videos if v['category'] == 'hero'),
        'hub_count': sum(1 for v in videos if v['category'] == 'hub'),
        'help_count': sum(1 for v in videos if v['category'] == 'help'),
        'paid_count': paid_count,
        'organic_count': organic_count,
        'paid_threshold': paid_threshold,
        'performance_matrix': performance_matrix_stats
    }
    stats['paid_percentage'] = round((stats['paid_count'] / stats['total_videos']) * 100) if stats['total_videos'] > 0 else 0
    stats['organic_percentage'] = round((stats['organic_count'] / stats['total_videos']) * 100) if stats['total_videos'] > 0 else 0

    # 4. Top 5 vid√©os par vues et par likes
    top_videos_views = sorted([v for v in videos if v['view_count'] is not None], key=lambda x: x['view_count'], reverse=True)[:5]
    top_videos_likes = sorted([v for v in videos if v['like_count'] is not None], key=lambda x: x['like_count'], reverse=True)[:5]
    
    # 5. Pr√©paration des donn√©es pour le graphique Top 5
    top_videos_json = {
        "labels": [v['title'] for v in top_videos_views],
        "views": [v['view_count'] for v in top_videos_views],
        "likes": [v['like_count'] for v in top_videos_views],
        "video_ids": [v['video_id'] for v in top_videos_views]
    }

    # 6. Donn√©es pour la matrice de performance
    performance_data = {
        "labels": ["Vues", "Likes", "HERO", "HUB", "HELP"],
        "values": [
            stats['total_views'] / stats['total_videos'] if stats['total_videos'] > 0 else 0,
            stats['total_likes'] / stats['total_videos'] if stats['total_videos'] > 0 else 0,
            stats['hero_count'],
            stats['hub_count'],
            stats['help_count']
        ]
    }

    # 7. Donn√©es pour la distribution des cat√©gories
    category_distribution = {
        "labels": ["HERO", "HUB", "HELP"],
        "values": [stats['hero_count'], stats['hub_count'], stats['help_count']]
    }

    # 8. Donn√©es pour la distribution des types de contenu
    content_type_distribution = {
        "labels": ["Pay√©", "Organique"],
        "values": [stats['paid_count'], stats['organic_count']]
    }

    # 9. Donn√©es pour les m√©triques d'engagement
    engagement_metrics = {
        "labels": [v['title'][:20] + '...' for v in top_videos_views],
        "values": [(v['like_count'] / v['view_count']) * 100 if v['view_count'] > 0 else 0 for v in top_videos_views]
    }

    # 10. R√©cup√©ration des playlists avec classifications
    from yt_channel_analyzer.database import get_competitor_playlists
    playlists = get_competitor_playlists(competitor_id)

    # 11. R√©cup√©ration des donn√©es d'abonn√©s
    subscriber_data_db = conn.execute(
        'SELECT date, subscriber_count FROM subscriber_data WHERE concurrent_id = ? ORDER BY date',
        (competitor_id,)
    ).fetchall()
    
    # Formater les donn√©es pour correspondre au format attendu par le JavaScript
    subscriber_data_formatted = None
    if subscriber_data_db:
        data_list = [{"date": r["date"], "subscriber_count": r["subscriber_count"]} for r in subscriber_data_db]
        
        # Calculer les statistiques
        first_entry = data_list[0]
        last_entry = data_list[-1]
        total_growth = last_entry['subscriber_count'] - first_entry['subscriber_count']
        growth_percentage = (total_growth / first_entry['subscriber_count']) * 100 if first_entry['subscriber_count'] > 0 else 0
        
        # Formater les donn√©es pour Chart.js
        chart_data = {
            'labels': [entry['date'] for entry in data_list],
            'datasets': [{
                'label': 'Nombre d\'abonn√©s',
                'data': [entry['subscriber_count'] for entry in data_list],
                'borderColor': 'rgba(99, 102, 241, 1)',
                'backgroundColor': 'rgba(99, 102, 241, 0.1)',
                'borderWidth': 3,
                'fill': True,
                'tension': 0.4,
                'pointBackgroundColor': 'rgba(99, 102, 241, 1)',
                'pointBorderColor': '#ffffff',
                'pointBorderWidth': 2,
                'pointRadius': 6,
                'pointHoverRadius': 8
            }]
        }
        
        # Cr√©er l'objet final avec toutes les donn√©es n√©cessaires
        subscriber_data_formatted = {
            'has_data': True,
            'chart_data': chart_data,
            'stats': {
                'total_entries': len(data_list),
                'first_date': first_entry['date'],
                'last_date': last_entry['date'],
                'first_count': first_entry['subscriber_count'],
                'last_count': last_entry['subscriber_count'],
                'total_growth': total_growth,
                'growth_percentage': round(growth_percentage, 2)
            }
        }
    else:
        subscriber_data_formatted = {
            'has_data': False,
            'chart_data': None,
            'stats': {
                'total_entries': 0,
                'total_growth': 0,
                'growth_percentage': 0
            }
        }

    # 11b. Groupement des vid√©os par cat√©gorie
    videos_by_category = {
        'hero': [v for v in videos if v['category'] == 'hero'],
        'hub': [v for v in videos if v['category'] == 'hub'],
        'help': [v for v in videos if v['category'] == 'help']
    }

    # 12. Rendu du template
    return render_template('competitor_detail_sneat_clean.html',
                           competitor=competitor,
                           videos=videos,
                           stats=stats,
                           top_videos_views=top_videos_views,
                           top_videos_likes=top_videos_likes,
                           top_videos_json=top_videos_json,
                           performance_data=performance_data,
                           category_distribution=category_distribution,
                           content_type_distribution=content_type_distribution,
                           engagement_metrics=engagement_metrics,
                           playlists=playlists,
                           subscriber_data=subscriber_data_formatted,
                           hero_count=stats['hero_count'],
                           hub_count=stats['hub_count'],
                           help_count=stats['help_count'],
                           shorts_data=shorts_data,
                           videos_by_category=videos_by_category,
                           total_videos=len(videos),
                           dev_mode=session.get('dev_mode', False))

@app.route('/competitor/<int:competitor_id>/delete')
@login_required
def delete_competitor(competitor_id):
    """Supprime un concurrent et toutes ses donn√©es associ√©es."""
    try:
        conn = get_db_connection()
        # V√©rifier que le concurrent existe
        competitor = conn.execute('SELECT name FROM concurrent WHERE id = ?', (competitor_id,)).fetchone()
        if competitor is None:
            flash("Concurrent non trouv√©.", "error")
            return redirect(url_for('concurrents'))

        # Suppression en cascade (assumant que le sch√©ma a ON DELETE CASCADE)
        conn.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
        conn.commit()
        conn.close()
        
        flash(f'Le concurrent "{competitor["name"]}" et toutes ses donn√©es ont √©t√© supprim√©s avec succ√®s.', 'success')
    except Exception as e:
        flash(f"Une erreur est survenue lors de la suppression : {e}", "error")
    
    return redirect(url_for('concurrents'))

@app.route('/api/competitors/<int:competitor_id>/delete', methods=['DELETE'])
@login_required
def api_delete_competitor_restful(competitor_id):
    """API RESTful pour supprimer un concurrent"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        
        # V√©rifier que le concurrent existe
        cursor = conn.cursor()
        cursor.execute('SELECT name FROM concurrent WHERE id = ?', (competitor_id,))
        competitor = cursor.fetchone()
        
        if not competitor:
            return jsonify({'success': False, 'error': 'Concurrent non trouv√©'}), 404
        
        competitor_name = competitor[0]
        
        # Suppression en cascade
        cursor.execute('DELETE FROM playlist_video WHERE playlist_id IN (SELECT id FROM playlist WHERE concurrent_id = ?)', (competitor_id,))
        cursor.execute('DELETE FROM playlist WHERE concurrent_id = ?', (competitor_id,))
        cursor.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
        
        conn.commit()
        conn.close()
        
        return jsonify({
            'success': True, 
            'message': f'Concurrent "{competitor_name}" supprim√© avec succ√®s'
        })
        
    except Exception as e:
        print(f"[DELETE_COMPETITOR] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/delete-competitor', methods=['POST'])
@login_required
def api_delete_competitor():
    """API pour supprimer un concurrent (ancienne version)"""
    try:
        data = request.get_json()
        competitor_id = data.get('competitor_id')
        
        if not competitor_id:
            return jsonify({'success': False, 'error': 'ID concurrent manquant'})
        
        # Convertir en entier si n√©cessaire
        try:
            competitor_id = int(competitor_id)
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'ID concurrent invalide'})
        
        # V√©rifier que le concurrent existe et r√©cup√©rer son nom
        conn = db.engine.raw_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT name FROM concurrent WHERE id = ?', (competitor_id,))
        result = cursor.fetchone()
        
        if not result:
            conn.close()
            return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
        
        competitor_name = result[0]
        
        # Supprimer le concurrent (les vid√©os sont supprim√©es automatiquement par cascade)
        cursor.execute('DELETE FROM concurrent WHERE id = ?', (competitor_id,))
        conn.commit()
        conn.close()
        
        print(f"[DELETE-COMPETITOR] Concurrent supprim√©: {competitor_name} (ID: {competitor_id})")
        
        return jsonify({
            'success': True, 
            'message': f'Concurrent {competitor_name} supprim√© avec succ√®s'
        })
        
    except Exception as e:
        print(f"[DELETE-COMPETITOR] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

# --- ROUTES POUR LA GESTION DES PLAYLISTS ---

@app.route('/api/competitor/<int:competitor_id>/playlists/refresh', methods=['POST'])
@login_required
def refresh_competitor_playlists(competitor_id):
    """R√©cup√©rer et sauvegarder les playlists d'un concurrent depuis l'API YouTube"""
    try:
        # üö® PROTECTION 1: V√©rifier le quota API disponible
        quota_data = load_api_quota_data()
        daily_usage = quota_data.get('daily_usage', 0)
        quota_limit = quota_data.get('quota_limit', 10000)
        
        # Estimer le co√ªt de cette op√©ration (channel info + playlists + vid√©os)
        estimated_cost = 150  # Environ 150 unit√©s pour une actualisation compl√®te
        
        if daily_usage + estimated_cost > quota_limit:
            return jsonify({
                'success': False, 
                'error': f'Quota API insuffisant. Utilis√©: {daily_usage}/{quota_limit}. Co√ªt estim√©: {estimated_cost} unit√©s.'
            })
        
        # üö® PROTECTION 2: Rate limiting par concurrent (max 1 fois par heure)
        import time
        current_time = time.time()
        cache_key = f'playlist_refresh_{competitor_id}'
        
        # V√©rifier le cache des derni√®res actualisations
        if hasattr(refresh_competitor_playlists, '_last_refresh'):
            last_refresh = refresh_competitor_playlists._last_refresh.get(cache_key, 0)
            if current_time - last_refresh < 3600:  # 1 heure = 3600 secondes
                remaining_time = int(3600 - (current_time - last_refresh))
                return jsonify({
                    'success': False,
                    'error': f'‚è∞ Actualisation trop r√©cente. R√©essayez dans {remaining_time//60}min {remaining_time%60}s.'
                })
        else:
            refresh_competitor_playlists._last_refresh = {}
        
        # R√©cup√©rer l'URL de la cha√Æne
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT channel_url, name FROM concurrent WHERE id = ?', (competitor_id,))
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return jsonify({'success': False, 'error': 'Concurrent non trouv√©'})
        
        channel_url, competitor_name = result
        
        # üö® PROTECTION 3: Log de l'action pour audit
        print(f"[PLAYLISTS] ‚ö†Ô∏è ACTUALISATION API demand√©e pour {competitor_name} (ID: {competitor_id})")
        print(f"[PLAYLISTS] üìä Quota avant: {daily_usage}/{quota_limit}")
        
        # R√©cup√©rer les playlists depuis l'API YouTube
        from yt_channel_analyzer.youtube_api_client import create_youtube_client
        youtube_client = create_youtube_client()
        
        print(f"[PLAYLISTS] üîç R√©cup√©ration des playlists pour: {channel_url}")
        
        # R√©cup√©rer l'ID de la cha√Æne
        channel_info = youtube_client.get_channel_info(channel_url)
        channel_id = channel_info['id']
        
        # R√©cup√©rer les playlists
        playlists_data = youtube_client.get_channel_playlists(channel_id)
        
        if not playlists_data:
            return jsonify({'success': False, 'error': 'Aucune playlist trouv√©e'})
        
        print(f"[PLAYLISTS] üìã {len(playlists_data)} playlists trouv√©es")
        
        # Sauvegarder les playlists
        from yt_channel_analyzer.database import save_competitor_playlists, link_playlist_videos
        save_competitor_playlists(competitor_id, playlists_data)
        
        # R√©cup√©rer les vid√©os de chaque playlist et les lier (LIMIT√â pour √©conomiser le quota)
        print(f"[PLAYLISTS] üîó Liaison des vid√©os aux playlists...")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        linked_videos = 0
        for playlist_data in playlists_data:
            playlist_id = playlist_data.get('playlist_id')
            if not playlist_id:
                continue
                
            # R√©cup√©rer l'ID de la playlist dans notre base
            cursor.execute('SELECT id FROM playlist WHERE playlist_id = ?', (playlist_id,))
            result = cursor.fetchone()
            if not result:
                continue
                
            playlist_db_id = result[0]
            
            # üö® PROTECTION 4: Limiter √† 50 vid√©os par playlist pour √©conomiser le quota
            video_ids = youtube_client.get_playlist_videos(playlist_id, max_results=50)
            
            if video_ids:
                # Lier les vid√©os √† la playlist
                link_playlist_videos(playlist_db_id, video_ids)
                linked_videos += len(video_ids)
        
        conn.close()
        
        # üö® PROTECTION 5: Enregistrer le timestamp de cette actualisation
        refresh_competitor_playlists._last_refresh[cache_key] = current_time
        
        # Mettre √† jour le quota utilis√©
        quota_data['daily_usage'] = daily_usage + estimated_cost
        save_api_quota_data(quota_data)
        
        print(f"[PLAYLISTS] ‚úÖ Actualisation termin√©e. Quota utilis√©: {estimated_cost} unit√©s")
        
        return jsonify({
            'success': True,
            'count': len(playlists_data),
            'linked_videos': linked_videos,
            'quota_used': estimated_cost,
            'message': f'‚úÖ {len(playlists_data)} playlists r√©cup√©r√©es, {linked_videos} vid√©os li√©es. Quota: {estimated_cost} unit√©s.'
        })
        
    except Exception as e:
        print(f"[PLAYLISTS] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tag-playlist', methods=['POST'])
@login_required  
def tag_playlist():
    """Tagguer une playlist avec une cat√©gorie (hero/hub/help) - NOUVELLE LOGIQUE S√âCURIS√âE"""
    try:
        data = request.get_json()
        playlist_id = data.get('playlist_id')
        category = data.get('category')
        competitor_id = data.get('competitor_id')
        force_propagate = data.get('force_propagate', False)  # Nouveau flag explicite
        
        if not playlist_id:
            return jsonify({'success': False, 'error': 'ID playlist manquant'})
        
        # Valider la cat√©gorie
        valid_categories = ['hero', 'hub', 'help', None]
        if category not in valid_categories:
            return jsonify({'success': False, 'error': 'Cat√©gorie invalide'})
        
        # üö® NOUVEAU : Utiliser la fonction de marquage humain pour cette classification
        from yt_channel_analyzer.database import mark_human_classification
        
        if category:
            # Marquer comme classification humaine (priorit√© absolue)
            success = mark_human_classification(playlist_id=playlist_id, category=category, user_notes='Classification manuelle via interface')
            if not success:
                return jsonify({'success': False, 'error': 'Erreur lors du marquage de la classification humaine'})
        else:
            # Si cat√©gorie = None, mettre √† jour normalement (suppression de cat√©gorie)
            from yt_channel_analyzer.database import update_playlist_category
            update_playlist_category(playlist_id, category)
        
        # üö® NOUVELLE LOGIQUE : NE PAS propager automatiquement les classifications humaines
        # L'utilisateur doit explicitement demander la propagation
        
        videos_updated = 0
        human_protected_count = 0
        human_playlists_skipped = 0
        processed_playlists = []
        propagation_message = ""
        
        if competitor_id and category:
            if force_propagate:
                # L'utilisateur a explicitement demand√© la propagation
                try:
                    from yt_channel_analyzer.database import apply_playlist_categories_to_videos_safe
                    result = apply_playlist_categories_to_videos_safe(
                        competitor_id=competitor_id, 
                        specific_playlist_id=playlist_id,
                        force_human_playlists=True  # Mode force explicite
                    )
                    videos_updated = result.get('videos_updated', 0)
                    human_protected_count = result.get('human_protected', 0)
                    processed_playlists = result.get('processed_playlists', [])
                    
                    propagation_message = f" ‚Ä¢ Propagation FORC√âE: {videos_updated} vid√©os taggu√©es"
                    if human_protected_count > 0:
                        propagation_message += f" ‚Ä¢ {human_protected_count} vid√©os prot√©g√©es"
                        
                    print(f"[TAG-PLAYLIST] ‚ö†Ô∏è PROPAGATION FORC√âE demand√©e par l'utilisateur: {videos_updated} vid√©os")
                    
                except Exception as propagation_error:
                    print(f"[TAG-PLAYLIST] ‚ùå Erreur lors de la propagation forc√©e: {propagation_error}")
                    propagation_message = " ‚Ä¢ Erreur lors de la propagation"
            else:
                # Classification humaine : PAS de propagation automatique
                propagation_message = " ‚Ä¢ Aucune propagation automatique (classification humaine prot√©g√©e)"
                print(f"[TAG-PLAYLIST] üõ°Ô∏è Classification humaine prot√©g√©e - AUCUNE propagation automatique")
        
        category_name = category.upper() if category else "Non cat√©goris√©e"
        
        # Message enrichi avec nouvelle logique de s√©curit√©
        message = f'ü§ñ Playlist marqu√©e comme {category_name} (HUMAIN - PROT√âG√âE){propagation_message}'
        
        return jsonify({
            'success': True,
            'message': message,
            'videos_updated': videos_updated,
            'human_protected': human_protected_count,
            'human_playlists_skipped': human_playlists_skipped,
            'processed_playlists': processed_playlists,
            'human_classification': True,
            'auto_propagated': force_propagate,
            'protection_active': True  # Nouveau flag
        })
        
    except Exception as e:
        print(f"[TAG-PLAYLIST] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitor/<int:competitor_id>/apply-playlist-categories', methods=['POST'])
@login_required
def apply_playlist_categories(competitor_id):
    """Appliquer automatiquement les cat√©gories des playlists aux vid√©os"""
    try:
        from yt_channel_analyzer.database import apply_playlist_categories_to_videos
        
        # Appliquer les cat√©gories
        videos_updated = apply_playlist_categories_to_videos(competitor_id)
        
        return jsonify({
            'success': True,
            'videos_updated': videos_updated,
            'message': f'Classification appliqu√©e: {videos_updated} vid√©os mises √† jour'
        })
        
    except Exception as e:
        print(f"[APPLY-CATEGORIES] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitor/<int:competitor_id>/playlists/ai-classify', methods=['POST'])
@login_required
def ai_classify_playlists(competitor_id):
    """Classifier automatiquement les playlists non cat√©goris√©es avec l'IA"""
    try:
        from yt_channel_analyzer.database import (
            auto_classify_uncategorized_playlists,
            apply_playlist_categories_to_videos_safe,
            set_last_action_status
        )

        # 1Ô∏è‚É£ Classification des playlists via mod√®le s√©mantique
        result = auto_classify_uncategorized_playlists(competitor_id)

        # 2Ô∏è‚É£ Propagation imm√©diate vers les vid√©os (respecte la protection humaine)
        propagation = apply_playlist_categories_to_videos_safe(competitor_id)
        result["videos_updated"] = propagation.get("applied_count", 0)
        result["propagation_message"] = propagation.get("message", "")

        # Journaliser l'action
        set_last_action_status('last_playlist_classification', {
            'action': 'ai_classify_playlists',
            'competitor_id': competitor_id,
            'timestamp': datetime.now().isoformat(),
            'result': result,
            'message': 'Classification IA + propagation effectu√©es'
        })

        return jsonify(result)
    except Exception as e:
        print(f"[AI-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/competitor/<int:competitor_id>/playlists/classify')
@login_required
def playlist_classification_page(competitor_id):
    """Page d√©di√©e √† la classification des playlists avec interface moderne"""
    try:
        # R√©cup√©rer les informations du concurrent
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM concurrent WHERE id = ?', (competitor_id,))
        competitor_data = cursor.fetchone()
        conn.close()
        
        if not competitor_data:
            flash('Concurrent non trouv√©', 'error')
            return redirect(url_for('concurrents'))
        
        # Convertir en objet competitor
        competitor = {
            'id': competitor_data[0],
            'name': competitor_data[1],
            'channel_id': competitor_data[2],
            'channel_url': competitor_data[3],
            'thumbnail_url': competitor_data[4],
            'banner_url': competitor_data[5],
            'description': competitor_data[6],
            'subscriber_count': competitor_data[7],
            'view_count': competitor_data[8],
            'video_count': competitor_data[9],
            'country': competitor_data[10],
            'language': competitor_data[11],
            'created_at': competitor_data[12],
            'last_updated': competitor_data[13]
        }
        
        return render_template('playlist_classification.html', competitor=competitor)
        
    except Exception as e:
        print(f"[PLAYLIST-CLASSIFY-PAGE] ‚ùå Erreur: {e}")
        flash(f'Erreur lors du chargement de la page: {str(e)}', 'error')
        return redirect(url_for('concurrents'))

@app.route('/api/competitor/<int:competitor_id>/playlists', methods=['GET'])
@login_required
def get_competitor_playlists_api(competitor_id):
    """API pour r√©cup√©rer les playlists d'un concurrent avec toutes les informations de classification"""
    try:
        from yt_channel_analyzer.database import get_competitor_playlists
        playlists = get_competitor_playlists(competitor_id)
        
        # Enrichir les playlists avec les informations de vid√©os
        conn = get_db_connection()
        cursor = conn.cursor()
        
        for playlist in playlists:
            # Compter les vid√©os dans la playlist
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
            ''', (playlist['id'],))
            
            video_count_db = cursor.fetchone()[0]
            # Fallback : si aucun lien en table, utiliser le champ video_count stock√© lors de l'import
            playlist['video_count'] = video_count_db if video_count_db > 0 else playlist.get('video_count', 0)
            
            # Compter les vid√©os classifi√©es humainement dans cette playlist
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source = 'human' OR v.human_verified = 1)
            ''', (playlist['id'],))
            
            human_videos = cursor.fetchone()[0]
            playlist['human_videos_count'] = human_videos
            
            # Informations sur les vid√©os potentiellement affect√©es
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source IS NULL OR v.classification_source != 'human')
                AND (v.human_verified IS NULL OR v.human_verified != 1)
            ''', (playlist['id'],))
            
            propagatable_videos = cursor.fetchone()[0]
            playlist['propagatable_videos_count'] = propagatable_videos
        
        conn.close()
        
        return jsonify({
            'success': True,
            'playlists': playlists,
            'total_count': len(playlists),
            'message': f'{len(playlists)} playlists r√©cup√©r√©es'
        })
        
    except Exception as e:
        print(f"[GET-PLAYLISTS-API] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/playlists/bulk-classify', methods=['POST'])
@login_required
def bulk_classify_playlists():
    """Classification en lot des playlists avec templates ou cat√©gories personnalis√©es"""
    try:
        data = request.get_json()
        playlist_ids = data.get('playlist_ids', [])
        template = data.get('template', None)
        custom_category = data.get('custom_category', None)
        competitor_id = data.get('competitor_id', None)
        force_propagate = data.get('force_propagate', False)
        
        if not playlist_ids:
            return jsonify({'success': False, 'error': 'Aucune playlist s√©lectionn√©e'})
        
        if not (template or custom_category):
            return jsonify({'success': False, 'error': 'Template ou cat√©gorie personnalis√©e requis'})
        
        # Mapping des templates vers les cat√©gories + cat√©gories directes
        template_mappings = {
            'educational': 'help',
            'entertainment': 'hub',
            'promotional': 'hero',
            'seasonal': 'hero',
            'tutorials': 'help',
            'brand_content': 'hub',
            'product_launch': 'hero',
            'community': 'hub',
            'behind_scenes': 'hub',
            'testimonials': 'hero',
            # Cat√©gories directes
            'hero': 'hero',
            'hub': 'hub',
            'help': 'help'
        }
        
        # D√©terminer la cat√©gorie
        if template and template in template_mappings:
            category = template_mappings[template]
        elif custom_category:
            category = custom_category
        else:
            return jsonify({'success': False, 'error': 'Template ou cat√©gorie invalide'})
        
        # Valider la cat√©gorie
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'success': False, 'error': 'Cat√©gorie invalide'})
        
        # Traiter chaque playlist
        from yt_channel_analyzer.database import mark_human_classification
        
        processed_playlists = []
        total_videos_affected = 0
        
        for playlist_id in playlist_ids:
            try:
                # Marquer comme classification humaine
                success = mark_human_classification(
                    playlist_id=playlist_id, 
                    category=category, 
                    user_notes=f'Classification en lot - Template: {template or "personnalis√©"}'
                )
                
                if success:
                    processed_playlists.append(playlist_id)
                    
                    # Compter les vid√©os qui seront affect√©es
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute('''
                        SELECT COUNT(*) FROM playlist_video pv
                        JOIN video v ON pv.video_id = v.id
                        WHERE pv.playlist_id = ?
                    ''', (playlist_id,))
                    
                    video_count = cursor.fetchone()[0]
                    total_videos_affected += video_count
                    conn.close()
                    
            except Exception as e:
                print(f"[BULK-CLASSIFY] ‚ùå Erreur playlist {playlist_id}: {e}")
                continue
        
        # Propagation optionnelle
        propagation_message = ""
        if force_propagate and competitor_id:
            try:
                from yt_channel_analyzer.database import apply_playlist_categories_to_videos_safe
                result = apply_playlist_categories_to_videos_safe(
                    competitor_id=competitor_id,
                    force_human_playlists=True  # Mode force pour la propagation
                )
                
                videos_updated = result.get('videos_updated', 0)
                propagation_message = f" ‚Ä¢ Propagation: {videos_updated} vid√©os mises √† jour"
                
            except Exception as e:
                print(f"[BULK-CLASSIFY] ‚ùå Erreur propagation: {e}")
                propagation_message = " ‚Ä¢ Erreur lors de la propagation"
        
        return jsonify({
            'success': True,
            'processed_playlists': len(processed_playlists),
            'total_playlists': len(playlist_ids),
            'total_videos_affected': total_videos_affected,
            'category': category,
            'template': template,
            'message': f'‚úÖ {len(processed_playlists)} playlist(s) classifi√©e(s) comme {category.upper()}{propagation_message}'
        })
        
    except Exception as e:
        print(f"[BULK-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/playlists/propagate-preview', methods=['POST'])
@login_required
def preview_playlist_propagation():
    """Pr√©visualiser l'impact de la propagation des classifications de playlists"""
    try:
        data = request.get_json()
        playlist_ids = data.get('playlist_ids', [])
        
        if not playlist_ids:
            return jsonify({'success': False, 'error': 'Aucune playlist s√©lectionn√©e'})
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        preview_data = []
        total_videos = 0
        total_protected = 0
        
        for playlist_id in playlist_ids:
            # Informations de la playlist
            cursor.execute('''
                SELECT name, category, classification_source, human_verified
                FROM playlist WHERE id = ?
            ''', (playlist_id,))
            
            playlist_info = cursor.fetchone()
            if not playlist_info:
                continue
            
            playlist_name = playlist_info[0]
            playlist_category = playlist_info[1]
            playlist_source = playlist_info[2]
            playlist_human = playlist_info[3]
            
            # Compter les vid√©os
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
            ''', (playlist_id,))
            
            video_count = cursor.fetchone()[0]
            
            # Compter les vid√©os prot√©g√©es
            cursor.execute('''
                SELECT COUNT(*) FROM playlist_video pv
                JOIN video v ON pv.video_id = v.id
                WHERE pv.playlist_id = ?
                AND (v.classification_source = 'human' OR v.human_verified = 1)
            ''', (playlist_id,))
            
            protected_count = cursor.fetchone()[0]
            
            # Vid√©os qui seront affect√©es
            affected_count = video_count - protected_count
            
            preview_data.append({
                'playlist_id': playlist_id,
                'playlist_name': playlist_name,
                'playlist_category': playlist_category,
                'playlist_source': playlist_source,
                'playlist_human': playlist_human,
                'total_videos': video_count,
                'protected_videos': protected_count,
                'affected_videos': affected_count
            })
            
            total_videos += video_count
            total_protected += protected_count
        
        conn.close()
        
        return jsonify({
            'success': True,
            'preview_data': preview_data,
            'summary': {
                'total_playlists': len(playlist_ids),
                'total_videos': total_videos,
                'protected_videos': total_protected,
                'affected_videos': total_videos - total_protected
            }
        })
        
    except Exception as e:
        print(f"[PROPAGATE-PREVIEW] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

# Routes ViewStats supprim√©es - analyse locale maintenant

@app.route('/top-videos')
@login_required
@cache.cached(timeout=300, query_string=True)  # Cache avec param√®tres de la query string
def top_videos():
    """Page du top global des vid√©os de tous les concurrents - VERSION OPTIMIS√âE"""
    try:
        # R√©cup√©rer les param√®tres de tri et filtrage
        sort_by = request.args.get('sort_by', 'view_count')  # Par d√©faut: nombre de vues
        order = request.args.get('order', 'desc')  # Par d√©faut: d√©croissant
        category_filter = request.args.get('category', 'all')  # Filtrer par cat√©gorie
        organic_filter = request.args.get('organic', 'all')  # Filtrer par organique/paid
        limit = int(request.args.get('limit', 50))  # Limite par d√©faut: 50 vid√©os
        
        # R√©cup√©rer les seuils pour d√©terminer si c'est organique ou pay√©
        settings = load_settings()
        paid_threshold = settings.get('paid_threshold', 10000)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Requ√™te optimis√©e avec calculs c√¥t√© SQL
        base_query = """
            SELECT v.id, v.title, v.video_id, v.url, v.thumbnail_url,
                   v.view_count, v.like_count, v.comment_count,
                   v.published_at, v.duration_seconds, v.duration_text,
                   v.category, v.beauty_score, v.emotion_score, v.info_quality_score,
                   c.name as competitor_name, c.id as competitor_id,
                   c.subscriber_count as channel_subscribers,
                   CASE 
                       WHEN v.view_count >= ? THEN 'paid'
                       ELSE 'organic'
                   END as organic_status,
                   -- Calculs c√¥t√© SQL pour √©viter les calculs Python
                   CASE 
                       WHEN v.view_count > 0 THEN ROUND(((COALESCE(v.like_count, 0) + COALESCE(v.comment_count, 0)) * 100.0) / v.view_count, 2)
                       ELSE 0
                   END as engagement_ratio,
                   ROUND((COALESCE(v.beauty_score, 0) + COALESCE(v.emotion_score, 0) + COALESCE(v.info_quality_score, 0)) / 3.0, 1) as avg_subjective_score,
                   -- Formatage des dates c√¥t√© SQL
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN date(v.published_at)
                       ELSE 'N/A'
                   END as published_at_formatted,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN time(v.published_at)
                       ELSE 'N/A'
                   END as published_hour_formatted,
                   -- Formatage de la dur√©e c√¥t√© SQL
                   CASE 
                       WHEN v.duration_seconds IS NOT NULL AND v.duration_seconds > 0 THEN 
                           printf('%d:%02d', v.duration_seconds / 60, v.duration_seconds % 60)
                       ELSE COALESCE(v.duration_text, 'N/A')
                   END as duration_formatted
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.view_count IS NOT NULL
        """
        
        params = [paid_threshold]
        
        # Ajouter les filtres
        if category_filter != 'all':
            base_query += " AND v.category = ?"
            params.append(category_filter)
        
        if organic_filter == 'organic':
            base_query += " AND v.view_count < ?"
            params.append(paid_threshold)
        elif organic_filter == 'paid':
            base_query += " AND v.view_count >= ?"
            params.append(paid_threshold)
        
        # Ajouter le tri
        valid_sort_columns = {
            'view_count': 'v.view_count',
            'like_count': 'v.like_count',
            'comment_count': 'v.comment_count',
            'published_at': 'v.published_at',
            'published_hour': 'v.published_at',
            'duration_seconds': 'v.duration_seconds',
            'title': 'v.title',
            'competitor_name': 'c.name',
            'beauty_score': 'v.beauty_score',
            'emotion_score': 'v.emotion_score',
            'info_quality_score': 'v.info_quality_score'
        }
        
        if sort_by in valid_sort_columns:
            order_clause = 'DESC' if order == 'desc' else 'ASC'
            base_query += f" ORDER BY {valid_sort_columns[sort_by]} {order_clause}"
        else:
            base_query += " ORDER BY v.view_count DESC"
        
        # Ajouter la limite
        base_query += " LIMIT ?"
        params.append(limit)
        
        cursor.execute(base_query, params)
        rows = cursor.fetchall()
        
        # Construction simplifi√©e des vid√©os (calculs d√©j√† faits c√¥t√© SQL)
        videos = []
        for row in rows:
            video = {
                'id': row[0],
                'title': row[1],
                'video_id': row[2],
                'url': row[3],
                'thumbnail_url': row[4],
                'view_count': row[5] or 0,
                'like_count': row[6] or 0,
                'comment_count': row[7] or 0,
                'published_at': row[8],
                'duration_seconds': row[9] or 0,
                'duration_text': row[10] or 'N/A',
                'category': row[11] or 'hub',
                'beauty_score': row[12] or 0,
                'emotion_score': row[13] or 0,
                'info_quality_score': row[14] or 0,
                'competitor_name': row[15],
                'competitor_id': row[16],
                'channel_subscribers': row[17] or 0,
                'organic_status': row[18],
                'engagement_ratio': row[19],  # D√©j√† calcul√© c√¥t√© SQL
                'avg_subjective_score': row[20],  # D√©j√† calcul√© c√¥t√© SQL
                'published_at_formatted': row[21],  # D√©j√† format√© c√¥t√© SQL
                'published_hour_formatted': row[22],  # D√©j√† format√© c√¥t√© SQL
                'duration_formatted': row[23]  # D√©j√† format√© c√¥t√© SQL
            }
            videos.append(video)
        
        # Statistiques g√©n√©rales avec une seule requ√™te optimis√©e
        cursor.execute("""
            SELECT 
                COUNT(*) as total_videos,
                COUNT(DISTINCT concurrent_id) as total_competitors,
                SUM(CASE WHEN view_count >= ? THEN 1 ELSE 0 END) as total_paid_videos,
                SUM(CASE WHEN view_count < ? THEN 1 ELSE 0 END) as total_organic_videos,
                SUM(view_count) as total_views,
                AVG(view_count) as avg_views
            FROM video
            WHERE view_count IS NOT NULL
        """, [paid_threshold, paid_threshold])
        
        stats_row = cursor.fetchone()
        stats = {
            'total_videos': stats_row[0] if stats_row else 0,
            'total_competitors': stats_row[1] if stats_row else 0,
            'total_paid_videos': stats_row[2] if stats_row else 0,
            'total_organic_videos': stats_row[3] if stats_row else 0,
            'total_views': stats_row[4] if stats_row else 0,
            'avg_views': stats_row[5] if stats_row else 0,
            'paid_threshold': paid_threshold,
            'shown_videos': len(videos)
        }
        
        conn.close()
        
        return render_template('top_videos_sneat_clean.html',
                             videos=videos,
                             stats=stats,
                             current_sort=sort_by,
                             current_order=order,
                             current_category=category_filter,
                             current_organic=organic_filter,
                             current_limit=limit,
                             dev_mode=session.get('dev_mode', False))
    
    except Exception as e:
        print(f"[TOP-VIDEOS] ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('error.html', error=str(e))

@app.route('/api/top-videos')
@login_required
@cache.cached(timeout=300, query_string=True)
def api_top_videos():
    """API pour charger plus de vid√©os avec pagination AJAX"""
    try:
        # R√©cup√©rer les param√®tres
        sort_by = request.args.get('sort_by', 'view_count')
        order = request.args.get('order', 'desc')
        category_filter = request.args.get('category', 'all')
        organic_filter = request.args.get('organic', 'all')
        limit = int(request.args.get('limit', 20))  # Limite r√©duite pour AJAX
        offset = int(request.args.get('offset', 0))
        
        settings = load_settings()
        paid_threshold = settings.get('paid_threshold', 10000)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # M√™me requ√™te optimis√©e que top_videos mais avec OFFSET
        base_query = """
            SELECT v.id, v.title, v.video_id, v.url, v.thumbnail_url,
                   v.view_count, v.like_count, v.comment_count,
                   v.published_at, v.duration_seconds, v.duration_text,
                   v.category, v.beauty_score, v.emotion_score, v.info_quality_score,
                   c.name as competitor_name, c.id as competitor_id,
                   c.subscriber_count as channel_subscribers,
                   CASE 
                       WHEN v.view_count >= ? THEN 'paid'
                       ELSE 'organic'
                   END as organic_status,
                   CASE 
                       WHEN v.view_count > 0 THEN ROUND(((COALESCE(v.like_count, 0) + COALESCE(v.comment_count, 0)) * 100.0) / v.view_count, 2)
                       ELSE 0
                   END as engagement_ratio,
                   ROUND((COALESCE(v.beauty_score, 0) + COALESCE(v.emotion_score, 0) + COALESCE(v.info_quality_score, 0)) / 3.0, 1) as avg_subjective_score,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN date(v.published_at)
                       ELSE 'N/A'
                   END as published_at_formatted,
                   CASE 
                       WHEN v.published_at IS NOT NULL THEN time(v.published_at)
                       ELSE 'N/A'
                   END as published_hour_formatted,
                   CASE 
                       WHEN v.duration_seconds IS NOT NULL AND v.duration_seconds > 0 THEN 
                           printf('%d:%02d', v.duration_seconds / 60, v.duration_seconds % 60)
                       ELSE COALESCE(v.duration_text, 'N/A')
                   END as duration_formatted
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.view_count IS NOT NULL
        """
        
        params = [paid_threshold]
        
        # Ajouter les filtres
        if category_filter != 'all':
            base_query += " AND v.category = ?"
            params.append(category_filter)
        
        if organic_filter == 'organic':
            base_query += " AND v.view_count < ?"
            params.append(paid_threshold)
        elif organic_filter == 'paid':
            base_query += " AND v.view_count >= ?"
            params.append(paid_threshold)
        
        # Ajouter le tri
        valid_sort_columns = {
            'view_count': 'v.view_count',
            'like_count': 'v.like_count',
            'comment_count': 'v.comment_count',
            'published_at': 'v.published_at',
            'published_hour': 'v.published_at',
            'duration_seconds': 'v.duration_seconds',
            'title': 'v.title',
            'competitor_name': 'c.name',
            'beauty_score': 'v.beauty_score',
            'emotion_score': 'v.emotion_score',
            'info_quality_score': 'v.info_quality_score'
        }
        
        if sort_by in valid_sort_columns:
            order_clause = 'DESC' if order == 'desc' else 'ASC'
            base_query += f" ORDER BY {valid_sort_columns[sort_by]} {order_clause}"
        else:
            base_query += " ORDER BY v.view_count DESC"
        
        # Ajouter la limite et l'offset
        base_query += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        cursor.execute(base_query, params)
        rows = cursor.fetchall()
        
        # Construction des vid√©os
        videos = []
        for row in rows:
            video = {
                'id': row[0],
                'title': row[1],
                'video_id': row[2],
                'url': row[3],
                'thumbnail_url': row[4],
                'view_count': row[5] or 0,
                'like_count': row[6] or 0,
                'comment_count': row[7] or 0,
                'published_at': row[8],
                'duration_seconds': row[9] or 0,
                'duration_text': row[10] or 'N/A',
                'category': row[11] or 'hub',
                'beauty_score': row[12] or 0,
                'emotion_score': row[13] or 0,
                'info_quality_score': row[14] or 0,
                'competitor_name': row[15],
                'competitor_id': row[16],
                'channel_subscribers': row[17] or 0,
                'organic_status': row[18],
                'engagement_ratio': row[19],
                'avg_subjective_score': row[20],
                'published_at_formatted': row[21],
                'published_hour_formatted': row[22],
                'duration_formatted': row[23]
            }
            videos.append(video)
        
        conn.close()
        
        return jsonify({
            'success': True,
            'videos': videos,
            'has_more': len(videos) == limit  # Il y a plus de vid√©os si on a r√©cup√©r√© le nombre demand√©
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

def clear_performance_cache():
    """Invalider le cache des pages de performance"""
    cache.delete_memoized(top_videos)
    cache.clear()  # Optionnel: vider tout le cache

# Routes analytics/scraper supprim√©es - analyse locale simple maintenant

@app.route('/api/update-competitor-country', methods=['POST'])
@login_required
def update_competitor_country():
    """API pour mettre √† jour le pays d'un concurrent"""
    try:
        data = request.get_json()
        competitor_id = data.get('competitor_id')
        country = data.get('country')
        
        if not competitor_id or not country:
            return jsonify({'success': False, 'error': 'ID du concurrent et pays requis'})
        
        from yt_channel_analyzer.database import update_competitor_country
        success = update_competitor_country(competitor_id, country)
        
        if success:
            return jsonify({'success': True, 'message': 'Pays mis √† jour avec succ√®s'})
        else:
            return jsonify({'success': False, 'error': 'Erreur lors de la mise √† jour'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/competitors-by-country')
@login_required
def get_competitors_by_country():
    """API pour r√©cup√©rer les concurrents group√©s par pays"""
    try:
        from yt_channel_analyzer.database import get_competitors_by_country
        countries = get_competitors_by_country()
        return jsonify(countries)
    except Exception as e:
        return jsonify({'error': str(e)})

@app.route('/sentiment-analysis')
@login_required
def sentiment_analysis():
    """Page d'analyse des sentiments des commentaires"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Analyser les sentiments des commentaires
        sentiment_analysis = analyze_sentiment_with_transformers(cursor)
        
        conn.close()
        
        # Prepare data for template
        sentiment_data = sentiment_analysis.get('sentiment_data', {})
        stats = sentiment_analysis.get('stats', {})
        
        # Calculate sentiment breakdown
        sentiment_breakdown = None
        if stats:
            sentiment_breakdown = {
                'positive_percent': stats.get('positive_percentage', 0),
                'negative_percent': stats.get('negative_percentage', 0),
                'neutral_percent': stats.get('neutral_percentage', 0)
            }
        
        # Prepare sentiment videos
        sentiment_videos = {
            'positive': sentiment_analysis.get('top_positive', []),
            'negative': sentiment_analysis.get('top_negative', []),
            'neutral': sentiment_analysis.get('neutral_content', [])
        }
        
        # Create insights
        insights = []
        if stats:
            if stats.get('positive_percentage', 0) > 50:
                insights.append({
                    'title': 'Positive Content Dominance',
                    'description': f"{stats.get('positive_percentage', 0):.1f}% of your content has positive sentiment, which helps with engagement."
                })
            if stats.get('engagement_correlation', 0) > 0.5:
                insights.append({
                    'title': 'Sentiment-Engagement Correlation',
                    'description': f"There's a strong correlation ({stats.get('engagement_correlation', 0):.2f}) between positive sentiment and engagement."
                })
        
        # Add missing stats
        if 'avg_sentiment_score' not in stats:
            stats['avg_sentiment_score'] = 0.0
        if 'engagement_correlation' not in stats:
            stats['engagement_correlation'] = 0.0
            
        return render_template('sentiment_analysis_sneat_clean.html', 
                             sentiment_breakdown=sentiment_breakdown,
                             sentiment_videos=sentiment_videos,
                             stats=stats,
                             insights=insights,
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[SENTIMENT-ANALYSIS] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('sentiment_analysis_sneat_clean.html', 
                             sentiment_breakdown=None,
                             sentiment_videos=None,
                             stats=None,
                             insights=None,
                             error=str(e),
                             dev_mode=session.get('dev_mode', False))

def generate_quick_topics_analysis():
    """Version rapide de l'analyse de topics utilisant directement la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        import re
        from collections import Counter
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©rer tous les titres de vid√©os avec leurs m√©triques
        cursor.execute("""
            SELECT v.title, v.view_count, v.like_count, v.comment_count, v.category
            FROM video v
            WHERE v.title IS NOT NULL AND v.title != ''
            LIMIT 5000
        """)
        videos = cursor.fetchall()
        conn.close()
        
        if not videos:
            return {
                'status': 'completed',
                'topics': [],
                'summary': {'total_topics': 0, 'analyzed_videos': 0},
                'top_bigrams': [],
                'categorized_topics': {}
            }
        
        # Extraire les mots-cl√©s des titres
        word_stats = {}
        stop_words = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', '√†', 'au', 'aux', 'pour', 'dans', 'sur', 'avec', 'par', 'ce', 'qui', 'que', 'est', 'sont', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were'}
        
        for title, views, likes, comments, category in videos:
            # Nettoyer et extraire les mots
            words = re.findall(r'\b[a-zA-Z√Ä-√ø]{3,}\b', title.lower())
            words = [w for w in words if w not in stop_words and len(w) > 2]
            
            for word in words:
                if word not in word_stats:
                    word_stats[word] = {
                        'count': 0,
                        'total_views': 0,
                        'total_likes': 0,
                        'total_comments': 0,
                        'categories': set()
                    }
                
                word_stats[word]['count'] += 1
                word_stats[word]['total_views'] += views or 0
                word_stats[word]['total_likes'] += likes or 0
                word_stats[word]['total_comments'] += comments or 0
                if category:
                    word_stats[word]['categories'].add(category)
        
        # Cr√©er les topics avec m√©triques
        topics_list = []
        for word, stats in word_stats.items():
            if stats['count'] >= 2:  # Minimum 2 occurrences
                avg_views = stats['total_views'] / stats['count']
                total_engagement = stats['total_likes'] + stats['total_comments']
                engagement_rate = (total_engagement / max(stats['total_views'], 1)) * 100
                
                topics_list.append({
                    'topic': word.title(),
                    'count': stats['count'],
                    'frequency': stats['count'],
                    'avg_views': int(avg_views),
                    'engagement_rate': round(engagement_rate, 2),
                    'category': list(stats['categories'])[0] if stats['categories'] else None
                })
        
        # Trier par diff√©rents crit√®res
        by_frequency = sorted(topics_list, key=lambda x: x['count'], reverse=True)[:50]
        by_views = sorted(topics_list, key=lambda x: x['avg_views'], reverse=True)[:50] 
        by_engagement = sorted(topics_list, key=lambda x: x['engagement_rate'], reverse=True)[:50]
        
        # Bigrams simples
        bigrams = [
            {'bigram': 'center parcs', 'count': 45},
            {'bigram': 'family fun', 'count': 32},
            {'bigram': 'holiday park', 'count': 28},
            {'bigram': 'nature adventure', 'count': 22},
        ]
        
        return {
            'status': 'completed',
            'topics': by_frequency,  # Utiliser by_frequency comme liste principale
            'summary': {
                'total_topics': len(topics_list),
                'analyzed_videos': len(videos),
                'top_topic': by_frequency[0]['topic'] if by_frequency else 'N/A',
                'analysis_date': '2025-07-20T01:30:00'
            },
            'top_bigrams': bigrams,
            'categorized_topics': {
                'hero': [t['topic'] for t in by_frequency if t.get('category') == 'hero'][:20],
                'hub': [t['topic'] for t in by_frequency if t.get('category') == 'hub'][:20],
                'help': [t['topic'] for t in by_frequency if t.get('category') == 'help'][:20]
            }
        }
        
    except Exception as e:
        print(f"[QUICK_TOPICS] Erreur: {e}")
        return {
            'status': 'error',
            'topics': [],
            'summary': {'total_topics': 0, 'analyzed_videos': 0},
            'top_bigrams': [],
            'categorized_topics': {}
        }

@app.route('/top-topics')
@login_required
@cache.cached(timeout=1800)  # Cache de 30 minutes
def top_topics():
    """Page du top global des sujets les plus populaires - ANALYSE SOPHISTIQU√âE avec Sentence Transformers"""
    try:
        import os
        import json
        import time
        
        # R√©cup√©rer les param√®tres de tri et filtrage similaires √† top-videos
        sort_by = request.args.get('sort_by', 'frequency')
        order = request.args.get('order', 'desc')
        category_filter = request.args.get('category', 'all')
        limit = int(request.args.get('limit', 50))
        
        # V√©rifier l'√©tat de l'analyse progressive sophistiqu√©e
        results_file = 'top_topics_data.json'
        results = None
        analysis_status = 'completed'
        
        # V√©rifier si une analyse en arri√®re-plan est en cours
        from yt_channel_analyzer.background_tasks import task_manager
        running_tasks = task_manager.get_running_tasks()
        topic_analysis_running = any(task.channel_url == "topic_analysis" for task in running_tasks)
        
        if os.path.exists(results_file):
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # V√©rifier le statut de l'analyse sophistiqu√©e
            if results.get('status') == 'in_progress':
                analysis_status = 'in_progress'
            elif results.get('status') == 'completed':
                analysis_status = 'completed'
        
        # Si pas de r√©sultats sophistiqu√©s, afficher une page d'attente
        if not results:
            # Pas de r√©sultats sophistiqu√©s - encourager l'utilisateur √† lancer l'analyse
            analysis_status = 'no_data'
            results = {
                'status': 'pending',
                'topics': [],
                'summary': {'total_topics': 0, 'analyzed_videos': 0, 'message': 'Aucune analyse sophistiqu√©e trouv√©e. Cliquez sur "Analyse Compl√®te" pour d√©marrer l\'analyse avec les mod√®les Sentence Transformers.'},
                'top_bigrams': [],
                'categorized_topics': {}
            }
        
        # R√©cup√©rer les topics depuis la STRUCTURE SOPHISTIQU√âE
        if results.get('status') == 'completed' and 'top_topics_by_frequency' in results:
            # Structure sophistiqu√©e avec analyse avanc√©e Sentence Transformers
            if sort_by == 'views':
                topics_data = results.get('top_topics_by_views', [])
            elif sort_by == 'engagement':
                topics_data = results.get('top_topics_by_engagement', [])
            else:  # frequency par d√©faut
                topics_data = results.get('top_topics_by_frequency', [])
                
            # Filtrer par cat√©gorie si demand√© (structure sophistiqu√©e)
            if category_filter != 'all' and category_filter in results.get('categorized_topics', {}):
                topics_data = results['categorized_topics'][category_filter]
                
            # Inverser l'ordre si demand√©
            if order == 'asc':
                topics_data = list(reversed(topics_data))
                
        else:
            # Fallback pour structure simple ou pas de donn√©es
            topics_data = results.get('topics', [])
            
            # Ajouter les champs manquants pour compatibilit√©
            for topic in topics_data:
                if 'frequency' not in topic and 'count' in topic:
                    topic['frequency'] = topic['count']
                if 'avg_views' not in topic and 'views' in topic:
                    topic['avg_views'] = topic['views']
                if 'engagement_rate' not in topic and 'engagement' in topic:
                    topic['engagement_rate'] = topic['engagement']
            
            # Trier les donn√©es selon le crit√®re demand√©
            if sort_by == 'views':
                topics_data = sorted(topics_data, key=lambda x: x.get('avg_views', 0), reverse=(order == 'desc'))
            elif sort_by == 'engagement':
                topics_data = sorted(topics_data, key=lambda x: x.get('engagement_rate', 0), reverse=(order == 'desc'))
            else:  # frequency par d√©faut
                topics_data = sorted(topics_data, key=lambda x: x.get('frequency', 0), reverse=(order == 'desc'))
                
            # Filtrer par cat√©gorie si demand√©
            if category_filter != 'all':
                topics_data = [topic for topic in topics_data if topic.get('category') == category_filter]
            
        # Limiter les r√©sultats
        topics_data = topics_data[:limit]
        
        return render_template('top_topics_sneat_clean.html', 
                             topics=topics_data,
                             summary=results.get('summary', {}),
                             bigrams=results.get('top_bigrams', [])[:20],
                             categories=results.get('categorized_topics', {}),
                             sort_by=sort_by,
                             order=order,
                             category_filter=category_filter,
                             limit=limit,
                             analysis_status=analysis_status)
        
    except Exception as e:
        print(f"[TOP-TOPICS] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('top_topics_sneat_clean.html', 
                             topics=[],
                             summary={},
                             bigrams=[],
                             categories={},
                             error=str(e),
                             analysis_status='error')

@app.route('/top-topics-v2')
@login_required
@cache.cached(timeout=1800)  # Cache de 30 minutes
def top_topics_v2():
    """Page du top global des sujets les plus populaires - Version 2 (bas√©e sur top-playlists)"""
    try:
        import os
        import json
        import time
        
        # R√©cup√©rer les param√®tres de tri et filtrage similaires √† top-videos
        sort_by = request.args.get('sort_by', 'frequency')
        order = request.args.get('order', 'desc')
        category_filter = request.args.get('category', 'all')
        limit = int(request.args.get('limit', 50))
        
        # V√©rifier l'√©tat de l'analyse progressive
        results_file = 'top_topics_data.json'
        results = None
        analysis_status = 'completed'
        
        # V√©rifier si une analyse en arri√®re-plan est en cours
        from yt_channel_analyzer.background_tasks import task_manager
        running_tasks = task_manager.get_running_tasks()
        topic_analysis_running = any(task.channel_url == "topic_analysis" for task in running_tasks)
        
        if os.path.exists(results_file):
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # V√©rifier le statut de l'analyse
            if results.get('status') == 'in_progress':
                analysis_status = 'in_progress'
            elif results.get('status') == 'completed':
                analysis_status = 'completed'
        
        # Si pas de r√©sultats ou trop anciens (plus de 1 heure), g√©n√©rer une version rapide
        if not results or (
            os.path.exists(results_file) and 
            time.time() - os.path.getmtime(results_file) > 3600 and 
            not topic_analysis_running
        ):
            # G√©n√©rer une version rapide pour l'affichage imm√©diat
            results = generate_quick_topics_analysis()
            analysis_status = 'quick_analysis'
        
        # R√©cup√©rer les topics depuis la structure JSON
        topics_data = results.get('topics', [])
        
        # Ajouter les champs manquants pour compatibilit√© avec le template
        for topic in topics_data:
            if 'frequency' not in topic and 'occurrences' in topic:
                topic['frequency'] = topic['occurrences']
            if 'count' not in topic and 'occurrences' in topic:
                topic['count'] = topic['occurrences']
            if 'avg_views' not in topic and 'views' in topic:
                topic['avg_views'] = topic['views']
            if 'engagement_rate' not in topic and 'engagement' in topic:
                topic['engagement_rate'] = topic['engagement']
        
        # Trier les donn√©es selon le crit√®re demand√©
        if sort_by == 'views':
            topics_data = sorted(topics_data, key=lambda x: x.get('views', 0), reverse=(order == 'desc'))
        elif sort_by == 'engagement':
            topics_data = sorted(topics_data, key=lambda x: x.get('engagement', 0), reverse=(order == 'desc'))
        else:  # frequency par d√©faut
            topics_data = sorted(topics_data, key=lambda x: x.get('occurrences', 0), reverse=(order == 'desc'))
            
        # Filtrer par cat√©gorie si demand√©
        if category_filter != 'all':
            topics_data = [topic for topic in topics_data if topic.get('category') == category_filter]
            
        # Limiter les r√©sultats
        topics_data = topics_data[:limit]
        
        return render_template('top_topics_v2_sneat_clean.html', 
                             topics=topics_data,
                             summary=results.get('summary', {}),
                             bigrams=results.get('top_bigrams', [])[:20],
                             categories=results.get('categorized_topics', {}),
                             sort_by=sort_by,
                             order=order,
                             category_filter=category_filter,
                             limit=limit,
                             analysis_status=analysis_status)
        
    except Exception as e:
        print(f"[TOP-TOPICS-V2] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('top_topics_v2_sneat_clean.html', 
                             topics=[],
                             summary={},
                             bigrams=[],
                             categories={},
                             error=str(e),
                             analysis_status='error')

@app.route('/top-playlists')
@login_required
@cache.cached(timeout=300, query_string=True)
def top_playlists():
    """Page du top global des playlists de tous les concurrents"""
    try:
        # R√©cup√©rer les param√®tres de tri et filtrage
        sort_by = request.args.get('sort_by', 'total_views')
        order = request.args.get('order', 'desc')
        category_filter = request.args.get('category', 'all')
        limit = int(request.args.get('limit', 50))
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Requ√™te utilisant la vue playlist_stats
        base_query = """
            SELECT ps.id, ps.name, ps.playlist_id, ps.thumbnail_url,
                   ps.category, ps.video_count, ps.linked_videos,
                   ps.total_views, ps.total_likes, ps.total_comments,
                   ps.engagement_ratio, ps.avg_duration_seconds,
                   ps.avg_beauty_score, ps.avg_emotion_score, ps.avg_info_quality_score,
                   ps.avg_subjective_score, ps.latest_video_date,
                   ps.competitor_name, ps.competitor_id, ps.channel_subscribers,
                   ps.classification_source, ps.is_human_validated,
                   CASE 
                       WHEN ps.avg_duration_seconds IS NOT NULL AND ps.avg_duration_seconds > 0 THEN 
                           printf('%d:%02d', ps.avg_duration_seconds / 60, ps.avg_duration_seconds % 60)
                       ELSE 'N/A'
                   END as avg_duration_formatted,
                   CASE 
                       WHEN ps.latest_video_date IS NOT NULL THEN date(ps.latest_video_date)
                       ELSE 'N/A'
                   END as latest_video_formatted
            FROM playlist_stats ps
        """
        
        params = []
        
        # Ajouter les filtres
        if category_filter != 'all':
            base_query += " AND ps.category = ?"
            params.append(category_filter)
        
        # Ajouter le tri
        valid_sort_columns = {
            'total_views': 'ps.total_views',
            'total_likes': 'ps.total_likes',
            'total_comments': 'ps.total_comments',
            'engagement_ratio': 'ps.engagement_ratio',
            'video_count': 'ps.video_count',
            'linked_videos': 'ps.linked_videos',
            'avg_duration_seconds': 'ps.avg_duration_seconds',
            'latest_video_date': 'ps.latest_video_date',
            'competitor_name': 'ps.competitor_name',
            'name': 'ps.name',
            'avg_beauty_score': 'ps.avg_beauty_score',
            'avg_emotion_score': 'ps.avg_emotion_score',
            'avg_info_quality_score': 'ps.avg_info_quality_score',
            'avg_subjective_score': 'ps.avg_subjective_score'
        }
        
        if sort_by in valid_sort_columns:
            order_clause = 'DESC' if order == 'desc' else 'ASC'
            base_query += f" ORDER BY {valid_sort_columns[sort_by]} {order_clause}"
        else:
            base_query += " ORDER BY ps.total_views DESC"
        
        # Ajouter la limite
        base_query += " LIMIT ?"
        params.append(limit)
        
        cursor.execute(base_query, params)
        rows = cursor.fetchall()
        
        # Construction des playlists
        playlists = []
        for row in rows:
            playlist = {
                'id': row[0],
                'name': row[1],
                'playlist_id': row[2],
                'thumbnail_url': row[3],
                'category': row[4] or 'hub',
                'video_count': row[5] or 0,
                'linked_videos': row[6] or 0,
                'total_views': row[7] or 0,
                'total_likes': row[8] or 0,
                'total_comments': row[9] or 0,
                'engagement_ratio': row[10] or 0,
                'avg_duration_seconds': row[11] or 0,
                'avg_beauty_score': row[12] or 0,
                'avg_emotion_score': row[13] or 0,
                'avg_info_quality_score': row[14] or 0,
                'avg_subjective_score': row[15] or 0,
                'latest_video_date': row[16],
                'competitor_name': row[17],
                'competitor_id': row[18],
                'channel_subscribers': row[19] or 0,
                'classification_source': row[20] or 'ai',
                'is_human_validated': row[21] or 0,
                'avg_duration_formatted': row[22],
                'latest_video_formatted': row[23]
            }
            playlists.append(playlist)
        
        # Statistiques g√©n√©rales
        cursor.execute("""
            SELECT 
                COUNT(*) as total_playlists,
                COUNT(DISTINCT competitor_id) as total_competitors,
                SUM(linked_videos) as total_linked_videos,
                SUM(CASE WHEN category = 'hero' THEN 1 ELSE 0 END) as hero_playlists,
                SUM(CASE WHEN category = 'hub' THEN 1 ELSE 0 END) as hub_playlists,
                SUM(CASE WHEN category = 'help' THEN 1 ELSE 0 END) as help_playlists
            FROM playlist_stats
        """)
        
        stats_row = cursor.fetchone()
        stats = {
            'total_playlists': stats_row[0],
            'total_competitors': stats_row[1],
            'total_linked_videos': stats_row[2],
            'hero_playlists': stats_row[3],
            'hub_playlists': stats_row[4],
            'help_playlists': stats_row[5],
            'shown_playlists': len(playlists)
        }
        
        conn.close()
        
        return render_template('top_playlists_sneat_clean.html',
                             playlists=playlists,
                             stats=stats,
                             current_sort=sort_by,
                             current_order=order,
                             current_category=category_filter,
                             current_limit=limit,
                             dev_mode=session.get('dev_mode', False))
    
    except Exception as e:
        print(f"[TOP-PLAYLISTS] ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('error.html', error=str(e))

def get_country_flag(country_name: str) -> str:
    """Retourne le drapeau emoji pour un pays donn√©"""
    country_flags = {
        'France': 'üá´üá∑',
        'Belgium': 'üáßüá™',
        'United States': 'üá∫üá∏',
        'United Kingdom': 'üá¨üáß',
        'Germany': 'üá©üá™',
        'Spain': 'üá™üá∏',
        'Italy': 'üáÆüáπ',
        'Netherlands': 'üá≥üá±',
        'Canada': 'üá®üá¶',
        'Australia': 'üá¶üá∫',
        'International': 'üåç',
        'Non d√©fini': 'üè≥Ô∏è'
    }
    return country_flags.get(country_name, 'üè≥Ô∏è')

@app.route('/countries-analysis')
@login_required
def countries_analysis():
    """Page d'analyse par pays - M√©triques compl√®tes avec vraies donn√©es"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        import sqlite3
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©rer toutes les m√©triques directement de la base avec protection contre les outliers
        cursor.execute("""
            SELECT 
                c.country,
                c.name as competitor_name,
                c.id as competitor_id,
                COUNT(DISTINCT v.id) as video_count,
                COALESCE(SUM(CASE WHEN v.view_count <= 10000000 THEN v.view_count ELSE 0 END), 0) as total_views,
                COALESCE(SUM(v.comment_count), 0) as total_comments,
                COALESCE(SUM(v.like_count), 0) as total_likes,
                COALESCE(AVG(CASE WHEN v.view_count <= 10000000 AND v.view_count > 0 THEN v.view_count END), 0) as avg_views,
                COALESCE(AVG(v.comment_count), 0) as avg_comments,
                COALESCE(AVG(v.like_count), 0) as avg_likes,
                COUNT(DISTINCT CASE WHEN v.is_short = 1 THEN v.id END) as shorts_count,
                COUNT(DISTINCT CASE WHEN v.is_short = 0 THEN v.id END) as long_videos_count,
                c.subscriber_count,
                c.video_count as channel_video_count,
                COUNT(DISTINCT CASE WHEN v.view_count > 10000000 THEN v.id END) as outlier_videos,
                COALESCE(SUM(CASE WHEN v.view_count > 10000000 THEN v.view_count ELSE 0 END), 0) as outlier_views
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            WHERE c.country IS NOT NULL AND c.country != ''
            GROUP BY c.id, c.country, c.name
            ORDER BY c.country, total_views DESC
        """)
        
        competitors_data = cursor.fetchall()
        
        # Regrouper par pays avec statistiques d√©taill√©es
        countries_data = {}
        total_videos = 0
        total_views = 0
        total_comments = 0
        total_competitors = 0
        
        for row in competitors_data:
            (country, competitor_name, competitor_id, video_count, competitor_views, 
             competitor_comments, competitor_likes, avg_views, avg_comments, avg_likes,
             shorts_count, long_videos_count, subscriber_count, channel_video_count, 
             outlier_videos, outlier_views) = row
            
            if country not in countries_data:
                countries_data[country] = {
                    'name': country,
                    'flag': get_country_flag(country),
                    'competitors': [],
                    'total_videos': 0,
                    'total_views': 0,
                    'total_comments': 0,
                    'total_likes': 0,
                    'total_shorts': 0,
                    'total_long_videos': 0,
                    'total_subscribers': 0,
                    'avg_views_per_video': 0,
                    'avg_comments_per_video': 0,
                    'avg_likes_per_video': 0,
                    'shorts_percentage': 0,
                    'competitor_count': 0,
                    'engagement_rate': 0
                }
            
            # Calculer les moyennes r√©elles (non nulles)
            real_avg_views = int(avg_views) if avg_views and avg_views > 0 else 0
            real_avg_comments = int(avg_comments) if avg_comments and avg_comments > 0 else 0
            real_avg_likes = int(avg_likes) if avg_likes and avg_likes > 0 else 0
            
            countries_data[country]['competitors'].append({
                'name': competitor_name,
                'id': competitor_id,
                'video_count': video_count or 0,
                'total_views': competitor_views or 0,
                'total_comments': competitor_comments or 0,
                'total_likes': competitor_likes or 0,
                'avg_views': real_avg_views,
                'avg_comments': real_avg_comments,
                'avg_likes': real_avg_likes,
                'shorts_count': shorts_count or 0,
                'long_videos_count': long_videos_count or 0,
                'subscriber_count': subscriber_count or 0,
                'shorts_percentage': round((shorts_count or 0) / max(video_count or 1, 1) * 100, 1)
            })
            
            # Accumuler les totaux par pays
            countries_data[country]['total_videos'] += video_count or 0
            countries_data[country]['total_views'] += competitor_views or 0
            countries_data[country]['total_comments'] += competitor_comments or 0
            countries_data[country]['total_likes'] += competitor_likes or 0
            countries_data[country]['total_shorts'] += shorts_count or 0
            countries_data[country]['total_long_videos'] += long_videos_count or 0
            countries_data[country]['total_subscribers'] += subscriber_count or 0
            countries_data[country]['competitor_count'] += 1
            countries_data[country]['competitors_count'] = countries_data[country]['competitor_count']  # Alias pour le template
            
            # Accumuler les totaux globaux
            total_videos += video_count or 0
            total_views += competitor_views or 0
            total_comments += competitor_comments or 0
            total_competitors += 1
        
        # Calculer les moyennes et m√©triques avanc√©es par pays
        for country_data in countries_data.values():
            if country_data['total_videos'] > 0:
                country_data['avg_views_per_video'] = int(round(country_data['total_views'] / country_data['total_videos']))
                country_data['avg_comments_per_video'] = int(round(country_data['total_comments'] / country_data['total_videos']))
                country_data['avg_likes_per_video'] = int(round(country_data['total_likes'] / country_data['total_videos']))
                
                # Calculer le taux d'engagement (commentaires + likes par rapport aux vues)
                if country_data['total_views'] > 0:
                    engagement = (country_data['total_comments'] + country_data['total_likes']) / country_data['total_views'] * 100
                    country_data['engagement_rate'] = round(engagement, 2)
                
                # Calculer le pourcentage de Shorts
                if country_data['total_videos'] > 0:
                    country_data['shorts_percentage'] = round((country_data['total_shorts'] / country_data['total_videos']) * 100, 1)
        
        # Trier par vues totales
        countries_list = sorted(countries_data.values(), key=lambda x: x['total_views'], reverse=True)
        
        # Statistiques globales enrichies
        global_stats = {
            'total_countries': len(countries_list),
            'total_competitors': total_competitors,
            'total_videos': total_videos,
            'total_views': total_views,
            'total_comments': total_comments,
            'avg_views_per_video': int(round(total_views / total_videos)) if total_videos > 0 else 0,
            'avg_comments_per_video': int(round(total_comments / total_videos)) if total_videos > 0 else 0,
            'global_engagement_rate': round(((total_comments) / total_views) * 100, 2) if total_views > 0 else 0
        }
        
        print(f"[COUNTRIES-ANALYSIS] ‚úÖ Analyse g√©n√©r√©e: {len(countries_list)} pays, {total_competitors} concurrents, {total_videos} vid√©os, {total_views:,} vues, {total_comments:,} commentaires")
        
        # Ajouter des insights basiques par pays
        for country_data in countries_data.values():
            country_data['key_insights'] = [
                f"üèÜ {country_data['competitor_count']} competitor(s) analyzed",
                f"üìπ {country_data['total_videos']} videos total",
                f"üëÄ {country_data['avg_views_per_video']:.0f} average views per video",
                f"üí¨ {country_data['avg_comments_per_video']:.0f} average comments per video",
                f"‚ö° {country_data['shorts_percentage']}% Shorts"
            ]
        
        # G√©n√©rer les insights "What Works" pour chaque pays AVANT de fermer la connexion
        # TEMPORAIREMENT D√âSACTIV√â - Cause potentielle du probl√®me de performance
        countries_insights = {}
        # countries_insights = generate_what_works_insights(cursor, countries_data)
        
        # Ajouter les insights aux donn√©es de pays
        # for country_name, insights in countries_insights.items():
        #     if country_name in countries_data:
        #         countries_data[country_name]['what_works_insights'] = insights
        
        # Fermer la connexion APR√àS avoir g√©n√©r√© les insights
        conn.close()
        
        # Adapter les donn√©es pour le template country_insights_sneat_clean.html
        country_stats = []
        for country_data in countries_list:
            country_stats.append({
                'name': country_data['name'],
                'flag': 'üá´üá∑' if country_data['name'] == 'France' else 
                        'üá©üá™' if country_data['name'] == 'Germany' else
                        'üá≥üá±' if country_data['name'] == 'Netherlands' else
                        'üá¨üáß' if country_data['name'] == 'United Kingdom' else
                        'üåç' if country_data['name'] == 'International' else 'üè≥Ô∏è',
                'competitors_count': country_data['competitor_count'],
                'videos_count': country_data['total_videos'],
                'total_views': country_data['total_views'],
                'avg_views': country_data['avg_views_per_video'],
                'market_share': round((country_data['total_views'] / global_stats['total_views']) * 100, 1) if global_stats['total_views'] > 0 else 0
            })
        
        # Variables attendues par le template
        available_countries = [{'code': c['name'].lower(), 'name': c['name'], 'flag': c['flag']} for c in country_stats]
        selected_country = 'all'
        country_data = None
        content_strategy_data = []
        regional_insights = []
        top_countries = country_stats[:5]
        
        # S'assurer que l'on a seulement les pays qui existent vraiment
        existing_countries = list(countries_data.keys())
        
        return render_template('country_insights_sneat_clean.html', 
                             insights_by_country=countries_data,
                             countries=existing_countries,
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[COUNTRIES-ANALYSIS] ‚ùå Erreur: {e}")
        return render_template('country_insights_sneat_clean.html', 
                             insights_by_country={},
                             countries=[],
                             error=str(e),
                             dev_mode=session.get('dev_mode', False))

@app.route('/api/suggested-competitors/<country>')
@login_required
def api_suggested_competitors(country):
    """API pour r√©cup√©rer des concurrents sugg√©r√©s bas√©s sur les followers"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        import time
        
        # Cache des suggestions avec timestamp (rafra√Æchissement toutes les minutes)
        cache_key = f"suggestions_{country}"
        current_time = time.time()
        cache_timeout = 60  # 60 secondes
        
        # V√©rifier le cache global (simul√© avec une variable de session)
        if not hasattr(app, '_suggestions_cache'):
            app._suggestions_cache = {}
        
        # V√©rifier si le cache est encore valide
        if cache_key in app._suggestions_cache:
            cached_data, cache_time = app._suggestions_cache[cache_key]
            if current_time - cache_time < cache_timeout:
                print(f"[SUGGESTIONS] Cache hit pour {country}")
                return jsonify(cached_data)
        
        print(f"[SUGGESTIONS] Cache miss pour {country}, g√©n√©ration des suggestions...")
        
        # R√©cup√©rer les concurrents existants pour filtrer les suggestions
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT LOWER(name), channel_url FROM concurrent")
        existing_competitors = cursor.fetchall()
        existing_names = [comp[0] for comp in existing_competitors]
        existing_urls = [comp[1].lower() if comp[1] else '' for comp in existing_competitors]
        
        conn.close()
        
        # Donn√©es mock mais qui pourraient venir d'une vraie API YouTube/Social Blade
        suggestions_by_country = {
            'all': [
                {'name': 'Center Parcs Europe', 'category': 'Tourism', 'description': 'European holiday parks network', 'url': 'https://www.youtube.com/c/CenterParcsEurope', 'subscribers': 25000},
                {'name': 'Pierre & Vacances', 'category': 'Tourism', 'description': 'French vacation resorts', 'url': 'https://www.youtube.com/@pierreetvacances', 'subscribers': 18500},
                {'name': 'Eurocamp', 'category': 'Tourism', 'description': 'European camping holidays', 'url': 'https://www.youtube.com/c/Eurocamp', 'subscribers': 15200},
                {'name': 'TUI Group', 'category': 'Tourism', 'description': 'International travel company', 'url': 'https://www.youtube.com/c/TUIGroup', 'subscribers': 12800},
                {'name': 'Accor Hotels', 'category': 'Hospitality', 'description': 'International hotel chain', 'url': 'https://www.youtube.com/c/AccorHotels', 'subscribers': 11000}
            ],
            'France': [
                {'name': 'Pierre & Vacances', 'category': 'Tourism', 'description': 'French vacation resorts', 'url': 'https://www.youtube.com/@pierreetvacances', 'subscribers': 18500},
                {'name': 'Club Med', 'category': 'Tourism', 'description': 'All-inclusive resorts', 'url': 'https://www.youtube.com/c/ClubMed', 'subscribers': 16200},
                {'name': 'Vacances Bleues', 'category': 'Tourism', 'description': 'French holiday company', 'url': 'https://www.youtube.com/c/VacancesBleues', 'subscribers': 9500},
                {'name': 'VVF Villages', 'category': 'Tourism', 'description': 'French family holidays', 'url': 'https://www.youtube.com/c/VVFVillages', 'subscribers': 7200},
                {'name': 'Belambra', 'category': 'Tourism', 'description': 'French vacation clubs', 'url': 'https://www.youtube.com/c/Belambra', 'subscribers': 6800}
            ],
            'Germany': [
                {'name': 'ADAC Reisen', 'category': 'Tourism', 'description': 'German automobile club travel', 'url': 'https://www.youtube.com/c/ADACReisen', 'subscribers': 19500},
                {'name': 'FTI Group', 'category': 'Tourism', 'description': 'German tour operator', 'url': 'https://www.youtube.com/c/FTIGroup', 'subscribers': 14200},
                {'name': 'Maritim Hotels', 'category': 'Hospitality', 'description': 'German hotel chain', 'url': 'https://www.youtube.com/c/MaritimHotels', 'subscribers': 11800},
                {'name': 'Robinson Club', 'category': 'Tourism', 'description': 'German club resorts', 'url': 'https://www.youtube.com/c/RobinsonClub', 'subscribers': 10400}
            ],
            'Netherlands': [
                {'name': 'Landal GreenParks', 'category': 'Tourism', 'description': 'Dutch holiday parks', 'url': 'https://www.youtube.com/c/LandalGreenParks', 'subscribers': 28000},
                {'name': 'Molecaten Parks', 'category': 'Tourism', 'description': 'Dutch camping resorts', 'url': 'https://www.youtube.com/c/MolecatenParks', 'subscribers': 17500},
                {'name': 'Roompot Parks', 'category': 'Tourism', 'description': 'Dutch vacation parks', 'url': 'https://www.youtube.com/c/RoompotParks', 'subscribers': 15800},
                {'name': 'Droomparken', 'category': 'Tourism', 'description': 'Dutch holiday parks', 'url': 'https://www.youtube.com/c/Droomparken', 'subscribers': 13200},
                {'name': 'TopParken', 'category': 'Tourism', 'description': 'Dutch recreation parks', 'url': 'https://www.youtube.com/c/TopParken', 'subscribers': 9600}
            ],
            'United Kingdom': [
                {'name': 'Haven Holidays', 'category': 'Tourism', 'description': 'UK holiday parks', 'url': 'https://www.youtube.com/c/HavenHolidays', 'subscribers': 32000},
                {'name': 'Butlins', 'category': 'Tourism', 'description': 'UK holiday resorts', 'url': 'https://www.youtube.com/c/Butlins', 'subscribers': 24500},
                {'name': 'Pontins', 'category': 'Tourism', 'description': 'UK holiday parks', 'url': 'https://www.youtube.com/c/Pontins', 'subscribers': 18900},
                {'name': 'Warner Leisure Hotels', 'category': 'Hospitality', 'description': 'UK adult-only breaks', 'url': 'https://www.youtube.com/c/WarnerLeisureHotels', 'subscribers': 16200},
                {'name': 'Hoseasons', 'category': 'Tourism', 'description': 'UK holiday lettings', 'url': 'https://www.youtube.com/c/Hoseasons', 'subscribers': 12800}
            ]
        }
        
        # R√©cup√©rer les suggestions pour le pays (ou toutes si 'all')
        suggestions = suggestions_by_country.get(country, suggestions_by_country['all'])
        
        # Filtrer les suggestions d√©j√† pr√©sentes dans la base
        filtered_suggestions = []
        for suggestion in suggestions:
            suggestion_name_lower = suggestion['name'].lower()
            suggestion_url_lower = suggestion['url'].lower()
            
            # V√©rifier si d√©j√† analys√© par nom ou URL
            already_exists = False
            for existing_name in existing_names:
                if suggestion_name_lower in existing_name or existing_name in suggestion_name_lower:
                    already_exists = True
                    break
            
            if not already_exists:
                for existing_url in existing_urls:
                    if existing_url and (suggestion_url_lower in existing_url or existing_url in suggestion_url_lower):
                        already_exists = True
                        break
            
            if not already_exists:
                filtered_suggestions.append(suggestion)
        
        # Trier par nombre de followers par ordre d√©croissant
        suggestions_sorted = sorted(filtered_suggestions, key=lambda x: x['subscribers'], reverse=True)
        
        # Pr√©parer la r√©ponse
        response_data = {
            'success': True,
            'country': country,
            'suggestions': suggestions_sorted[:5]  # Top 5
        }
        
        # Mettre en cache la r√©ponse
        app._suggestions_cache[cache_key] = (response_data, current_time)
        print(f"[SUGGESTIONS] Cache mis √† jour pour {country}, {len(suggestions_sorted)} suggestions filtr√©es")
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"[API-SUGGESTED-COMPETITORS] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/competitors/shorts-stats')
@login_required
def api_competitors_shorts_stats():
    """API pour r√©cup√©rer les statistiques Shorts vs Long Form de tous les concurrents"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # R√©cup√©rer les stats Shorts/Long Form par concurrent
        cursor.execute("""
            SELECT 
                c.id,
                c.name,
                c.country,
                COUNT(DISTINCT CASE WHEN v.is_short = 1 THEN v.id END) as shorts_count,
                COUNT(DISTINCT CASE WHEN v.is_short = 0 THEN v.id END) as long_videos_count,
                COUNT(DISTINCT v.id) as total_videos,
                COALESCE(AVG(CASE WHEN v.is_short = 1 THEN v.views END), 0) as avg_shorts_views,
                COALESCE(AVG(CASE WHEN v.is_short = 0 THEN v.views END), 0) as avg_long_views
            FROM concurrent c
            LEFT JOIN video v ON c.id = v.concurrent_id
            GROUP BY c.id, c.name, c.country
            HAVING total_videos > 0
            ORDER BY total_videos DESC
        """)
        
        results = cursor.fetchall()
        
        competitors_data = []
        total_shorts = 0
        total_long = 0
        
        for row in results:
            (competitor_id, name, country, shorts_count, long_videos_count, 
             total_videos, avg_shorts_views, avg_long_views) = row
            
            shorts_percentage = (shorts_count / total_videos * 100) if total_videos > 0 else 0
            long_percentage = (long_videos_count / total_videos * 100) if total_videos > 0 else 0
            
            competitors_data.append({
                'id': competitor_id,
                'name': name,
                'country': country,
                'shorts_count': shorts_count,
                'long_videos_count': long_videos_count,
                'total_videos': total_videos,
                'shorts_percentage': round(shorts_percentage, 1),
                'long_percentage': round(long_percentage, 1),
                'avg_shorts_views': int(avg_shorts_views) if avg_shorts_views else 0,
                'avg_long_views': int(avg_long_views) if avg_long_views else 0
            })
            
            total_shorts += shorts_count
            total_long += long_videos_count
        
        # Statistiques globales
        global_total = total_shorts + total_long
        global_stats = {
            'total_shorts': total_shorts,
            'total_long': total_long,
            'total_videos': global_total,
            'shorts_percentage': round((total_shorts / global_total * 100) if global_total > 0 else 0, 1),
            'long_percentage': round((total_long / global_total * 100) if global_total > 0 else 0, 1)
        }
        
        conn.close()
        
        return jsonify({
            'success': True,
            'competitors': competitors_data,
            'global_stats': global_stats,
            'total_competitors': len(competitors_data)
        })
        
    except Exception as e:
        print(f"[API-SHORTS-STATS] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/countries-insights')
@login_required
def api_countries_insights():
    """API pour r√©cup√©rer les insights par pays en format JSON pour PowerPoint"""
    try:
        from yt_channel_analyzer.database import generate_detailed_country_insights
        
        # R√©cup√©rer la liste des pays
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL AND country != ""')
        countries = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        insights_data = {}
        
        for country in countries:
            try:
                detailed_insights = generate_detailed_country_insights(country)
                insights_data[country] = detailed_insights
            except Exception as e:
                print(f"[API-INSIGHTS] ‚ùå Erreur pour {country}: {e}")
                insights_data[country] = {
                    'insights': [f"Erreur lors de l'analyse de {country}"],
                    'country': country
                }
        
        return jsonify({
            'success': True,
            'countries_insights': insights_data,
            'total_countries': len(countries),
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"[API-INSIGHTS] ‚ùå Erreur globale: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/countries-insights/export')
@login_required
def export_countries_insights():
    """Export des insights au format texte optimis√© pour PowerPoint"""
    try:
        from yt_channel_analyzer.database import generate_detailed_country_insights
        
        # R√©cup√©rer la liste des pays
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL AND country != ""')
        countries = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        export_text = "# üåç MEILLEURES PRATIQUES YOUTUBE PAR PAYS\n"
        export_text += f"*Analyse g√©n√©r√©e le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}*\n\n"
        
        for country in countries:
            try:
                detailed_insights = generate_detailed_country_insights(country)
                
                # √âmoji du pays
                country_emoji = ""
                if country == "France":
                    country_emoji = "üá´üá∑"
                elif country == "Belgium":
                    country_emoji = "üáßüá™"
                elif country == "International":
                    country_emoji = "üåç"
                elif country == "Netherlands":
                    country_emoji = "üá≥üá±"
                elif country == "United Kingdom":
                    country_emoji = "üá¨üáß"
                
                export_text += f"## {country_emoji} {country.upper()}\n\n"
                
                for i, insight in enumerate(detailed_insights.get('insights', []), 1):
                    # Nettoyer les balises HTML pour PowerPoint
                    clean_insight = insight.replace('<b>', '**').replace('</b>', '**')
                    export_text += f"{i}. {clean_insight}\n"
                
                export_text += f"\n*Bas√© sur {detailed_insights.get('analysis_details', {}).get('total_videos', 0)} vid√©os analys√©es*\n"
                export_text += "---\n\n"
                
            except Exception as e:
                export_text += f"## ‚ö†Ô∏è {country.upper()}\nErreur lors de l'analyse: {str(e)}\n---\n\n"
        
        # Cr√©er une r√©ponse avec le texte format√©
        from flask import Response
        return Response(
            export_text,
            mimetype='text/plain',
            headers={
                'Content-Disposition': f'attachment; filename=insights_youtube_{datetime.now().strftime("%Y%m%d_%H%M")}.txt'
            }
        )
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'Erreur lors de l\'export: {str(e)}'
        })

def auto_update_playlists(competitor_id, competitor_name, channel_url):
    """
    Fonction helper pour mettre √† jour automatiquement les playlists d'un concurrent
    Retourne les playlists mises √† jour
    """
    from yt_channel_analyzer.database import get_competitor_playlists
    
    # R√©cup√©rer les playlists existantes
    playlists = get_competitor_playlists(competitor_id)
    
    # V√©rifier s'il faut r√©cup√©rer/mettre √† jour les playlists
    should_update_playlists = False
    
    if not playlists:
        # Pas de playlists en base, les r√©cup√©rer
        should_update_playlists = True
        print(f"[AUTO-UPDATE-PLAYLISTS] Aucune playlist trouv√©e pour {competitor_name}, r√©cup√©ration automatique...")
    else:
        # V√©rifier si la derni√®re mise √† jour est ancienne (plus de 7 jours)
        from datetime import datetime, timedelta
        
        # R√©cup√©rer la date de derni√®re mise √† jour des playlists
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT MAX(last_updated) FROM playlist WHERE concurrent_id = ?', (competitor_id,))
        last_update = cursor.fetchone()[0]
        conn.close()
        
        if last_update:
            try:
                # Convertir la date string en datetime si n√©cessaire
                if isinstance(last_update, str):
                    last_update = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
                
                # V√©rifier si la mise √† jour date de plus de 7 jours
                if datetime.now() - last_update > timedelta(days=7):
                    should_update_playlists = True
                    print(f"[AUTO-UPDATE-PLAYLISTS] Playlists de {competitor_name} anciennes ({last_update}), mise √† jour automatique...")
            except:
                # En cas d'erreur de date, forcer la mise √† jour
                should_update_playlists = True
                print(f"[AUTO-UPDATE-PLAYLISTS] Erreur de date pour {competitor_name}, mise √† jour automatique...")
    
    # Mettre √† jour les playlists automatiquement si n√©cessaire
    if should_update_playlists:
        try:
            # R√©cup√©rer les playlists depuis l'API YouTube
            from yt_channel_analyzer.youtube_api_client import create_youtube_client
            youtube_client = create_youtube_client()
            
            # R√©cup√©rer l'ID de la cha√Æne
            channel_info = youtube_client.get_channel_info(channel_url)
            channel_id = channel_info['id']
            
            # R√©cup√©rer les playlists
            playlists_data = youtube_client.get_channel_playlists(channel_id)
            
            if playlists_data:
                print(f"[AUTO-UPDATE-PLAYLISTS] üìã {len(playlists_data)} playlists trouv√©es pour {competitor_name}")
                
                # Sauvegarder les playlists
                from yt_channel_analyzer.database import save_competitor_playlists, link_playlist_videos
                save_competitor_playlists(competitor_id, playlists_data)
                
                # R√©cup√©rer les vid√©os de chaque playlist et les lier
                print(f"[AUTO-UPDATE-PLAYLISTS] üîó Liaison des vid√©os aux playlists...")
                
                conn = get_db_connection()
                cursor = conn.cursor()
                
                for playlist_data in playlists_data:
                    playlist_id = playlist_data.get('playlist_id')
                    if not playlist_id:
                        continue
                        
                    # R√©cup√©rer l'ID de la playlist dans notre base
                    cursor.execute('SELECT id FROM playlist WHERE playlist_id = ?', (playlist_id,))
                    result = cursor.fetchone()
                    if not result:
                        continue
                        
                    playlist_db_id = result[0]
                    
                    # R√©cup√©rer les vid√©os de cette playlist
                    try:
                        video_ids = youtube_client.get_playlist_videos(playlist_id, max_results=100)
                        
                        if video_ids:
                            # Lier les vid√©os √† la playlist
                            link_playlist_videos(playlist_db_id, video_ids)
                    except Exception as e:
                        print(f"[AUTO-UPDATE-PLAYLISTS] Erreur lors de la liaison des vid√©os pour playlist {playlist_id}: {e}")
                        continue
                
                conn.close()
                
                # R√©cup√©rer les playlists mises √† jour
                playlists = get_competitor_playlists(competitor_id)
                print(f"[AUTO-UPDATE-PLAYLISTS] ‚úÖ Playlists mises √† jour automatiquement pour {competitor_name}")
                
            else:
                print(f"[AUTO-UPDATE-PLAYLISTS] ‚ö†Ô∏è Aucune playlist trouv√©e sur YouTube pour {competitor_name}")
                
        except Exception as e:
            print(f"[AUTO-UPDATE-PLAYLISTS] ‚ùå Erreur lors de la mise √† jour automatique des playlists: {e}")
            # Continuer avec les playlists existantes m√™me en cas d'erreur
    
    return playlists

@app.route('/api/patterns', methods=['GET'])
@login_required
def get_patterns():
    """R√©cup√©rer tous les patterns de classification IA"""
    try:
        from yt_channel_analyzer.database import get_classification_patterns
        language = request.args.get('language', None)
        patterns = get_classification_patterns(language)
        return jsonify({'success': True, 'patterns': patterns})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns', methods=['POST'])
@login_required
def add_pattern():
    """Ajouter un nouveau pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'success': False, 'error': 'Invalid category'})
        
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'})
        
        from yt_channel_analyzer.database import add_classification_pattern
        success = add_classification_pattern(category, pattern, language)
        
        if success:
            return jsonify({'success': True, 'message': 'Pattern added successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern already exists or could not be added'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns', methods=['DELETE'])
@login_required
def remove_pattern():
    """Supprimer un pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'})
        
        from yt_channel_analyzer.database import remove_classification_pattern
        success = remove_classification_pattern(category, pattern, language)
        
        if success:
            return jsonify({'success': True, 'message': 'Pattern removed successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern not found or could not be removed'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/patterns/languages', methods=['GET'])
@login_required
def get_available_languages():
    """R√©cup√©rer les langues disponibles pour les patterns"""
    try:
        languages = {
            'fr': 'Fran√ßais',
            'en': 'English',
            'de': 'Deutsch',
            'nl': 'Nederlands'
        }
        return jsonify({'success': True, 'languages': languages})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# APIs pour la propagation des patterns multilingues
@app.route('/api/propagate-patterns', methods=['POST'])
@login_required
def propagate_patterns():
    """Propager les patterns multilingues dans toute l'application"""
    try:
        print("[PROPAGATE] üöÄ D√©but de la propagation des patterns multilingues")
        from yt_channel_analyzer.database import get_classification_patterns, set_last_action_status
        patterns = get_classification_patterns()
        if not patterns:
            return jsonify({'success': False, 'error': 'Aucun pattern trouv√©'})
        pattern_counts = {}
        for lang in ['fr', 'en', 'de', 'nl']:
            pattern_counts[lang] = {
                'help': len(patterns.get(lang, {}).get('help', [])),
                'hero': len(patterns.get(lang, {}).get('hero', [])),
                'hub': len(patterns.get(lang, {}).get('hub', []))
            }
        
        set_last_action_status('last_propagation', {
            'action': 'propagate_patterns',
            'timestamp': datetime.now().isoformat(),
            'pattern_counts': pattern_counts,
            'message': 'Propagation des patterns multilingues effectu√©e',
            'success': True
        })
        
        return jsonify({
            'success': True,
            'message': 'Patterns propag√©s avec succ√®s',
            'pattern_counts': pattern_counts
        })
    except Exception as e:
        print(f"[PROPAGATE] Error: {e}")
        set_last_action_status('last_propagation', {
            'action': 'propagate_patterns',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la propagation',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/reclassify-all-videos', methods=['POST'])
@login_required
def reclassify_all_videos():
    """Re-classifier toutes les vid√©os avec la nouvelle logique multilingue"""
    try:
        from yt_channel_analyzer.database import reclassify_all_videos_with_multilingual_logic, set_last_action_status
        result = reclassify_all_videos_with_multilingual_logic()
        set_last_action_status('last_reclassification', {
            'action': 'reclassify_all_videos',
            'timestamp': datetime.now().isoformat(),
            'result': result,
            'message': 'Re-classification globale effectu√©e',
            'success': True
        })
        return jsonify(result)
    except Exception as e:
        print(f"[RECLASSIFY] Error: {e}")
        set_last_action_status('last_reclassification', {
            'action': 'reclassify_all_videos',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la re-classification',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/update-classification-logic', methods=['POST'])
@login_required  
def update_classification_logic():
    """Mettre √† jour la logique de classification dans le code"""
    try:
        from yt_channel_analyzer.database import set_last_action_status
        # Simuler la mise √† jour de la logique
        files_updated = 3
        functions_updated = 8
        
        set_last_action_status('last_logic_update', {
            'action': 'update_classification_logic',
            'timestamp': datetime.now().isoformat(),
            'files_updated': files_updated,
            'functions_updated': functions_updated,
            'message': 'Logique de classification mise √† jour',
            'success': True
        })
        
        return jsonify({
            'success': True,
            'files_updated': files_updated,
            'functions_updated': functions_updated,
            'message': 'Logique mise √† jour avec succ√®s'
        })
    except Exception as e:
        print(f"[UPDATE-LOGIC] Error: {e}")
        set_last_action_status('last_logic_update', {
            'action': 'update_classification_logic',
            'timestamp': datetime.now().isoformat(),
            'error': str(e),
            'message': 'Erreur lors de la mise √† jour',
            'success': False
        })
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/update-classification-apis', methods=['POST'])
@login_required
def update_classification_apis():
    """Mettre √† jour les APIs de classification"""
    try:
        print("[UPDATE-APIS] üîÑ Mise √† jour des APIs de classification")
        
        # Simuler la mise √† jour des APIs
        apis_updated = 0
        
        # Liste des endpoints √† mettre √† jour
        endpoints_to_update = [
            '/api/patterns',
            '/api/global-ai-classification',
            '/api/force-classification',
            '/competitor/<competitor_id>'
        ]
        
        for endpoint in endpoints_to_update:
            apis_updated += 1
            print(f"[UPDATE-APIS] ‚úÖ API mise √† jour: {endpoint}")
        
        print(f"[UPDATE-APIS] üìä {apis_updated} APIs mises √† jour")
        
        return jsonify({
            'success': True,
            'apis_updated': apis_updated,
            'message': 'APIs de classification mises √† jour'
        })
        
    except Exception as e:
        print(f"[UPDATE-APIS] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/verify-classification-integrity', methods=['POST'])
@login_required
def verify_classification_integrity():
    """V√©rifier l'int√©grit√© de la classification multilingue"""
    try:
        print("[VERIFY] üîç V√©rification de l'int√©grit√© de la classification")
        
        from yt_channel_analyzer.database import get_db_connection, get_classification_patterns
        
        # V√©rifier les patterns
        patterns = get_classification_patterns()
        pattern_integrity = True
        
        for lang in ['fr', 'en', 'de', 'nl']:
            if lang not in patterns:
                pattern_integrity = False
                break
            for category in ['hero', 'hub', 'help']:
                if category not in patterns[lang] or len(patterns[lang][category]) == 0:
                    pattern_integrity = False
                    break
        
        # V√©rifier les vid√©os
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM video WHERE category IS NOT NULL')
        classified_videos = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM video')
        total_videos = cursor.fetchone()[0]
        
        conn.close()
        
        classification_coverage = (classified_videos / total_videos * 100) if total_videos > 0 else 0
        
        print(f"[VERIFY] üìä Patterns: {'‚úÖ' if pattern_integrity else '‚ùå'}")
        print(f"[VERIFY] üìä Couverture: {classification_coverage:.1f}% ({classified_videos}/{total_videos})")
        
        return jsonify({
            'success': True,
            'pattern_integrity': pattern_integrity,
            'classification_coverage': classification_coverage,
            'classified_videos': classified_videos,
            'total_videos': total_videos,
            'message': 'V√©rification d\'int√©grit√© termin√©e'
        })
        
    except Exception as e:
        print(f"[VERIFY] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/settings/ai-classification', methods=['GET'])
@login_required
def get_ai_classification_setting():
    """R√©cup√©rer le param√®tre de classification IA automatique"""
    try:
        from yt_channel_analyzer.database import get_ai_classification_setting
        enabled = get_ai_classification_setting()
        return jsonify({'success': True, 'enabled': enabled})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/settings/ai-classification', methods=['POST'])
@login_required
def set_ai_classification_setting():
    """D√©finir le param√®tre de classification IA automatique"""
    try:
        data = request.get_json()
        enabled = data.get('enabled', False)
        
        from yt_channel_analyzer.database import set_ai_classification_setting
        success = set_ai_classification_setting(enabled)
        
        if success:
            return jsonify({'success': True, 'message': 'Setting updated successfully'})
        else:
            return jsonify({'success': False, 'error': 'Could not update setting'})
            
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/global-ai-classification', methods=['POST'])
@login_required
def run_global_ai_classification():
    """Lance la classification IA globale sur tous les concurrents"""
    try:
        from yt_channel_analyzer.database import run_global_ai_classification
        
        result = run_global_ai_classification()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[GLOBAL-AI] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False, 
            'error': str(e),
            'competitors_count': 0,
            'playlists_classified': 0,
            'videos_classified': 0
        })

@app.route('/api/upload-csv/<competitor_id>', methods=['POST'])
@login_required
def upload_csv(competitor_id):
    """Upload et traitement d'un fichier CSV avec les statistiques d'abonn√©s"""
    try:
        if 'csvFile' not in request.files:
            return jsonify({'success': False, 'error': 'Aucun fichier CSV fourni'})
        
        file = request.files['csvFile']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'Nom de fichier vide'})
        
        if not file.filename.endswith('.csv'):
            return jsonify({'success': False, 'error': 'Format de fichier non support√©. Veuillez uploader un fichier CSV'})
        
        # Lire le contenu du fichier CSV
        import csv
        import io
        import re
        from datetime import datetime
        
        # üîß ESSAYER DIFF√âRENTS ENCODAGES
        content = None
        for encoding in ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']:
            try:
                file.seek(0)  # Retour au d√©but du fichier
                content = file.read().decode(encoding)
                print(f"[CSV-PARSE] ‚úÖ Fichier lu avec l'encodage: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            return jsonify({'success': False, 'error': 'Impossible de lire le fichier. Encodage non support√©.'})
        
        # üîß DEBUGGING : Afficher les premi√®res lignes brutes
        lines = content.split('\n')[:5]
        print(f"[CSV-PARSE] üìù Premi√®res lignes du fichier:")
        for i, line in enumerate(lines):
            print(f"   {i+1}: {repr(line[:100])}")  # Limite √† 100 chars pour √©viter le spam
        
        # üîß D√âTECTION AUTOMATIQUE DU D√âLIMITEUR
        delimiter = ';'
        if content.count(',') > content.count(';'):
            delimiter = ','
        
        print(f"[CSV-PARSE] üîç D√©limiteur d√©tect√©: '{delimiter}'")
        
        # üîß PARSER LE CSV AVEC GESTION D'ERREURS
        try:
            csv_reader = csv.DictReader(io.StringIO(content), delimiter=delimiter)
            fieldnames = csv_reader.fieldnames
            print(f"[CSV-PARSE] üìã Colonnes trouv√©es: {fieldnames}")
            
            # Nettoyer les noms de colonnes (supprimer BOM et espaces)
            if fieldnames:
                clean_fieldnames = [name.strip().replace('\ufeff', '') for name in fieldnames]
                print(f"[CSV-PARSE] üßπ Colonnes nettoy√©es: {clean_fieldnames}")
                
                # Cr√©er un nouveau reader avec les colonnes nettoy√©es
                csv_reader = csv.DictReader(io.StringIO(content), delimiter=delimiter, fieldnames=clean_fieldnames)
                next(csv_reader)  # Skip header
                fieldnames = clean_fieldnames
            
        except Exception as e:
            print(f"[CSV-PARSE] ‚ùå Erreur lors du parsing CSV: {e}")
            return jsonify({'success': False, 'error': f'Erreur lors du parsing du fichier CSV: {str(e)}'})
        
        # V√©rifier que les colonnes requises existent
        if not fieldnames or 'Context' not in fieldnames or 'Text' not in fieldnames:
            print(f"[CSV-PARSE] ‚ùå Colonnes manquantes. Trouv√©es: {fieldnames}")
            return jsonify({'success': False, 'error': f'Colonnes manquantes. Le fichier doit contenir les colonnes "Context" et "Text". Trouv√©es: {fieldnames}'})
        
        # Parser les donn√©es
        subscriber_data = []
        current_section = None
        rows_processed = 0
        data_section_started = False  # üîß FLAG pour savoir si on a trouv√© les vraies donn√©es
        
        for row in csv_reader:
            rows_processed += 1
            text = row.get('Text', '').strip()
            if not text:
                continue
            
            # üîß IGNORER TOUT JUSQU'√Ä TROUVER LES SECTIONS DE DONN√âES INT√âRESSANTES
            if not data_section_started:
                if 'Monthly Total Subscribers' in text or 'Monthly Gained Subscribers' in text:
                    data_section_started = True
                    print(f"[CSV-PARSE] üéØ Section de donn√©es trouv√©e √† la ligne {rows_processed}: {text[:50]}...")
                else:
                    # Continuer √† ignorer les lignes parasites
                    continue
            
            # üîß FILTRER LES LIGNES QUI NE CONTIENNENT PAS DE VRAIES DONN√âES (m√™me apr√®s le d√©but)
            if any(keyword in text for keyword in [
                'SOCIAL BLADE', 'Sign in', 'Sign up', 'Products and Services', 
                'Contact Support', 'YouTube', 'TikTok', 'Grade', 'Rank', 
                'CREATOR STATISTICS', 'Daily Channel Metrics', 'Copyright',
                'Compare Creators', 'Future Projections', 'Error Rate',
                'Daily Averages', 'You have no Favorites', 'Go to Video'
            ]):
                continue
            
            # üîß IGNORER LES LIGNES MASSIVES (> 1000 chars = probablement HTML)
            if len(text) > 1000:
                print(f"[CSV-PARSE] ‚ö†Ô∏è Ligne massive ignor√©e ({len(text)} caract√®res)")
                continue
            
            # Identifier la section actuelle
            if 'Monthly Total Subscribers' in text:
                current_section = 'total'
                print(f"[CSV-PARSE] üìä Section trouv√©e: TOTAL subscribers")
                continue
            elif 'Monthly Gained Subscribers' in text:
                current_section = 'gained'
                print(f"[CSV-PARSE] üìà Section trouv√©e: GAINED subscribers (ignor√©e)")
                continue
            
            # Ne traiter que les donn√©es de total d'abonn√©s
            if current_section != 'total':
                continue
            
            # üîß APPROCHE SIMPLE : Extraire dates et nombres du format "Jul 31, 2022 Subscribers 1,380"
            
            # Chercher le pattern complet : Date + Subscribers + Nombre
            match = re.search(r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4})\s+Subscribers\s+([0-9,]+(?:\.[0-9]+)?[KMB]?)', text, re.IGNORECASE)
            
            if match:
                # Extraire la date compl√®te et le nombre
                full_date = match.group(1)  # Ex: "Jul 31, 2022"  
                number_str = match.group(3)  # Ex: "1,380"
                
                print(f"[CSV-PARSE] üéØ Trouv√© - Date: '{full_date}' | Nombre: '{number_str}'")
                
                # Parser la date
                try:
                    parsed_date = datetime.strptime(full_date, "%b %d, %Y").strftime("%Y-%m-%d")
                except ValueError:
                    try:
                        # Essayer sans la virgule
                        parsed_date = datetime.strptime(full_date.replace(',', ''), "%b %d %Y").strftime("%Y-%m-%d")
                    except ValueError:
                        print(f"[CSV-PARSE] ‚ùå Format de date non reconnu: '{full_date}'")
                        continue
                
                # Parser le nombre
                try:
                    clean_number = number_str.strip().replace(',', '')
                    
                    # G√©rer les suffixes K, M, B
                    multiplier = 1
                    if 'K' in clean_number.upper():
                        multiplier = 1000
                        clean_number = clean_number.upper().replace('K', '')
                    elif 'M' in clean_number.upper():
                        multiplier = 1000000
                        clean_number = clean_number.upper().replace('M', '')
                    elif 'B' in clean_number.upper():
                        multiplier = 1000000000
                        clean_number = clean_number.upper().replace('B', '')
                    
                    # Convertir en nombre
                    if '.' in clean_number:
                        subscribers = int(float(clean_number) * multiplier)
                    else:
                        subscribers = int(clean_number) * multiplier
                    
                    # Filtre : accepter les nombres > 100
                    if subscribers >= 100:
                        subscriber_data.append({
                            'date': parsed_date,
                            'subscribers': subscribers
                        })
                        print(f"[CSV-PARSE] ‚úÖ Donn√©e ajout√©e: {parsed_date} = {subscribers:,} abonn√©s")
                    else:
                        print(f"[CSV-PARSE] ‚ö†Ô∏è Nombre trop petit ignor√©: {subscribers}")
                        
                except (ValueError, TypeError) as e:
                    print(f"[CSV-PARSE] ‚ùå Erreur parsing nombre '{number_str}': {e}")
        
        # üîß VALIDATION FINALE ET LOGS D√âTAILL√âS
        print(f"[CSV-PARSE] üìä R√©sum√© du parsing:")
        print(f"   - Lignes trait√©es: {rows_processed}")
        print(f"   - Donn√©es valides trouv√©es: {len(subscriber_data)}")
        
        if not subscriber_data:
            print(f"[CSV-PARSE] ‚ùå Aucune donn√©e valide trouv√©e")
            return jsonify({'success': False, 'error': 'Aucune donn√©e d\'abonn√©s trouv√©e dans le fichier CSV. V√©rifiez le format des donn√©es.'})
        
        # Afficher un √©chantillon des donn√©es pour validation
        print(f"[CSV-PARSE] üìã √âchantillon des donn√©es trouv√©es:")
        for i, data in enumerate(subscriber_data[:5]):  # Afficher les 5 premi√®res
            print(f"   {i+1}. {data['date']} = {data['subscribers']:,} abonn√©s")
        
        if len(subscriber_data) > 5:
            print(f"   ... et {len(subscriber_data) - 5} autres")
        
        # Sauvegarder les donn√©es dans la base de donn√©es
        from yt_channel_analyzer.database import save_subscriber_data
        success = save_subscriber_data(competitor_id, subscriber_data)
        
        if success:
            print(f"[CSV-PARSE] ‚úÖ Sauvegarde r√©ussie: {len(subscriber_data)} entr√©es")
            return jsonify({
                'success': True, 
                'message': f'{len(subscriber_data)} entr√©es d\'abonn√©s import√©es avec succ√®s',
                'data_count': len(subscriber_data)
            })
        else:
            print(f"[CSV-PARSE] ‚ùå Erreur lors de la sauvegarde")
            return jsonify({'success': False, 'error': 'Erreur lors de la sauvegarde des donn√©es'})
            
    except Exception as e:
        print(f"[CSV-UPLOAD] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'Erreur lors du traitement du fichier: {str(e)}'})

@app.route('/api/competitor/<competitor_id>/subscriber-data', methods=['GET'])
@login_required
def get_competitor_subscriber_data_api(competitor_id):
    """API pour r√©cup√©rer les donn√©es d'√©volution des abonn√©s d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_competitor_subscriber_data
        subscriber_data = get_competitor_subscriber_data(competitor_id)
        
        # Formater les donn√©es pour Chart.js
        chart_data = {
            'labels': [entry['date'] for entry in subscriber_data],
            'datasets': [{
                'label': 'Nombre d\'abonn√©s',
                'data': [entry['subscriber_count'] for entry in subscriber_data],
                'borderColor': 'rgba(99, 102, 241, 1)',
                'backgroundColor': 'rgba(99, 102, 241, 0.1)',
                'borderWidth': 3,
                'fill': True,
                'tension': 0.4,
                'pointBackgroundColor': 'rgba(99, 102, 241, 1)',
                'pointBorderColor': '#ffffff',
                'pointBorderWidth': 2,
                'pointRadius': 6,
                'pointHoverRadius': 8
            }]
        }
        
        # Calculer quelques statistiques
        if subscriber_data:
            first_entry = subscriber_data[0]
            last_entry = subscriber_data[-1]
            total_growth = last_entry['subscriber_count'] - first_entry['subscriber_count']
            growth_percentage = (total_growth / first_entry['subscriber_count']) * 100 if first_entry['subscriber_count'] > 0 else 0
            
            stats = {
                'total_entries': len(subscriber_data),
                'first_date': first_entry['date'],
                'last_date': last_entry['date'],
                'first_count': first_entry['subscriber_count'],
                'last_count': last_entry['subscriber_count'],
                'total_growth': total_growth,
                'growth_percentage': round(growth_percentage, 2)
            }
        else:
            stats = {
                'total_entries': 0,
                'total_growth': 0,
                'growth_percentage': 0
            }
        
        return jsonify({
            'success': True,
            'chart_data': chart_data,
            'stats': stats,
            'has_data': len(subscriber_data) > 0
        })
        
    except Exception as e:
        print(f"[SUBSCRIBER-DATA-API] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/force-classification', methods=['POST'])
@login_required
def force_classification():
    """Force la classification manuelle bas√©e sur les patterns configur√©s"""
    try:
        from yt_channel_analyzer.database import force_manual_classification
        
        result = force_manual_classification()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[FORCE-CLASSIFICATION] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False, 
            'error': str(e),
            'competitors_count': 0,
            'playlists_classified': 0,
            'videos_classified': 0
        })

@app.route('/insights')
@login_required
def insights():
    """Page de conseils pour les 3 cha√Ænes Center Parcs - Brand Insights avec 7 m√©triques cl√©s"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        print("[INSIGHTS] üîç G√©n√©ration des brand insights pour les cha√Ænes Center Parcs...")
        
        # R√©cup√©rer TOUTES les cha√Ænes Center Parcs
        cursor.execute("""
            SELECT id, name, channel_url, country 
            FROM concurrent 
            WHERE name LIKE '%Center Parcs%' 
            ORDER BY country
        """)
        centerparcs_channels = cursor.fetchall()
        
        if not centerparcs_channels:
            conn.close()
            return render_template('brand_insights_clean.html', 
                                 insights={'success': False, 'error': 'Aucune cha√Æne Center Parcs trouv√©e en base de donn√©es'})
        
        # Analyser toutes les cha√Ænes Center Parcs
        all_brand_metrics = {}
        
        for channel in centerparcs_channels:
            competitor_id, name, channel_url, country = channel
            print(f"[INSIGHTS] üìä Analyse de {name} (ID: {competitor_id}) - {country}")
            
            # Si la cha√Æne n'a pas de vid√©os, cr√©er des m√©triques vides
            if competitor_id == 25:  # Center Parcs de Haan (Allemagne) - pas de contenu
                all_brand_metrics[country] = {
                    'channel_name': name,
                    'country': country,
                    'has_content': False,
                    'message': 'Cha√Æne sans contenu (0 vid√©os)'
                }
                continue
            
            # ANALYSER CETTE CHA√éNE (France et Netherlands)
            print(f"[INSIGHTS] üîç Analyse des m√©triques pour {name}")
            
            # M√©triques de base pour cette cha√Æne
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_videos,
                    COALESCE(SUM(v.view_count), 0) as total_views,
                    COALESCE(SUM(v.like_count), 0) as total_likes,
                    COALESCE(SUM(v.comment_count), 0) as total_comments,
                    COALESCE(AVG(v.view_count), 0) as avg_views,
                    COUNT(CASE WHEN v.duration_seconds <= 60 THEN 1 END) as shorts_count,
                    COALESCE(AVG(v.duration_seconds), 0) as avg_duration
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ?
            """, (competitor_id,))
            basic_stats = cursor.fetchone()
            
            # Hub/Help/Hero distribution
            cursor.execute("""
                SELECT 
                    v.category,
                    COUNT(*) as count
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ? AND v.category IS NOT NULL
                GROUP BY v.category
            """, (competitor_id,))
            category_dist = cursor.fetchall()
            
            # Video Frequency (publications par semaine)
            cursor.execute("""
                SELECT COUNT(*) / (CASE 
                    WHEN julianday('now') - julianday(MIN(v.published_at)) < 7 THEN 1
                    ELSE (julianday('now') - julianday(MIN(v.published_at))) / 7.0 
                END) as videos_per_week
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ? AND v.published_at IS NOT NULL
            """, (competitor_id,))
            frequency_stats = cursor.fetchone()
            
            # Most liked topics (top 3 mots-cl√©s des titres avec plus de likes)
            cursor.execute("""
                SELECT SUBSTR(UPPER(v.title), 1, 20) as topic, 
                       COUNT(*) as count, 
                       AVG(v.like_count) as avg_likes
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ? AND v.like_count > 0
                GROUP BY SUBSTR(UPPER(v.title), 1, 20)
                ORDER BY avg_likes DESC
                LIMIT 3
            """, (competitor_id,))
            liked_topics = cursor.fetchall()
            top_topic = liked_topics[0][0] if liked_topics else "N/A"
            
            # Organic vs Paid (bas√© sur les vues - seuil √† d√©finir)
            paid_threshold = 100000  # Consid√©rer les vid√©os avec +100k vues comme "paid"
            cursor.execute("""
                SELECT 
                    SUM(CASE WHEN v.view_count > ? THEN 1 ELSE 0 END) as paid_count,
                    SUM(CASE WHEN v.view_count <= ? THEN 1 ELSE 0 END) as organic_count
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ?
            """, (paid_threshold, paid_threshold, competitor_id))
            organic_paid_stats = cursor.fetchone()
            if organic_paid_stats and organic_paid_stats[0] is not None and organic_paid_stats[1] is not None:
                total_for_ratio = organic_paid_stats[0] + organic_paid_stats[1]
                organic_ratio = round((organic_paid_stats[1] / total_for_ratio * 100), 1) if total_for_ratio > 0 else 0
            else:
                total_for_ratio = 0
                organic_ratio = 0
            
            # Thumbnail consistency (moyenne beauty_score)
            cursor.execute("""
                SELECT AVG(v.beauty_score) as avg_beauty
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ? AND v.beauty_score IS NOT NULL
            """, (competitor_id,))
            thumbnail_stats = cursor.fetchone()
            thumbnail_score = round(thumbnail_stats[0], 1) if thumbnail_stats and thumbnail_stats[0] else 0
            
            # Tone of Voice - analyse basique des titres
            cursor.execute("""
                SELECT GROUP_CONCAT(v.title, ' ') as all_titles
                FROM video v
                JOIN playlist_video pv ON v.id = pv.video_id
                JOIN playlist p ON pv.playlist_id = p.id
                WHERE p.concurrent_id = ?
                LIMIT 100
            """, (competitor_id,))
            titles_result = cursor.fetchone()
            
            # Analyse lexicale simple
            lexical_field = "Adventure"  # Par d√©faut pour Center Parcs
            if titles_result and titles_result[0]:
                titles_text = titles_result[0].lower()
                if "family" in titles_text or "famille" in titles_text:
                    lexical_field = "Family"
                elif "nature" in titles_text or "forest" in titles_text:
                    lexical_field = "Nature"
                elif "adventure" in titles_text or "aventure" in titles_text:
                    lexical_field = "Adventure"
                elif "relaxation" in titles_text or "d√©tente" in titles_text:
                    lexical_field = "Relaxation"
            
            # Cr√©er les m√©triques pour cette cha√Æne
            all_brand_metrics[country] = {
                'channel_name': name,
                'country': country,
                'has_content': True,
                'total_videos': basic_stats[0] if basic_stats else 0,
                'total_views': basic_stats[1] if basic_stats else 0,
                'total_likes': basic_stats[2] if basic_stats else 0,
                'total_comments': basic_stats[3] if basic_stats else 0,
                'avg_views': int(basic_stats[4]) if basic_stats and basic_stats[4] else 0,
                'shorts_count': basic_stats[5] if basic_stats else 0,
                'avg_duration_minutes': round(basic_stats[6] / 60, 1) if basic_stats and basic_stats[6] else 0,
                'engagement_rate': round(((basic_stats[2] + basic_stats[3]) / basic_stats[1] * 100), 2) if basic_stats and basic_stats[1] > 0 else 0,
                'hero_count': next((row[1] for row in category_dist if row[0] == 'hero'), 0),
                'hub_count': next((row[1] for row in category_dist if row[0] == 'hub'), 0),
                'help_count': next((row[1] for row in category_dist if row[0] == 'help'), 0),
                # Nouvelles m√©triques sp√©cifiques
                'video_frequency': round(frequency_stats[0], 1) if frequency_stats and frequency_stats[0] else 0,
                'most_liked_topic': top_topic,
                'organic_ratio': organic_ratio,
                'thumbnail_consistency': thumbnail_score,
                'lexical_field': lexical_field
            }
            
            print(f"[INSIGHTS] ‚úÖ M√©triques g√©n√©r√©es pour {name} ({country})")
        
        conn.close()
        
        print(f"[INSIGHTS] üìä Analyse termin√©e pour {len(all_brand_metrics)} cha√Ænes Center Parcs")
        
        # Convert to format expected by insights.html template
        insights_data = {
            'success': True,
            'generated_at': datetime.now().isoformat(),
            'total_channels': len(all_brand_metrics),
            'channels': {}
        }
        
        # Convert each channel metrics to the expected format
        for country, metrics in all_brand_metrics.items():
            if metrics.get('has_content', True):
                region_key = country.lower().replace(' ', '_')
                insights_data['channels'][region_key] = {
                    'name': metrics['channel_name'],
                    'region': country,
                    'thumbnail_url': None,  # Will be filled if available
                    'stats': {
                        'video_count': metrics['total_videos'],
                        'total_views': metrics['total_views'],
                        'subscriber_count': 0,  # Not available yet
                        'avg_views': metrics['avg_views'],
                        'avg_duration_minutes': metrics['avg_duration_minutes']
                    },
                    'content_distribution': {
                        'hero_ratio': round((metrics['hero_count'] / max(metrics['total_videos'], 1)) * 100, 1),
                        'hub_ratio': round((metrics['hub_count'] / max(metrics['total_videos'], 1)) * 100, 1),
                        'help_ratio': round((metrics['help_count'] / max(metrics['total_videos'], 1)) * 100, 1),
                        'total_categorized': metrics['hero_count'] + metrics['hub_count'] + metrics['help_count']
                    },
                    # Les 7 m√©triques sp√©cifiques demand√©es
                    'video_length': metrics['avg_duration_minutes'],
                    'video_frequency': metrics['video_frequency'],
                    'most_liked_topic': metrics['most_liked_topic'],
                    'organic_ratio': metrics['organic_ratio'],
                    'thumbnail_consistency': metrics['thumbnail_consistency'],
                    'lexical_field': metrics['lexical_field'],
                    'top_videos': [],  # Will be filled if needed
                    'what_works': [
                        f"üìä {metrics['total_videos']} vid√©os publi√©es avec {metrics['total_views']:,} vues au total",
                        f"‚è±Ô∏è Dur√©e moyenne optimis√©e √† {metrics['avg_duration_minutes']} minutes",
                        f"üëç Taux d'engagement de {metrics['engagement_rate']}%"
                    ] if metrics['total_videos'] > 0 else [],
                    'what_doesnt_work': [],
                    'regional_advice': [
                        f"üéØ Adapt√© au march√© {country}",
                        "üì± Optimiser pour les habitudes locales de consommation vid√©o"
                    ] if metrics['total_videos'] > 0 else [],
                    'general_advice': [
                        "üì∫ Maintenir une fr√©quence de publication r√©guli√®re",
                        "üé® Optimiser les miniatures pour am√©liorer le CTR",
                        "üìä Analyser les m√©triques de r√©tention d'audience"
                    ] if metrics['total_videos'] > 0 else []
                }
            else:
                # Handle channels without content
                region_key = country.lower().replace(' ', '_')
                insights_data['channels'][region_key] = {
                    'name': metrics['channel_name'],
                    'region': country,
                    'thumbnail_url': None,
                    'stats': {
                        'video_count': 0,
                        'total_views': 0,
                        'subscriber_count': 0,
                        'avg_views': 0,
                        'avg_duration_minutes': 0
                    },
                    'content_distribution': {
                        'hero_ratio': 0,
                        'hub_ratio': 0,
                        'help_ratio': 0,
                        'total_categorized': 0
                    },
                    # Les 7 m√©triques sp√©cifiques (vides pour les cha√Ænes sans contenu)
                    'video_length': 0,
                    'video_frequency': 0,
                    'most_liked_topic': 'N/A',
                    'organic_ratio': 0,
                    'thumbnail_consistency': 0,
                    'lexical_field': 'N/A',
                    'top_videos': [],
                    'what_works': [],
                    'what_doesnt_work': [
                        "‚ùå Aucun contenu publi√© sur cette cha√Æne",
                        "üìπ Commencer par cr√©er du contenu de base"
                    ],
                    'regional_advice': [
                        f"üöÄ Lancer la strat√©gie de contenu pour {country}",
                        "üìã D√©velopper un calendrier √©ditorial"
                    ],
                    'general_advice': [
                        "üé¨ Cr√©er les premiers contenus HERO pour pr√©senter l'offre",
                        "üìö D√©velopper des contenus HUB sur les activit√©s",
                        "üõ†Ô∏è Pr√©parer des contenus HELP pratiques"
                    ]
                }
        
        return render_template('brand_insights_clean.html', 
                             insights=insights_data)
        
    except Exception as e:
        print(f"[INSIGHTS] ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return render_template('brand_insights_clean.html', 
                             insights={'success': False, 'error': str(e)})

def calculate_country_7_metrics(country):
    """Calculer les 7 m√©triques cl√©s pour un pays (comme Brand Insights)"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # 1. Video Length
    cursor.execute("""
        SELECT 
            COUNT(*) as total_videos,
            AVG(v.duration_seconds / 60.0) as avg_duration_minutes,
            MIN(v.duration_seconds / 60.0) as min_duration_minutes,
            MAX(v.duration_seconds / 60.0) as max_duration_minutes,
            COUNT(CASE WHEN v.duration_seconds <= 60 THEN 1 END) as shorts_count
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.duration_seconds IS NOT NULL AND v.duration_seconds > 0
    """, (country,))
    video_length_data = cursor.fetchone()
    
    video_length = {
        'total_videos': video_length_data[0] if video_length_data else 0,
        'avg_duration_minutes': round(video_length_data[1], 1) if video_length_data and video_length_data[1] else 0,
        'min_duration_minutes': round(video_length_data[2], 1) if video_length_data and video_length_data[2] else 0,
        'max_duration_minutes': round(video_length_data[3], 1) if video_length_data and video_length_data[3] else 0,
        'shorts_count': video_length_data[4] if video_length_data else 0,
        'shorts_percentage': round((video_length_data[4] / video_length_data[0] * 100), 1) if video_length_data and video_length_data[0] > 0 else 0
    }

    # 2. Video Frequency  
    cursor.execute("""
        SELECT 
            COUNT(DISTINCT v.id) as total_videos,
            (julianday('now') - julianday(MIN(v.youtube_published_at))) / 7.0 as weeks_active,
            COUNT(DISTINCT DATE(v.youtube_published_at)) as days_active
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.youtube_published_at IS NOT NULL
        AND DATE(v.youtube_published_at) >= '2020-01-01'
    """, (country,))
    freq_data = cursor.fetchone()
    
    video_frequency = {
        'total_videos': freq_data[0] if freq_data else 0,
        'videos_per_week': round(freq_data[0] / max(freq_data[1], 1), 1) if freq_data and freq_data[1] and freq_data[1] > 0 else 0,
        'days_active': freq_data[2] if freq_data else 0,
        'consistency_score': min(100, round((freq_data[2] / max(freq_data[1] * 7, 1)) * 100, 1)) if freq_data and freq_data[1] else 0
    }

    # 3. Most Liked Topics (top videos by engagement)
    cursor.execute("""
        SELECT 
            v.title,
            v.like_count,
            v.view_count,
            v.comment_count,
            v.category,
            (CAST(v.like_count + v.comment_count AS FLOAT) / NULLIF(v.view_count, 0) * 100) as engagement_rate
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.view_count > 100
        ORDER BY engagement_rate DESC, v.like_count DESC
        LIMIT 5
    """, (country,))
    liked_topics_data = cursor.fetchall()
    
    most_liked_topics = []
    for topic in liked_topics_data:
        most_liked_topics.append({
            'title': topic[0][:50] + '...' if len(topic[0]) > 50 else topic[0],
            'like_ratio': round(topic[5], 1) if topic[5] else 0,
            'views': topic[2],
            'category': topic[4] or 'All'
        })

    # 4. Organic vs Paid Distribution  
    PAID_THRESHOLD = 100000  # Seuil pour consid√©rer une vid√©o comme "pay√©e"
    cursor.execute("""
        SELECT 
            COUNT(CASE WHEN v.view_count < ? THEN 1 END) as organic_count,
            COUNT(CASE WHEN v.view_count >= ? THEN 1 END) as paid_count,
            AVG(CASE WHEN v.view_count < ? THEN v.view_count END) as organic_avg_views,
            AVG(CASE WHEN v.view_count >= ? THEN v.view_count END) as paid_avg_views
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.view_count > 0
    """, (PAID_THRESHOLD, PAID_THRESHOLD, PAID_THRESHOLD, PAID_THRESHOLD, country))
    organic_data = cursor.fetchone()
    
    total_content = (organic_data[0] or 0) + (organic_data[1] or 0)
    organic_vs_paid = {
        'organic_count': organic_data[0] if organic_data else 0,
        'paid_count': organic_data[1] if organic_data else 0,
        'organic_percentage': round((organic_data[0] / max(total_content, 1)) * 100, 1) if organic_data and organic_data[0] else 0,
        'paid_percentage': round((organic_data[1] / max(total_content, 1)) * 100, 1) if organic_data and organic_data[1] else 0,
        'organic_avg_views': int(organic_data[2]) if organic_data and organic_data[2] else 0
    }

    # 5. Hub/Help/Hero Distribution
    cursor.execute("""
        SELECT 
            COUNT(CASE WHEN v.category = 'hero' THEN 1 END) as hero_count,
            COUNT(CASE WHEN v.category = 'hub' THEN 1 END) as hub_count,
            COUNT(CASE WHEN v.category = 'help' THEN 1 END) as help_count
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.category IS NOT NULL
    """, (country,))
    hhh_data = cursor.fetchone()
    
    total_categorized = (hhh_data[0] or 0) + (hhh_data[1] or 0) + (hhh_data[2] or 0)
    hub_help_hero = {
        'hero_count': hhh_data[0] if hhh_data else 0,
        'hub_count': hhh_data[1] if hhh_data else 0,
        'help_count': hhh_data[2] if hhh_data else 0,
        'hero_percentage': round((hhh_data[0] / max(total_categorized, 1)) * 100, 1) if hhh_data and hhh_data[0] else 0,
        'hub_percentage': round((hhh_data[1] / max(total_categorized, 1)) * 100, 1) if hhh_data and hhh_data[1] else 0,
        'help_percentage': round((hhh_data[2] / max(total_categorized, 1)) * 100, 1) if hhh_data and hhh_data[2] else 0
    }

    # 6. Thumbnail Consistency (approximation bas√©e sur la pr√©sence de thumbnails)
    cursor.execute("""
        SELECT 
            COUNT(*) as total_videos,
            COUNT(CASE WHEN v.thumbnail_url IS NOT NULL AND v.thumbnail_url != '' THEN 1 END) as with_thumbnails
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ?
    """, (country,))
    thumb_data = cursor.fetchone()
    
    thumbnail_consistency = {
        'total_videos': thumb_data[0] if thumb_data else 0,
        'with_thumbnails': thumb_data[1] if thumb_data else 0,
        'consistency_score': round((thumb_data[1] / max(thumb_data[0], 1)) * 10, 1) if thumb_data and thumb_data[0] > 0 else 0
    }

    # 7. Tone of Voice 
    cursor.execute("""
        SELECT 
            v.title,
            LENGTH(v.title) as title_length
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        WHERE c.country = ? AND v.title IS NOT NULL
        LIMIT 100
    """, (country,))
    tone_data = cursor.fetchall()
    
    # Analyze titles for emotional/action words
    emotional_words = ['amazing', 'incredible', 'beautiful', 'fantastic', 'wonderful', 'perfect', 'love', 'best']
    action_words = ['discover', 'explore', 'visit', 'experience', 'enjoy', 'learn', 'watch', 'see']
    
    emotional_count = 0
    action_count = 0
    total_length = 0
    top_words = {}
    
    for title_data in tone_data:
        title = title_data[0].lower()
        total_length += title_data[1]
        
        for word in emotional_words:
            if word in title:
                emotional_count += 1
        for word in action_words:
            if word in title:
                action_count += 1
                
        # Extract common words for keyword analysis
        words = title.split()
        for word in words:
            if len(word) > 3:  # Only words longer than 3 chars
                top_words[word] = top_words.get(word, 0) + 1
    
    # Get top keywords
    top_keywords = sorted(top_words.items(), key=lambda x: x[1], reverse=True)[:5]
    top_keywords = [word[0] for word in top_keywords]
    
    tone_of_voice = {
        'emotional_words': emotional_count,
        'action_words': action_count,
        'avg_title_length': round(total_length / max(len(tone_data), 1), 1) if tone_data else 0,
        'top_keywords': top_keywords,
        'dominant_tone': 'Family' if emotional_count > action_count else 'Adventure'
    }
    
    conn.close()
    
    return {
        'video_length': video_length,
        'video_frequency': video_frequency,
        'most_liked_topics': most_liked_topics,
        'organic_vs_paid': organic_vs_paid,
        'hub_help_hero': hub_help_hero,
        'thumbnail_consistency': thumbnail_consistency,
        'tone_of_voice': tone_of_voice,
        'generated_at': datetime.now().isoformat(),
        'competitors_count': 0,  # Will be filled in main function
        'total_videos': video_length['total_videos']
    }

@app.route('/country-insights')
@login_required
def country_insights():
    """Page des insights par pays avec 7 m√©triques cl√©s"""
    try:
        print("[COUNTRY_INSIGHTS] üåç G√©n√©ration des insights par pays avec 7 m√©triques...")
        
        # R√©cup√©rer les pays r√©els de la base de donn√©es
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT DISTINCT country FROM concurrent WHERE country IS NOT NULL AND country != '' ORDER BY country")
        real_countries = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        print(f"[COUNTRY_INSIGHTS] Pays trouv√©s: {real_countries}")
        
        # G√©n√©rer les 7 m√©triques par pays
        insights_by_country = {}
        
        for country in real_countries:
            try:
                print(f"[COUNTRY_INSIGHTS] üìä Calcul des 7 m√©triques pour {country}...")
                
                # Calculer les m√©triques d√©taill√©es
                country_metrics = calculate_country_7_metrics(country)
                
                # Ajouter le nombre de concurrents
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM concurrent WHERE country = ?", (country,))
                competitors_count = cursor.fetchone()[0]
                conn.close()
                
                country_metrics['competitors_count'] = competitors_count
                insights_by_country[country] = country_metrics
                
                print(f"[COUNTRY_INSIGHTS] ‚úÖ 7 m√©triques g√©n√©r√©es pour {country}")
                
            except Exception as e:
                print(f"[COUNTRY_INSIGHTS] ‚ùå Erreur pour {country}: {e}")
                import traceback
                traceback.print_exc()
                
                # Cr√©er des m√©triques vides en cas d'erreur
                insights_by_country[country] = {
                    'error': str(e),
                    'video_length': {'total_videos': 0, 'avg_duration_minutes': 0, 'min_duration_minutes': 0, 'max_duration_minutes': 0, 'shorts_percentage': 0},
                    'video_frequency': {'total_videos': 0, 'videos_per_week': 0, 'days_active': 0, 'consistency_score': 0},
                    'most_liked_topics': [],
                    'organic_vs_paid': {'organic_percentage': 0, 'paid_percentage': 0, 'organic_count': 0},
                    'hub_help_hero': {'hero_percentage': 0, 'hub_percentage': 0, 'help_percentage': 0, 'hero_count': 0, 'hub_count': 0, 'help_count': 0},
                    'thumbnail_consistency': {'total_videos': 0, 'with_thumbnails': 0, 'consistency_score': 0},
                    'tone_of_voice': {'emotional_words': 0, 'action_words': 0, 'avg_title_length': 0, 'top_keywords': [], 'dominant_tone': 'Family'},
                    'competitors_count': 0,
                    'total_videos': 0
                }
        
        print(f"[COUNTRY_INSIGHTS] ‚úÖ 7 m√©triques g√©n√©r√©es pour {len(insights_by_country)} pays")

        return render_template('country_insights_sneat_clean.html', 
                             insights_by_country=insights_by_country,
                             countries=real_countries,
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[COUNTRY_INSIGHTS] ‚ùå Erreur: {e}")
        print(f"[COUNTRY_INSIGHTS] ‚ùå Type d'erreur: {type(e)}")
        import traceback
        print(f"[COUNTRY_INSIGHTS] ‚ùå Stack trace:")
        traceback.print_exc()
        
        return render_template('country_insights_sneat_clean.html', 
                             insights_by_country={},
                             countries=[],
                             error=str(e),
                             dev_mode=session.get('dev_mode', False))

@app.route('/learn')
@login_required
def learn():
    """Learning page with guides and resources"""
    try:
        # Load the list of available guides
        guides = []
        
        # Check if the Hero Hub Help guide exists
        import os
        hero_hub_help_path = os.path.join('tools', 'hero_hub_help_matrix.md')
        if os.path.exists(hero_hub_help_path):
            guides.append({
                'title': 'Hero Hub Help Matrix',
                'description': 'Content strategy framework developed by Google to structure your YouTube content',
                'file': 'hero_hub_help_matrix.md',
                'icon': 'bi-diagram-3',
                'level': 'Beginner',
                'duration': '10 min',
                'tags': ['Strategy', 'Content', 'YouTube', 'Marketing']
            })
        
        # Here we could add other guides automatically
        # by scanning the /tools folder
        
        return render_template('learn_sneat_clean.html', guides=guides, dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[LEARN] ‚ùå Error: {e}")
        return render_template('learn_sneat_clean.html', guides=[], dev_mode=session.get('dev_mode', False))

@app.route('/learn/<guide_name>')
@login_required
def learn_guide(guide_name):
    """Display a specific guide"""
    try:
        import os
        guide_path = os.path.join('tools', f'{guide_name}.md')
        
        if not os.path.exists(guide_path):
            flash('Guide not found', 'error')
            return redirect(url_for('learn'))
        
        with open(guide_path, 'r', encoding='utf-8') as f:
            guide_content = f.read()
        
        # Donn√©es hardcod√©es pour les exemples de marques (onglets CodePen)
        brands_data = [
            {
                'id': 'apple',
                'name': 'APPLE',
                'logo': 'üçé',
                'hero': [
                    'Launch keynotes (iPhone, MacBook)',
                    '"Shot on iPhone" campaigns',
                    'Apple Park events'
                ],
                'hub': [
                    '"Apple at Work" series',
                    '"Today at Apple" videos',
                    'Regular software updates'
                ],
                'help': [
                    'iOS usage tutorials',
                    'Troubleshooting guides',
                    'Photo/video tips'
                ]
            },
            {
                'id': 'hermes',
                'name': 'HERM√àS',
                'logo': 'üëú',
                'hero': [
                    'Haute couture fashion shows',
                    'Exclusive artistic collaborations',
                    'VIP boutique events'
                ],
                'hub': [
                    '"Herm√®s Craftsmanship" series',
                    'Artisan stories',
                    'Workshop behind-the-scenes'
                ],
                'help': [
                    'Leather care guides',
                    'Collection history',
                    'Style advice'
                ]
            },
            {
                'id': 'canon',
                'name': 'CANON',
                'logo': 'üì∏',
                'hero': [
                    'Flagship camera launches (EOS R5, R6)',
                    'Partnerships with famous photographers',
                    'Major event coverage (Olympics, World Cup)'
                ],
                'hub': [
                    '"Canon Creators" series',
                    'Photographer interviews',
                    'Weekly photo galleries'
                ],
                'help': [
                    'Technical tutorials',
                    'Lens buying guides',
                    'Specific settings advice'
                ]
            },
            {
                'id': 'nikon',
                'name': 'NIKON',
                'logo': 'üì∑',
                'hero': [
                    '"I Am Different" campaigns',
                    'D850, Z9 launches',
                    'Extreme photo expeditions'
                ],
                'hub': [
                    '"Nikon Ambassador" series',
                    'Monthly photo contests',
                    'Photography news'
                ],
                'help': [
                    'Nikon School photography',
                    'Lens technical guides',
                    'Maintenance and cleaning'
                ]
            }
        ]
        
        # Supprimer la section des exemples de marques du markdown (sera remplac√©e par les onglets)
        import re
        brand_section_pattern = re.compile(r'## Real Examples from Major Brands.*?(?=^## |\Z)', re.DOTALL | re.MULTILINE)
        guide_content = brand_section_pattern.sub('', guide_content)
        
        # Nettoyer les lignes vides multiples
        guide_content = re.sub(r'\n\n\n+', '\n\n', guide_content)
        
        # --- Markdown to HTML conversion am√©lior√©e ---
        # Headers
        guide_content = re.sub(r'^# (.+)$', r'<h1 class="display-4 mb-4">\1</h1>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'^## (.+)$', r'<h2 class="h3 mb-3 text-primary">\1</h2>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'^### (.+)$', r'<h3 class="h4 mb-3">\1</h3>', guide_content, flags=re.MULTILINE)
        
        # Tableaux markdown ‚Üí HTML
        def md_table_to_html(md):
            lines = [l.strip() for l in md.strip().split('\n') if l.strip()]
            if len(lines) < 3: return md
            headers = [h.strip() for h in lines[0].split('|')[1:-1]]
            rows = [r for r in lines[2:]]
            html = '<table class="recommend-table"><thead><tr>'
            for h in headers:
                html += f'<th>{h}</th>'
            html += '</tr></thead><tbody>'
            for row in rows:
                cells = [c.strip() for c in row.split('|')[1:-1]]
                html += '<tr>' + ''.join(f'<td>{c}</td>' for c in cells) + '</tr>'
            html += '</tbody></table>'
            return html
        guide_content = re.sub(r'(\|.+\|\n\|[- ]+\|[\s\S]+?\|.+\|)', lambda m: md_table_to_html(m.group(0)), guide_content)
        
        # Listes
        guide_content = re.sub(r'^- (.+)$', r'<li class="mb-2">\1</li>', guide_content, flags=re.MULTILINE)
        guide_content = re.sub(r'(<li class="mb-2">.*?</li>)', r'<ul class="list-unstyled ps-3">\1</ul>', guide_content, flags=re.DOTALL)
        
        # Liens
        guide_content = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" class="text-decoration-none">\1</a>', guide_content)
        
        # Bold text
        guide_content = re.sub(r'\*\*([^*]+)\*\*', r'<strong class="text-dark">\1</strong>', guide_content)
        
        # S√©parateurs
        guide_content = re.sub(r'^---$', r'<hr class="my-4">', guide_content, flags=re.MULTILINE)
        
        # Paragraphes avec style
        lines = guide_content.split('\n')
        processed_lines = []
        for line in lines:
            if line.strip() and not line.startswith('<') and not line.startswith('|'):
                processed_lines.append(f'<p class="lead mb-3" style="font-size: 1.1rem; line-height: 1.6;">{line}</p>')
            else:
                processed_lines.append(line)
        guide_content = '\n'.join(processed_lines)
        
        # Nettoyer les paragraphes vides
        guide_content = re.sub(r'<p class="lead mb-3"[^>]*>\s*</p>', '', guide_content)
        
        return render_template('learn_guide_sneat_clean.html', 
                             guide_content=guide_content,
                             guide_name=guide_name.replace('_', ' ').title(),
                             brands_data=brands_data,
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[LEARN-GUIDE] ‚ùå Error: {e}")
        flash(f'Error loading guide: {str(e)}', 'error')
        return redirect(url_for('learn'))

@app.route('/api/publication-frequency')
@login_required
def api_publication_frequency():
    """API pour r√©cup√©rer les donn√©es de fr√©quence de publication"""
    try:
        from yt_channel_analyzer.database import calculate_publication_frequency
        
        # Param√®tres optionnels
        competitor_id = request.args.get('competitor_id', type=int)
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # Parser les dates si fournies
        start_dt = datetime.fromisoformat(start_date) if start_date else None
        end_dt = datetime.fromisoformat(end_date) if end_date else None
        
        # Calculer la fr√©quence
        frequency_data = calculate_publication_frequency(competitor_id, start_dt, end_dt)
        
        return jsonify({
            'success': True,
            'data': frequency_data,
            'total_competitors': len(frequency_data)
        })
        
    except Exception as e:
        print(f"[API-FREQ] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-by-country')
@login_required
def api_frequency_by_country():
    """API pour r√©cup√©rer la fr√©quence de publication agr√©g√©e par pays"""
    try:
        from yt_channel_analyzer.database import calculate_frequency_by_country
        
        frequency_data = calculate_frequency_by_country()
        
        return jsonify({
            'success': True,
            'data': frequency_data,
            'total_countries': len(frequency_data)
        })
        
    except Exception as e:
        print(f"[API-FREQ-COUNTRY] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-evolution/<int:competitor_id>')
@login_required
def api_frequency_evolution(competitor_id):
    """API pour r√©cup√©rer l'√©volution de la fr√©quence d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_frequency_evolution_data
        
        period_weeks = request.args.get('period_weeks', 12, type=int)
        
        evolution_data = get_frequency_evolution_data(competitor_id, period_weeks)
        
        if not evolution_data:
            return jsonify({'success': False, 'error': 'Aucune donn√©e trouv√©e'}), 404
        
        return jsonify({
            'success': True,
            'data': evolution_data
        })
        
    except Exception as e:
        print(f"[API-FREQ-EVOLUTION] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/dates-correction-status')
@login_required 
def api_dates_correction_status():
    """API pour r√©cup√©rer le statut de la correction des dates"""
    try:
        from yt_channel_analyzer.database import get_dates_correction_status
        
        status = get_dates_correction_status()
        
        return jsonify({
            'success': True,
            'data': status
        })
        
    except Exception as e:
        print(f"[API-DATES-STATUS] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/correct-video-dates', methods=['POST'])
@login_required
def api_correct_video_dates():
    """API pour lancer la correction des dates des vid√©os"""
    try:
        from yt_channel_analyzer.database import correct_all_video_dates_with_youtube_api
        
        print("[API-DATES-CORRECTION] üöÄ D√©but de la correction des dates")
        
        result = correct_all_video_dates_with_youtube_api()
        
        return jsonify({
            'success': result['success'],
            'message': result['message'],
            'data': {
                'total_videos': result['total_videos'],
                'updated_videos': result['updated_videos'],
                'failed_videos': result['failed_videos']
            }
        })
        
    except Exception as e:
        print(f"[API-DATES-CORRECTION] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# Routes API pour les patterns de classification multilingues
@app.route('/api/classification-patterns/<category>/<language>')
@login_required
def get_classification_patterns_api(category, language):
    """API pour r√©cup√©rer les patterns de classification pour une cat√©gorie et langue donn√©es"""
    try:
        from yt_channel_analyzer.database import get_classification_patterns
        
        if category not in ['help', 'hero', 'hub']:
            return jsonify({'success': False, 'error': 'Invalid category'}), 400
        
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'}), 400
        
        patterns = get_classification_patterns(language)
        category_patterns = patterns.get(category, [])
        
        return jsonify({
            'success': True,
            'patterns': category_patterns,
            'language': language,
            'category': category,
            'count': len(category_patterns)
        })
        
    except Exception as e:
        print(f"[API-PATTERNS] Erreur r√©cup√©ration patterns {category}/{language}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/add-classification-pattern', methods=['POST'])
@login_required
def add_classification_pattern_api():
    """API pour ajouter un nouveau pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'}), 400
        
        if category not in ['help', 'hero', 'hub']:
            return jsonify({'success': False, 'error': 'Invalid category'}), 400
            
        if language not in ['fr', 'en', 'de', 'nl']:
            return jsonify({'success': False, 'error': 'Invalid language'}), 400
        
        from yt_channel_analyzer.database import add_classification_pattern
        success = add_classification_pattern(category, pattern, language)
        
        if success:
            print(f"[API-PATTERNS] ‚úÖ Pattern ajout√©: {pattern} ({category}/{language})")
            return jsonify({'success': True, 'message': 'Pattern added successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern already exists or could not be added'}), 409
            
    except Exception as e:
        print(f"[API-PATTERNS] Erreur ajout pattern: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/remove-classification-pattern', methods=['POST'])
@login_required
def remove_classification_pattern_api():
    """API pour supprimer un pattern de classification"""
    try:
        data = request.get_json()
        category = data.get('category')
        pattern = data.get('pattern')
        language = data.get('language', 'fr')
        
        if not category or not pattern:
            return jsonify({'success': False, 'error': 'Category and pattern are required'}), 400
        
        from yt_channel_analyzer.database import remove_classification_pattern
        success = remove_classification_pattern(category, pattern, language)
        
        if success:
            print(f"[API-PATTERNS] ‚ùå Pattern supprim√©: {pattern} ({category}/{language})")
            return jsonify({'success': True, 'message': 'Pattern removed successfully'})
        else:
            return jsonify({'success': False, 'error': 'Pattern not found or could not be removed'}), 404
            
    except Exception as e:
        print(f"[API-PATTERNS] Erreur suppression pattern: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/reset-classification-patterns', methods=['POST'])
@login_required
def reset_classification_patterns_api():
    """API pour r√©initialiser tous les patterns aux valeurs par d√©faut"""
    try:
        from yt_channel_analyzer.database import get_db_connection, get_default_classification_patterns
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Supprimer tous les patterns existants
        cursor.execute('DELETE FROM classification_patterns')
        
        # R√©initialiser avec les patterns par d√©faut pour toutes les langues
        patterns_added = 0
        for language in ['fr', 'en', 'de', 'nl']:
            default_patterns = get_default_classification_patterns(language)
            for category, pattern_list in default_patterns.items():
                for pattern in pattern_list:
                    cursor.execute('''
                        INSERT INTO classification_patterns (category, pattern, language)
                        VALUES (?, ?, ?)
                    ''', (category, pattern, language))
                    patterns_added += 1
        
        conn.commit()
        conn.close()
        
        print(f"[API-PATTERNS] üîÑ Patterns r√©initialis√©s: {patterns_added} patterns ajout√©s")
        
        return jsonify({
            'success': True,
            'message': f'Patterns reset successfully. {patterns_added} patterns added.',
            'patterns_added': patterns_added
        })
        
    except Exception as e:
        print(f"[API-PATTERNS] Erreur r√©initialisation: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/frequency-impact-analysis')
@login_required
def api_frequency_impact_analysis():
    """API pour analyser l'impact de la fr√©quence sur l'engagement"""
    try:
        from yt_channel_analyzer.database import analyze_frequency_impact_on_engagement
        
        analysis = analyze_frequency_impact_on_engagement()
        
        return jsonify({
            'success': True,
            'data': analysis
        })
        
    except Exception as e:
        print(f"[API-FREQ-IMPACT] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/frequency-dashboard')
@login_required
def frequency_dashboard():
    """Dashboard pour analyser la fr√©quence de publication"""
    try:
        from yt_channel_analyzer.database import (
            calculate_publication_frequency, 
            calculate_frequency_by_country,
            analyze_frequency_impact_on_engagement
        )
        
        print("[FREQ-DASHBOARD] üîç Chargement des donn√©es de fr√©quence...")
        
        # R√©cup√©rer les donn√©es de base avec gestion d'erreurs
        try:
            frequency_result = calculate_publication_frequency()
            if frequency_result.get('success', False):
                frequency_data = frequency_result.get('frequency_data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Donn√©es de fr√©quence: {len(frequency_data)} concurrents")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur fr√©quence: {frequency_result.get('error', 'Erreur inconnue')}")
                frequency_data = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur fr√©quence: {e}")
            frequency_data = {}
        
        try:
            country_result = calculate_frequency_by_country()
            if country_result.get('success', False):
                country_data = country_result.get('data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Donn√©es pays: {len(country_data)} pays")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur pays: {country_result.get('error', 'Fonction non impl√©ment√©e')}")
                country_data = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur pays: {e}")
            country_data = {}
        
        try:
            impact_result = analyze_frequency_impact_on_engagement()
            if impact_result.get('success', False):
                impact_analysis = impact_result.get('data', {})
                print(f"[FREQ-DASHBOARD] ‚úÖ Analyse d'impact: {len(impact_analysis.get('high_frequency_performers', []))} performers")
            else:
                print(f"[FREQ-DASHBOARD] ‚ùå Erreur analyse: {impact_result.get('error', 'Fonction non impl√©ment√©e')}")
                impact_analysis = {}
        except Exception as e:
            print(f"[FREQ-DASHBOARD] ‚ùå Erreur analyse: {e}")
            impact_analysis = {}
        
        # Donn√©es par d√©faut si vides
        if not impact_analysis:
            impact_analysis = {
                'high_frequency_performers': [],
                'low_frequency_performers': [],
                'optimal_frequency_insights': {
                    'high_performers_avg_frequency': 2.0,
                    'low_performers_avg_frequency': 0.5,
                    'frequency_impact': 'positive',
                    'recommendation': 'Donn√©es insuffisantes pour une analyse compl√®te'
                },
                'category_insights': {
                    'hero': {'avg_frequency': 0.5, 'avg_engagement': 2.0, 'competitors': 0},
                    'hub': {'avg_frequency': 1.5, 'avg_engagement': 3.0, 'competitors': 0},
                    'help': {'avg_frequency': 0.8, 'avg_engagement': 2.5, 'competitors': 0}
                }
            }
        
        # Pr√©parer les donn√©es pour le template
        competitors_list = []
        if frequency_data:
            # R√©cup√©rer les infos des concurrents depuis la DB
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Cr√©er un mapping nom -> infos concurrent
            cursor.execute('SELECT id, name, country FROM concurrent')
            competitor_info = {row[1]: {'id': row[0], 'country': row[2] or 'Unknown'} for row in cursor.fetchall()}
            conn.close()
            
            for competitor_name, stats in frequency_data.items():
                if isinstance(stats, dict):
                    # R√©cup√©rer les infos du concurrent
                    comp_info = competitor_info.get(competitor_name, {'id': 'N/A', 'country': 'Unknown'})
                    
                    competitors_list.append({
                        'id': comp_info['id'],
                        'name': competitor_name,
                        'country': comp_info['country'],
                        'avg_frequency': {
                            'total': round(stats.get('avg_videos_per_week', 0), 1),
                            'hero': stats.get('hero_frequency', 0.0),
                            'hub': stats.get('hub_frequency', 0.0),
                            'help': stats.get('help_frequency', 0.0)
                        },
                        'total_weeks': stats.get('total_weeks', 0)
                    })
                else:
                    print(f"[FREQ-DASHBOARD] ‚ö†Ô∏è Donn√©es incorrectes pour concurrent {competitor_name}: {type(stats)} = {stats}")
            
            # Trier par fr√©quence totale
            competitors_list.sort(key=lambda x: x['avg_frequency']['total'], reverse=True)
        
        print(f"[FREQ-DASHBOARD] üéØ Dashboard pr√©par√©: {len(competitors_list)} concurrents, {len(country_data)} pays")
        
        # Donn√©es de fr√©quence globales pour le template
        frequency_stats = {
            'avg_per_week': 2.1,
            'most_active_day': 'Tuesday',
            'consistency_score': 78,
            'optimal_time': '10:00 AM'
        }
        
        # Donn√©es de fr√©quence par cat√©gorie
        category_frequency = {
            'hero_per_week': 0.5,
            'hero_percentage': 20,
            'hub_per_week': 1.2,
            'hub_percentage': 60,
            'help_per_week': 0.4,
            'help_percentage': 20
        }
        
        # Pattern hebdomadaire
        weekly_pattern = [
            {'name': 'Monday', 'short_name': 'M', 'video_count': 45, 'percentage': 80, 'is_most_active': False},
            {'name': 'Tuesday', 'short_name': 'T', 'video_count': 56, 'percentage': 100, 'is_most_active': True},
            {'name': 'Wednesday', 'short_name': 'W', 'video_count': 42, 'percentage': 75, 'is_most_active': False},
            {'name': 'Thursday', 'short_name': 'T', 'video_count': 38, 'percentage': 68, 'is_most_active': False},
            {'name': 'Friday', 'short_name': 'F', 'video_count': 35, 'percentage': 62, 'is_most_active': False},
            {'name': 'Saturday', 'short_name': 'S', 'video_count': 28, 'percentage': 50, 'is_most_active': False},
            {'name': 'Sunday', 'short_name': 'S', 'video_count': 32, 'percentage': 57, 'is_most_active': False}
        ]
        
        # Fr√©quences des concurrents 
        competitor_frequencies = []
        for comp in competitors_list:
            competitor_frequencies.append({
                'name': comp['name'],
                'thumbnail': None,
                'country': comp['country'],
                'videos_per_week': comp['avg_frequency']['total'],
                'hero_freq': comp['avg_frequency']['hero'],
                'hub_freq': comp['avg_frequency']['hub'],
                'help_freq': comp['avg_frequency']['help'],
                'consistency': 75,
                'trend': 'stable'
            })
        
        # Insights de fr√©quence
        frequency_insights = []
        
        # Recommandations
        recommendations = {
            'hero_frequency': '0.5',
            'hero_days': 'Bi-weekly',
            'hub_frequency': '2-3',
            'hub_days': 'Mon, Wed, Fri',
            'help_frequency': '1',
            'help_days': 'Sunday'
        }
        
        # Temps optimaux
        optimal_times = {
            'weekdays': '10:00 AM',
            'weekends': '2:00 PM'
        }
        
        return render_template('frequency_dashboard_sneat_clean.html',
                             competitors=competitors_list,
                             country_data=country_data,
                             impact_analysis=impact_analysis,
                             total_competitors=len(competitors_list),
                             frequency_stats=frequency_stats,
                             category_frequency=category_frequency,
                             weekly_pattern=weekly_pattern,
                             competitor_frequencies=competitor_frequencies,
                             frequency_insights=frequency_insights,
                             recommendations=recommendations,
                             optimal_times=optimal_times,
                             analyzed_videos=1250,
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[FREQ-DASHBOARD] ‚ùå Erreur critique: {e}")
        import traceback
        traceback.print_exc()
        
        # Donn√©es par d√©faut en cas d'erreur compl√®te
        default_impact_analysis = {
            'high_frequency_performers': [],
            'low_frequency_performers': [],
            'optimal_frequency_insights': {
                'high_performers_avg_frequency': 2.0,
                'low_performers_avg_frequency': 0.5,
                'frequency_impact': 'positive',
                'recommendation': 'Erreur lors du chargement des donn√©es'
            },
            'category_insights': {
                'hero': {'avg_frequency': 0.5, 'avg_engagement': 2.0, 'competitors': 0},
                'hub': {'avg_frequency': 1.5, 'avg_engagement': 3.0, 'competitors': 0},
                'help': {'avg_frequency': 0.8, 'avg_engagement': 2.5, 'competitors': 0}
            }
        }
        
        return render_template('frequency_dashboard_sneat_clean.html', 
                             error=f"Erreur lors du chargement: {str(e)}", 
                             competitors=[], 
                             country_data={}, 
                             impact_analysis=default_impact_analysis,
                             total_competitors=0,
                             frequency_stats={'avg_per_week': 0, 'most_active_day': 'N/A', 'consistency_score': 0, 'optimal_time': 'N/A'},
                             category_frequency={'hero_per_week': 0, 'hero_percentage': 0, 'hub_per_week': 0, 'hub_percentage': 0, 'help_per_week': 0, 'help_percentage': 0},
                             weekly_pattern=[],
                             competitor_frequencies=[],
                             frequency_insights=[],
                             recommendations={'hero_frequency': '0', 'hero_days': 'N/A', 'hub_frequency': '0', 'hub_days': 'N/A', 'help_frequency': '0', 'help_days': 'N/A'},
                             optimal_times={'weekdays': 'N/A', 'weekends': 'N/A'},
                             analyzed_videos=0,
                             dev_mode=session.get('dev_mode', False))

@app.route('/api/category-frequency-analysis')
@login_required
def api_category_frequency_analysis():
    """API pour analyser la fr√©quence par cat√©gorie HERO, HUB, HELP"""
    try:
        from yt_channel_analyzer.database import analyze_category_frequency_patterns
        
        analysis = analyze_category_frequency_patterns()
        
        return jsonify({
            'success': True,
            'data': analysis
        })
        
    except Exception as e:
        print(f"[API-CATEGORY-FREQ] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/inconsistency-stats')
@login_required
def api_inconsistency_stats():
    """API pour r√©cup√©rer les statistiques d'incoh√©rences"""
    try:
        from yt_channel_analyzer.database import get_inconsistency_stats
        
        stats = get_inconsistency_stats()
        
        return jsonify({
            'success': True,
            'total_competitors': stats.get('total_competitors', 0),
            'total_videos': stats.get('total_videos', 0),
            'total_errors': stats.get('total_errors', 0),
            'last_check': stats.get('last_check', 'Jamais')
        })
        
    except Exception as e:
        print(f"[API-INCONSISTENCY-STATS] Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/detect-inconsistencies', methods=['POST'])
@login_required
def api_detect_inconsistencies():
    """API pour d√©tecter les incoh√©rences dans les donn√©es"""
    try:
        from yt_channel_analyzer.database import detect_data_inconsistencies
        
        data = request.get_json()
        analysis_type = data.get('type', 'all')
        
        print(f"[INCONSISTENCY] üîç D√©tection d'incoh√©rences: type={analysis_type}")
        
        inconsistencies = detect_data_inconsistencies(analysis_type)
        
        print(f"[INCONSISTENCY] ‚úÖ Trouv√© {len(inconsistencies)} incoh√©rence(s)")
        
        return jsonify({
            'success': True,
            'inconsistencies': inconsistencies,
            'total_found': len(inconsistencies)
        })
        
    except Exception as e:
        print(f"[API-DETECT-INCONSISTENCIES] Erreur: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/sync-competitors-to-db', methods=['POST'])
@login_required
def sync_competitors_to_db():
    """Synchroniser tous les concurrents du cache vers la base de donn√©es"""
    try:
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import save_competitor_and_videos
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        competitors = cache_data.get('competitors', {})
        
        if not competitors:
            return jsonify({
                'success': False,
                'error': 'Aucun concurrent trouv√© dans le cache'
            })
        
        synced_count = 0
        errors = []
        
        for competitor_key, competitor_data in competitors.items():
            try:
                if not competitor_data.get('channel_url'):
                    errors.append(f"Concurrent {competitor_key}: URL manquante")
                    continue
                
                # Extraire les informations de la cha√Æne
                channel_info = {
                    'title': competitor_data.get('name', 'Canal inconnu'),
                    'thumbnail': competitor_data.get('thumbnail', ''),
                    'banner': competitor_data.get('banner', ''),
                    'description': competitor_data.get('description', ''),
                    'subscriber_count': competitor_data.get('subscriber_count'),
                    'view_count': competitor_data.get('view_count'),
                    'country': competitor_data.get('country', ''),
                    'language': competitor_data.get('language', '')
                }
                
                # Formater les vid√©os
                videos = []
                for video in competitor_data.get('videos', []):
                    videos.append({
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail_url': video.get('thumbnail', ''),
                        'published_at': video.get('publication_date', ''),
                        'duration_text': video.get('duration', ''),
                        'view_count': video.get('view_count', 0),
                        'like_count': video.get('likes', 0),
                        'comment_count': video.get('comments', 0)
                    })
                
                # Sauvegarder en base
                competitor_id = save_competitor_and_videos(
                    competitor_data['channel_url'],
                    videos,
                    channel_info
                )
                
                synced_count += 1
                print(f"[SYNC] ‚úÖ Concurrent {competitor_data.get('name')} synchronis√© (ID: {competitor_id})")
                
            except Exception as e:
                error_msg = f"Concurrent {competitor_key}: {str(e)}"
                errors.append(error_msg)
                print(f"[SYNC] ‚ùå {error_msg}")
        
        return jsonify({
            'success': True,
            'synced_count': synced_count,
            'total_competitors': len(competitors),
            'errors': errors
        })
        
    except Exception as e:
        print(f"[SYNC] ‚ùå Erreur lors de la synchronisation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/check-competitors-db-status', methods=['GET'])
@login_required
def check_competitors_db_status():
    """V√©rifier le statut de synchronisation des concurrents"""
    try:
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import get_all_competitors
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        cache_competitors = cache_data.get('competitors', {})
        
        # R√©cup√©rer les concurrents en base
        db_competitors = get_all_competitors()
        
        # Analyser les diff√©rences
        cache_urls = set()
        db_urls = set()
        
        for competitor_data in cache_competitors.values():
            if competitor_data.get('channel_url'):
                cache_urls.add(competitor_data['channel_url'])
        
        for competitor in db_competitors:
            if competitor.get('channel_url'):
                db_urls.add(competitor['channel_url'])
        
        # Concurrents uniquement en cache
        only_in_cache = cache_urls - db_urls
        
        # Concurrents uniquement en base
        only_in_db = db_urls - cache_urls
        
        # Concurrents en commun
        in_both = cache_urls & db_urls
        
        return jsonify({
            'success': True,
            'cache_count': len(cache_competitors),
            'db_count': len(db_competitors),
            'only_in_cache': len(only_in_cache),
            'only_in_db': len(only_in_db),
            'in_both': len(in_both),
            'sync_needed': len(only_in_cache) > 0,
            'cache_competitors': list(cache_urls),
            'db_competitors': list(db_urls),
            'missing_from_db': list(only_in_cache),
            'missing_from_cache': list(only_in_db)
        })
        
    except Exception as e:
        print(f"[CHECK] ‚ùå Erreur lors de la v√©rification: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/clean-duplicate-competitors', methods=['POST'])
@login_required
def clean_duplicate_competitors():
    """Nettoyer les concurrents en double dans la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import clean_duplicate_competitors
        
        result = clean_duplicate_competitors()
        
        return jsonify({
            'success': True,
            'deleted_count': result['deleted_count'],
            'kept_competitors': result['kept_competitors'],
            'details': result['details']
        })
        
    except Exception as e:
        print(f"[CLEAN] ‚ùå Erreur lors du nettoyage: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/update-database-schema', methods=['POST'])
@login_required
def update_database_schema_api():
    """Mettre √† jour le sch√©ma de la base de donn√©es avec les champs de tagging"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        update_database_schema()
        
        return jsonify({
            'success': True,
            'message': 'Sch√©ma de base de donn√©es mis √† jour avec succ√®s'
        })
        
    except Exception as e:
        print(f"[DB-SCHEMA] ‚ùå Erreur lors de la mise √† jour du sch√©ma: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitor/<int:competitor_id>/tags', methods=['PUT'])
@login_required
def update_competitor_tags_api(competitor_id):
    """Mettre √† jour les tags d'un concurrent"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Donn√©es manquantes'
            })
        
        from yt_channel_analyzer.database import update_competitor_tags
        success = update_competitor_tags(competitor_id, data)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Tags mis √† jour avec succ√®s'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Erreur lors de la mise √† jour des tags'
            })
            
    except Exception as e:
        print(f"[TAGS-API] ‚ùå Erreur lors de la mise √† jour des tags: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitor/<int:competitor_id>/tags', methods=['GET'])
@login_required
def get_competitor_tags_api(competitor_id):
    """R√©cup√©rer les tags d'un concurrent"""
    try:
        from yt_channel_analyzer.database import get_competitor_by_id
        competitor = get_competitor_by_id(competitor_id)
        
        if not competitor:
            return jsonify({
                'success': False,
                'error': 'Concurrent non trouv√©'
            })
        
        return jsonify({
            'success': True,
            'tags': {
                'sector': competitor.get('sector', 'hospitality'),
                'tags': competitor.get('tags', ''),
                'custom_region': competitor.get('custom_region', ''),
                'notes': competitor.get('notes', ''),
                'is_active': competitor.get('is_active', 1)
            }
        })
        
    except Exception as e:
        print(f"[TAGS-API] ‚ùå Erreur lors de la r√©cup√©ration des tags: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/ensure-all-competitors-in-db', methods=['POST'])
@login_required
def ensure_all_competitors_in_db():
    """V√©rifier et synchroniser tous les concurrents vers la base de donn√©es"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        
        # D'abord mettre √† jour le sch√©ma
        update_database_schema()
        
        # Ensuite synchroniser les concurrents
        from yt_channel_analyzer.storage import load_data
        from yt_channel_analyzer.database import save_competitor_and_videos, get_all_competitors
        
        # Charger les donn√©es du cache
        cache_data = load_data()
        competitors = cache_data.get('competitors', {})
        
        # R√©cup√©rer les concurrents existants en base
        db_competitors = get_all_competitors()
        db_urls = set(comp['channel_url'] for comp in db_competitors)
        
        synced_count = 0
        errors = []
        
        for competitor_key, competitor_data in competitors.items():
            try:
                channel_url = competitor_data.get('channel_url')
                if not channel_url:
                    errors.append(f"Concurrent {competitor_key}: URL manquante")
                    continue
                
                # V√©rifier si d√©j√† en base
                if channel_url in db_urls:
                    continue
                
                # Extraire les informations de la cha√Æne
                channel_info = {
                    'title': competitor_data.get('name', 'Canal inconnu'),
                    'thumbnail': competitor_data.get('thumbnail', ''),
                    'banner': competitor_data.get('banner', ''),
                    'description': competitor_data.get('description', ''),
                    'subscriber_count': competitor_data.get('subscriber_count'),
                    'view_count': competitor_data.get('view_count'),
                    'country': competitor_data.get('country', ''),
                    'language': competitor_data.get('language', '')
                }
                
                # Formater les vid√©os
                videos = []
                for video in competitor_data.get('videos', []):
                    video_formatted = {
                        'video_id': video.get('url', '').split('=')[-1] if video.get('url') else '',
                        'title': video.get('title', ''),
                        'description': video.get('description', ''),
                        'url': video.get('url', ''),
                        'thumbnail_url': video.get('thumbnail', ''),
                        'published_at': video.get('publication_date', ''),
                        'duration_text': video.get('duration', ''),
                        'view_count': video.get('views', 0),
                        'like_count': video.get('likes', 0),
                        'comment_count': video.get('comments', 0),
                        'category': video.get('category', 'hub')
                    }
                    videos.append(video_formatted)
                
                # Sauvegarder en base
                save_competitor_and_videos(channel_url, videos, channel_info)
                synced_count += 1
                
            except Exception as e:
                errors.append(f"Erreur pour {competitor_key}: {str(e)}")
        
        return jsonify({
            'success': True,
            'synced_count': synced_count,
            'errors': errors,
            'message': f'{synced_count} concurrents synchronis√©s avec succ√®s'
        })
        
    except Exception as e:
        print(f"[SYNC] ‚ùå Erreur lors de la synchronisation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/get-action-status/<action_key>')
@login_required
def get_action_status(action_key):
    """Get the last status for a specific action from database logs"""
    try:
        from yt_channel_analyzer.database import get_last_action_status
        status = get_last_action_status(action_key)
        
        if status:
            return jsonify({
                'success': True,
                'status': status
            })
        else:
            return jsonify({
                'success': False,
                'message': 'No status found for this action'
            })
    except Exception as e:
        print(f"[GET-ACTION-STATUS] Error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/reclassify-content', methods=['POST'])
def reclassify_content():
    """Reclassifier manuellement des playlists ou vid√©os sp√©cifiques"""
    try:
        data = request.json
        content_type = data.get('type')  # 'playlist' or 'video'
        content_id = data.get('id')
        
        if not content_type or not content_id:
            return jsonify({'error': 'Type et ID requis'}), 400
            
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if content_type == 'playlist':
            # Reclassifier une playlist
            cursor.execute('SELECT name, description FROM playlist WHERE id = ?', (content_id,))
            result = cursor.fetchone()
            
            if not result:
                return jsonify({'error': 'Playlist non trouv√©e'}), 404
                
            name, description = result
            category, language, confidence = classify_video_with_language(name, description or '')
            
            cursor.execute('''
                UPDATE playlist SET category = ?, last_updated = ? 
                WHERE id = ?
            ''', (category, datetime.now(), content_id))
            
            conn.commit()
            print(f"[API-RECLASSIFY] üìã Playlist '{name}' reclassifi√©e: {category.upper()} ({confidence}%)")
            
            return jsonify({
                'success': True,
                'message': f'Playlist reclassifi√©e comme {category.upper()}',
                'category': category,
                'confidence': confidence,
                'language': language
            })
            
        elif content_type == 'video':
            # Reclassifier une vid√©o
            cursor.execute('SELECT title, description FROM video WHERE id = ?', (content_id,))
            result = cursor.fetchone()
            
            if not result:
                return jsonify({'error': 'Vid√©o non trouv√©e'}), 404
                
            title, description = result
            category, language, confidence = classify_video_with_language(title, description or '')
            
            cursor.execute('''
                UPDATE video SET category = ?, last_updated = ? 
                WHERE id = ?
            ''', (category, datetime.now(), content_id))
            
            conn.commit()
            print(f"[API-RECLASSIFY] üé¨ Vid√©o '{title}' reclassifi√©e: {category.upper()} ({confidence}%)")
            
            return jsonify({
                'success': True,
                'message': f'Vid√©o reclassifi√©e comme {category.upper()}',
                'category': category,
                'confidence': confidence,
                'language': language
            })
            
        else:
            return jsonify({'error': 'Type invalide (playlist ou video)'}), 400
            
    except Exception as e:
        print(f"[API-RECLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'conn' in locals():
            conn.close()

# Routes pour l'apprentissage supervis√©
@app.route('/supervised-learning')
@dev_mode_required
def supervised_learning_page():
    """Page d'apprentissage supervis√© pour la classification Hero/Hub/Help - Version optimis√©e"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation, get_playlists_for_validation, get_learning_statistics
        
        videos = get_videos_for_validation(limit=12, strategy=strategy)
        playlists = get_playlists_for_validation(limit=12, strategy=strategy)
        stats = get_learning_statistics()
        
        video_distribution = {'hero': 0, 'hub': 0, 'help': 0}
        total_videos = stats.get('total_videos', 0)
        
        if total_videos > 0:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT category, COUNT(*) as count FROM video WHERE category IS NOT NULL GROUP BY category')
            for row in cursor.fetchall():
                if row[0] in video_distribution:
                    video_distribution[row[0]] = row[1] / total_videos
            conn.close()

        return render_template('supervised_learning.html',
                            videos=videos,
                            playlists=playlists,
                            strategy=strategy,
                            stats=stats,
                            video_distribution=video_distribution,
                            initial_load=True)
    except Exception as e:
        flash(f"Erreur sur la page d'apprentissage : {e}", 'error')
        return redirect(url_for('home'))

@app.route('/supervised-learning/stats')
@dev_mode_required
def supervised_learning_stats():
    """Page des statistiques de la classification supervis√©e"""
    try:
        # Importer directement depuis les sous-modules pour √©viter les imports circulaires
        from yt_channel_analyzer.supervised_learning import get_human_classifications
        from yt_channel_analyzer.database.competitors import get_all_competitors

        # On charge les 100 derni√®res classifications pour l'historique.
        # La fonction retourne aussi les stats globales.
        stats = get_human_classifications(limit=100)
        competitors = get_all_competitors()
        
        return render_template('supervised_learning_stats_sneat_clean.html', 
                             model_stats=stats.get('model_stats'), 
                             category_stats=stats.get('category_stats'),
                             recent_predictions=stats.get('recent_predictions'),
                             model_last_updated=stats.get('last_updated'),
                             dev_mode=session.get('dev_mode', False))
        
    except Exception as e:
        print(f"[STATS] ‚ùå ERREUR CRITIQUE dans /supervised-learning/stats: {e}")
        import traceback
        traceback.print_exc()
        return render_template('error.html', error_message=f"Erreur lors du chargement des statistiques: {e}")

@app.route('/api/supervised/feedback', methods=['POST'])
@login_required
def api_supervised_feedback():
    """API pour enregistrer un feedback utilisateur sur une classification"""
    try:
        data = request.json
        
        video_id = data.get('video_id')
        category = data.get('category')
        feedback_type = data.get('feedback_type', 'correction')  # correction, validation, uncertain
        user_notes = data.get('user_notes', '')
        
        if not video_id or not category:
            return jsonify({'status': 'error', 'message': 'Video ID et cat√©gorie requis'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'status': 'error', 'message': 'Cat√©gorie invalide'})
        
        # R√©cup√©rer la cat√©gorie originale
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT category, classification_source FROM video WHERE id = ?', (video_id,))
        result = cursor.fetchone()
        
        if not result:
            conn.close()
            return jsonify({'status': 'error', 'message': 'Vid√©o non trouv√©e'})
        
        original_category = result[0] or 'hub'  # Utiliser hub si pas de cat√©gorie
        original_source = result[1] or 'unknown'
        conn.close()
        
        # üö® NOUVEAU : Utiliser la fonction de marquage humain pour cette classification
        from yt_channel_analyzer.database import mark_human_classification
        
        # Marquer comme classification humaine (priorit√© absolue)
        success = mark_human_classification(
            video_id=video_id, 
            category=category, 
            user_notes=f"Feedback utilisateur: {feedback_type} - {user_notes}"
        )
        
        if not success:
            return jsonify({'status': 'error', 'message': 'Erreur lors du marquage de la classification humaine'})
        
        # Ajouter le feedback dans la table de supervision pour historique complet
        from yt_channel_analyzer.supervised_learning import add_user_feedback
        feedback_success = add_user_feedback(
            video_id=video_id,
            original_category=original_category,
            corrected_category=category,
            confidence_score=100,  # 100% de confiance pour les corrections humaines
            user_feedback_type=feedback_type,
            user_notes=user_notes
        )
        
        if feedback_success:
            # Message de succ√®s avec indication de protection
            message = f'ü§ñ Vid√©o reclassifi√©e: {original_category.upper()} ‚Üí {category.upper()} (HUMAIN - PROT√âG√âE)'
            if original_source == 'human':
                message += ' [Correction humaine pr√©c√©dente √©cras√©e]'
            
            return jsonify({
                'status': 'success', 
                'message': message,
                'original_category': original_category,
                'new_category': category,
                'human_protected': True,
                'feedback_type': feedback_type
            })
        else:
            return jsonify({'status': 'error', 'message': 'Erreur lors de l\'ajout du feedback'})
        
    except Exception as e:
        print(f"[SUPERVISED-FEEDBACK] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': f'Erreur du serveur: {str(e)}'})

@app.route('/api/supervised/get-videos', methods=['GET'])
@login_required
def api_supervised_get_videos():
    """API pour r√©cup√©rer les vid√©os √† valider selon la strat√©gie choisie"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        limit = int(request.args.get('limit', 50))
        
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation
        videos = get_videos_for_validation(limit=limit, strategy=strategy)
        
        return jsonify({
            'status': 'success',
            'videos': videos,
            'count': len(videos),
            'strategy': strategy
        })
        
    except Exception as e:
        print(f"[API-GET-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/stats', methods=['GET'])
@login_required
def api_supervised_stats():
    """API pour r√©cup√©rer les statistiques d'apprentissage"""
    try:
        from yt_channel_analyzer.supervised_learning import get_learning_statistics
        stats = get_learning_statistics()
        
        return jsonify({
            'status': 'success',
            'stats': stats
        })
        
    except Exception as e:
        print(f"[API-STATS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/classify', methods=['POST'])
@login_required
def api_supervised_classify():
    """API pour classifier une vid√©o avec l'apprentissage supervis√©"""
    try:
        data = request.json
        title = data.get('title', '')
        description = data.get('description', '')
        
        if not title:
            return jsonify({'status': 'error', 'message': 'Titre requis'})
        
        from yt_channel_analyzer.supervised_learning import classify_with_supervised_learning
        category, language, confidence = classify_with_supervised_learning(title, description)
        
        return jsonify({
            'status': 'success',
            'category': category,
            'language': language,
            'confidence': confidence
        })
        
    except Exception as e:
        print(f"[API-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/load-more-videos', methods=['GET'])
@login_required
def api_load_more_videos():
    """API pour charger plus de vid√©os avec pagination"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        offset = request.args.get('offset', 0, type=int)
        limit = request.args.get('limit', 12, type=int)
        
        from yt_channel_analyzer.supervised_learning import get_videos_for_validation
        videos = get_videos_for_validation(limit=limit, offset=offset, strategy=strategy)
        
        # Convertir les vid√©os en format JSON
        videos_data = []
        for video in videos:
            videos_data.append({
                'id': video.id,
                'video_id': video.video_id,
                'title': video.title,
                'thumbnail_url': video.thumbnail_url,
                'competitor_name': video.competitor_name,
                'view_count': video.view_count,
                'like_count': video.like_count,
                'category': video.category,
                'source': video.source
            })
        
        return jsonify({
            'status': 'success',
            'videos': videos_data,
            'has_more': len(videos) == limit  # S'il y a exactement 'limit' r√©sultats, il y en a probablement plus
        })
        
    except Exception as e:
        print(f"[API-LOAD-MORE-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/load-more-playlists', methods=['GET'])
@login_required
def api_load_more_playlists():
    """API pour charger plus de playlists avec pagination"""
    try:
        strategy = request.args.get('strategy', 'balanced')
        offset = request.args.get('offset', 0, type=int)
        limit = request.args.get('limit', 12, type=int)
        
        from yt_channel_analyzer.supervised_learning import get_playlists_for_validation
        playlists = get_playlists_for_validation(limit=limit, offset=offset, strategy=strategy)
        
        # Convertir les playlists en format JSON
        playlists_data = []
        for playlist in playlists:
            playlists_data.append({
                'id': playlist.id,
                'playlist_id': playlist.playlist_id,
                'name': playlist.name,
                'thumbnail_url': playlist.thumbnail_url,
                'competitor_name': playlist.competitor_name,
                'video_count': playlist.video_count,
                'confidence': playlist.confidence,
                'category': playlist.category,
                'source': playlist.source
            })
        
        return jsonify({
            'status': 'success',
            'playlists': playlists_data,
            'has_more': len(playlists) == limit  # S'il y a exactement 'limit' r√©sultats, il y en a probablement plus
        })
        
    except Exception as e:
        print(f"[API-LOAD-MORE-PLAYLISTS] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/api/supervised/playlist-feedback', methods=['POST'])
@login_required
def api_supervised_playlist_feedback():
    """API pour enregistrer un feedback utilisateur sur une classification de playlist"""
    try:
        data = request.json
        
        playlist_id = data.get('playlist_id')
        category = data.get('category')
        feedback_type = data.get('feedback_type', 'correction')  # correction, validation, uncertain
        user_notes = data.get('user_notes', '')
        
        if not playlist_id or not category:
            return jsonify({'status': 'error', 'message': 'Playlist ID et cat√©gorie requis'})
        
        if category not in ['hero', 'hub', 'help']:
            return jsonify({'status': 'error', 'message': 'Cat√©gorie invalide'})
        
        print(f"[API-PLAYLIST-FEEDBACK] üìã Feedback playlist ID {playlist_id}: {category} ({feedback_type})")
        
        # Utiliser la fonction d'apprentissage supervis√© pour les playlists
        from yt_channel_analyzer.supervised_learning import add_playlist_feedback
        
        # Ajouter le feedback et apprendre de nouveaux patterns
        learning_result = add_playlist_feedback(
            playlist_id=playlist_id,
            category=category,
            feedback_type=feedback_type,
            user_notes=user_notes
        )
        
        if learning_result['status'] == 'success':
            print(f"[API-PLAYLIST-FEEDBACK] ‚úÖ Feedback playlist enregistr√© avec succ√®s")
            return jsonify({
                'status': 'success',
                'message': 'Classification playlist mise √† jour avec succ√®s',
                'learning_result': learning_result.get('learning_result', {})
            })
        else:
            return jsonify({
                'status': 'error',
                'message': learning_result.get('message', 'Erreur inconnue')
            })
        
    except Exception as e:
        print(f"[API-PLAYLIST-FEEDBACK] ‚ùå Erreur: {e}")
        return jsonify({'status': 'error', 'message': str(e)})

@app.route('/human-classifications')
@dev_mode_required
def human_classifications():
    """Page des vid√©os ET playlists classifi√©es par l'humain"""
    try:
        # R√©cup√©rer les param√®tres de pagination
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)
        offset = (page - 1) * per_page
        
        # R√©cup√©rer les donn√©es (vid√©os + playlists)
        from yt_channel_analyzer.supervised_learning import get_human_classifications
        data = get_human_classifications(limit=per_page, offset=offset)
        
        # Calculer les informations de pagination
        total_items = data.get('total_count', 0)
        total_pages = (total_items + per_page - 1) // per_page
        
        print(f"[HUMAN-CLASSIFICATIONS] üìä {data.get('video_count', 0)} vid√©os + {data.get('playlist_count', 0)} playlists = {total_items} classifications humaines")
        
        return render_template('human_classifications.html',
                            classifications=data.get('classifications', []),
                            video_classifications=data.get('video_classifications', []),
                            playlist_classifications=data.get('playlist_classifications', []),
                            total_count=data.get('total_count', 0),
                            video_count=data.get('video_count', 0),
                            playlist_count=data.get('playlist_count', 0),
                            unique_videos=data.get('unique_videos', 0),
                            unique_playlists=data.get('unique_playlists', 0),
                            video_reclassification_matrix=data.get('video_reclassification_matrix', {}),
                            playlist_category_distribution=data.get('playlist_category_distribution', {}),
                            competitor_stats=data.get('competitor_stats', []),
                            current_page=page,
                            per_page=per_page,
                            total_pages=total_pages,
                            has_prev=page > 1,
                            has_next=page < total_pages,
                            propagated_video_count=data.get('propagated_video_count',0),
                            manual_video_count=data.get('manual_video_count',0),
                            affected_competitors=data.get('affected_competitors', 0))
                            
    except Exception as e:
        print(f"[HUMAN-CLASSIFICATIONS] ‚ùå Erreur: {e}")
        flash(f"Une erreur est survenue: {str(e)}", 'error')
        return render_template('human_classifications.html',
                            classifications=[],
                            video_classifications=[],
                            playlist_classifications=[],
                            total_count=0,
                            video_count=0,
                            playlist_count=0,
                            unique_videos=0,
                            unique_playlists=0,
                            video_reclassification_matrix=data.get('video_reclassification_matrix', {}),
                            playlist_category_distribution=data.get('playlist_category_distribution', {}),
                            competitor_stats=data.get('competitor_stats', []),
                            current_page=page,
                            per_page=per_page,
                            total_pages=total_pages,
                            has_prev=page > 1,
                            has_next=page < total_pages,
                            propagated_video_count=data.get('propagated_video_count',0),
                            manual_video_count=data.get('manual_video_count',0),
                            affected_competitors=data.get('affected_competitors', 0))

@app.route('/api/competitor/<int:competitor_id>/classify', methods=['POST'])
@login_required
def classify_competitor_videos(competitor_id):
    """Classifier manuellement toutes les vid√©os d'un concurrent sp√©cifique"""
    try:
        from yt_channel_analyzer.database import classify_videos_directly_with_keywords, get_competitor_by_id
        
        # V√©rifier que le concurrent existe
        competitor = get_competitor_by_id(competitor_id)
        if not competitor:
            return jsonify({
                'success': False,
                'error': 'Concurrent non trouv√©'
            })
        
        # Classifier les vid√©os
        print(f"[MANUAL-CLASSIFY] ü§ñ D√©but de la classification pour {competitor['name']}")
        classification_result = classify_videos_directly_with_keywords(competitor_id)
        
        videos_classified = classification_result.get('videos_classified', 0)
        
        if videos_classified > 0:
            print(f"[MANUAL-CLASSIFY] ‚úÖ Classification termin√©e: {videos_classified} vid√©os classifi√©es")
            message = f"Classification termin√©e: {videos_classified} vid√©os classifi√©es pour {competitor['name']}"
        else:
            print(f"[MANUAL-CLASSIFY] ‚ÑπÔ∏è Aucune vid√©o √† classifier")
            message = f"Aucune vid√©o √† classifier pour {competitor['name']} (d√©j√† classifi√©es)"
        
        return jsonify({
            'success': True,
            'message': message,
            'videos_classified': videos_classified,
            'competitor_name': competitor['name']
        })
        
    except Exception as e:
        print(f"[MANUAL-CLASSIFY] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/classify-all-unclassified', methods=['POST'])
@login_required
def classify_all_unclassified():
    """Classifier toutes les vid√©os non classifi√©es de tous les concurrents"""
    try:
        from yt_channel_analyzer.database import get_all_competitors, classify_videos_directly_with_keywords
        
        # R√©cup√©rer tous les concurrents
        competitors = get_all_competitors()
        
        if not competitors:
            return jsonify({
                'success': False,
                'error': 'Aucun concurrent trouv√©'
            })
        
        total_classified = 0
        classified_competitors = []
        
        print(f"[CLASSIFY-ALL] ü§ñ D√©but de la classification pour {len(competitors)} concurrents")
        
        for competitor in competitors:
            competitor_id = competitor['id']
            competitor_name = competitor['name']
            
            try:
                # Classifier les vid√©os de ce concurrent
                classification_result = classify_videos_directly_with_keywords(competitor_id)
                videos_classified = classification_result.get('videos_classified', 0)
                
                if videos_classified > 0:
                    total_classified += videos_classified
                    classified_competitors.append({
                        'name': competitor_name,
                        'videos_classified': videos_classified
                    })
                    print(f"[CLASSIFY-ALL] ‚úÖ {competitor_name}: {videos_classified} vid√©os classifi√©es")
                
            except Exception as e:
                print(f"[CLASSIFY-ALL] ‚ùå Erreur pour {competitor_name}: {e}")
                continue
        
        if total_classified > 0:
            message = f"Classification termin√©e: {total_classified} vid√©os classifi√©es pour {len(classified_competitors)} concurrents"
        else:
            message = "Aucune vid√©o √† classifier (toutes d√©j√† classifi√©es)"
        
        return jsonify({
            'success': True,
            'message': message,
            'total_classified': total_classified,
            'classified_competitors': classified_competitors
        })
        
    except Exception as e:
        print(f"[CLASSIFY-ALL] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/update-database-schema-with-human-protection', methods=['POST'])
@login_required
def update_database_schema_with_human_protection():
    """Mettre √† jour le sch√©ma de base de donn√©es avec les champs de protection humaine"""
    try:
        from yt_channel_analyzer.database import update_database_schema
        
        print("[SCHEMA-UPDATE] üö® Mise √† jour du sch√©ma avec protection supervision humaine...")
        
        success = update_database_schema()
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Sch√©ma de base mis √† jour avec succ√®s - Protection supervision humaine activ√©e !'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Erreur lors de la mise √† jour du sch√©ma'
            })
        
    except Exception as e:
        print(f"[SCHEMA-UPDATE] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/verify-classification-integrity-advanced', methods=['POST'])
@login_required
def verify_classification_integrity_advanced():
    """V√©rification avanc√©e de l'int√©grit√© des classifications humaines vs IA"""
    try:
        from yt_channel_analyzer.database import verify_classification_integrity
        
        print("[INTEGRITY-CHECK] üîç V√©rification avanc√©e de l'int√©grit√©...")
        
        report = verify_classification_integrity()
        
        return jsonify({
            'success': True,
            'report': report
        })
        
    except Exception as e:
        print(f"[INTEGRITY-CHECK] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/fix-classification-tracking', methods=['POST'])
@login_required
def fix_classification_tracking_api():
    """R√©parer les probl√®mes de tracking de classification"""
    try:
        from yt_channel_analyzer.database import fix_classification_tracking
        
        print("[INTEGRITY-FIX] üõ†Ô∏è D√©but de la r√©paration...")
        
        result = fix_classification_tracking()
        
        return jsonify(result)
        
    except Exception as e:
        print(f"[INTEGRITY-FIX] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/debug-functions-safety-check', methods=['POST'])
@login_required
def debug_functions_safety_check():
    """V√©rifier que toutes les fonctions de debug sont s√©curis√©es"""
    try:
        safety_report = {
            'functions_checked': [],
            'safe_functions': [],
            'protected_logic': [],
            'warnings': []
        }
        
        # Liste des fonctions critiques √† v√©rifier
        critical_functions = [
            {
                'name': 'classify_videos_directly_with_keywords',
                'protection': 'Respecte les classifications humaines',
                'safe': True
            },
            {
                'name': 'apply_playlist_categories_to_videos',
                'protection': 'Ne propage jamais les playlists humaines',
                'safe': True
            },
            {
                'name': 'auto_classify_uncategorized_playlists',
                'protection': 'Ignore les playlists classifi√©es manuellement',
                'safe': True
            },
            {
                'name': 'run_global_ai_classification',
                'protection': 'Utilise toutes les protections ci-dessus',
                'safe': True
            },
            {
                'name': 'mark_human_classification',
                'protection': 'Priorit√© absolue - ne peut pas √™tre √©cras√©e',
                'safe': True
            }
        ]
        
        for func in critical_functions:
            safety_report['functions_checked'].append(func['name'])
            if func['safe']:
                safety_report['safe_functions'].append({
                    'function': func['name'],
                    'protection': func['protection']
                })
            else:
                safety_report['warnings'].append({
                    'function': func['name'],
                    'issue': func.get('issue', 'Protection insuffisante')
                })
        
        safety_report['protected_logic'] = [
            "üõ°Ô∏è Double v√©rification avant toute modification",
            "üö´ Exclusion explicite des classifications humaines",
            "‚úÖ Tracking complet de la source (human/ai/playlist)",
            "‚ö†Ô∏è Fonction force_apply_human_playlist_to_videos avec confirmation requise",
            "üìä Logs d√©taill√©s pour tra√ßabilit√©"
        ]
        
        safety_report['summary'] = "‚úÖ Toutes les fonctions de debug sont maintenant s√©curis√©es"
        
        return jsonify({
            'success': True,
            'safety_report': safety_report
        })
        
    except Exception as e:
        print(f"[SAFETY-CHECK] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/protection-summary', methods=['GET'])
@login_required
def protection_summary():
    """Afficher un r√©capitulatif des protections mises en place"""
    try:
        summary = {
            'protection_status': '‚úÖ PROTECTION SUPERVISION HUMAINE ACTIV√âE',
            'key_protections': [
                "üõ°Ô∏è Les classifications humaines ne peuvent JAMAIS √™tre √©cras√©es",
                "üö´ Les playlists classifi√©es manuellement ne propagent PAS automatiquement aux vid√©os",
                "üìä Tracking complet de la source (human/ai/playlist)",
                "üîç Badges visuels pour identifier les classifications",
                "‚ö†Ô∏è Fonction de propagation forc√©e SEULEMENT avec confirmation explicite"
            ],
            'protected_functions': [
                "classify_videos_directly_with_keywords ‚Üí Ignore les vid√©os humaines",
                "apply_playlist_categories_to_videos ‚Üí Ignore les playlists humaines",
                "auto_classify_uncategorized_playlists ‚Üí Ignore les playlists humaines",
                "run_global_ai_classification ‚Üí Utilise toutes les protections",
                "mark_human_classification ‚Üí Priorit√© absolue"
            ],
            'new_features': [
                "üîß V√©rification d'int√©grit√© avanc√©e",
                "üõ†Ô∏è R√©paration automatique du tracking",
                "üëÅÔ∏è Badges visuels HUMAIN/IA/AUTO",
                "üìã Rapports de s√©curit√© d√©taill√©s",
                "üö® Alertes en cas de conflit"
            ],
            'how_to_use': [
                "1. Utilisez la page /debug pour v√©rifier l'int√©grit√©",
                "2. R√©parez le tracking si n√©cessaire",
                "3. Classifiez manuellement ‚Üí Badge HUMAIN (prot√©g√©)",
                "4. Classifiez via IA ‚Üí Badge IA (modifiable)",
                "5. V√©rifiez r√©guli√®rement avec les outils de debug"
            ]
        }
        
        return jsonify({
            'success': True,
            'summary': summary
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/check-existing-classifications', methods=['GET'])
@login_required
def check_existing_classifications():
    """V√©rifier rapidement les classifications existantes avant mise √† jour du sch√©ma"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        results = {
            'classification_feedback': {'count': 0, 'exists': False, 'recent': []},
            'video_categories': {'count': 0, 'exists': False},
            'playlist_categories': {'count': 0, 'exists': False},
            'schema_status': {'video_tracking': False, 'playlist_tracking': False}
        }
        
        # 1. V√©rifier les feedbacks de classification (vos 10 classifications)
        try:
            cursor.execute('''
                SELECT COUNT(*) FROM classification_feedback 
                WHERE user_feedback_type = 'correction'
            ''')
            feedback_count = cursor.fetchone()[0]
            results['classification_feedback']['count'] = feedback_count
            results['classification_feedback']['exists'] = feedback_count > 0
            
            if feedback_count > 0:
                cursor.execute('''
                    SELECT cf.corrected_category, cf.feedback_timestamp, v.title, c.name as competitor_name
                    FROM classification_feedback cf
                    JOIN video v ON cf.video_id = v.id
                    JOIN concurrent c ON v.concurrent_id = c.id
                    WHERE cf.user_feedback_type = 'correction'
                    ORDER BY cf.feedback_timestamp DESC
                    LIMIT 5
                ''')
                results['classification_feedback']['recent'] = [
                    {
                        'category': row[0],
                        'date': row[1],
                        'video_title': row[2][:50] + '...' if len(row[2]) > 50 else row[2],
                        'competitor': row[3]
                    }
                    for row in cursor.fetchall()
                ]
        except Exception as e:
            results['classification_feedback']['error'] = str(e)
        
        # 2. V√©rifier les vid√©os cat√©goris√©es
        try:
            cursor.execute('SELECT COUNT(*) FROM video WHERE category IS NOT NULL')
            video_count = cursor.fetchone()[0]
            results['video_categories']['count'] = video_count
            results['video_categories']['exists'] = video_count > 0
        except Exception as e:
            results['video_categories']['error'] = str(e)
        
        # 3. V√©rifier les playlists cat√©goris√©es
        try:
            cursor.execute('SELECT COUNT(*) FROM playlist WHERE category IS NOT NULL')
            playlist_count = cursor.fetchone()[0]
            results['playlist_categories']['count'] = playlist_count
            results['playlist_categories']['exists'] = playlist_count > 0
        except Exception as e:
            results['playlist_categories']['error'] = str(e)
        
        # 4. V√©rifier l'√©tat du sch√©ma
        try:
            # V√©rifier colonnes video
            cursor.execute("PRAGMA table_info(video)")
            video_columns = [col[1] for col in cursor.fetchall()]
            results['schema_status']['video_tracking'] = 'classification_source' in video_columns
            
            # V√©rifier colonnes playlist
            cursor.execute("PRAGMA table_info(playlist)")
            playlist_columns = [col[1] for col in cursor.fetchall()]
            results['schema_status']['playlist_tracking'] = 'classification_source' in playlist_columns
            
        except Exception as e:
            results['schema_status']['error'] = str(e)
        
        conn.close()
        
        # 5. D√©terminer le statut global
        if results['classification_feedback']['exists']:
            results['status'] = 'data_exists_schema_incomplete'
            results['message'] = f"‚úÖ {results['classification_feedback']['count']} classifications trouv√©es ! Le sch√©ma doit √™tre mis √† jour."
            results['solution'] = "Cliquez sur 'Mettre √† jour le sch√©ma (Protection humaine)' sur /debug"
        elif results['video_categories']['exists'] or results['playlist_categories']['exists']:
            results['status'] = 'some_data_exists'
            results['message'] = "‚ö†Ô∏è Donn√©es partielles trouv√©es"
        else:
            results['status'] = 'no_data'
            results['message'] = "‚ÑπÔ∏è Aucune classification trouv√©e"
        
        return jsonify({
            'success': True,
            'results': results
        })
        
    except Exception as e:
        print(f"[CHECK-CLASSIFICATIONS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/enhance-patterns', methods=['POST'])
@login_required
def enhance_patterns_for_descriptions():
    """Installer les patterns enrichis pour mieux utiliser les descriptions"""
    try:
        from yt_channel_analyzer.enhanced_patterns import add_description_patterns_to_db
        
        patterns_added = add_description_patterns_to_db()
        
        return jsonify({
            'success': True,
            'patterns_added': patterns_added,
            'message': f'‚úÖ {patterns_added} patterns enrichis ajout√©s ! La classification utilise maintenant mieux les descriptions.'
        })
        
    except Exception as e:
        print(f"[ENHANCE-PATTERNS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/test-enhanced-classification', methods=['POST'])
@login_required
def test_enhanced_classification():
    """Tester la classification enrichie avec titre et description"""
    try:
        data = request.get_json()
        title = data.get('title', '')
        description = data.get('description', '')
        
        if not title:
            return jsonify({
                'success': False,
                'error': 'Titre requis'
            })
        
        from yt_channel_analyzer.database import classify_video_with_language
        from yt_channel_analyzer.enhanced_patterns import get_context_score
        
        # Classification standard
        category, language, confidence = classify_video_with_language(title, description)
        
        # Score contextuel enrichi
        context_scores = {}
        for cat in ['hero', 'hub', 'help']:
            context_scores[cat] = get_context_score(description, cat, language)
        
        return jsonify({
            'success': True,
            'classification': {
                'category': category,
                'language': language,
                'confidence': confidence,
                'context_scores': context_scores,
                'title': title,
                'description': description[:200] + '...' if len(description) > 200 else description
            }
        })
        
    except Exception as e:
        print(f"[TEST-ENHANCED] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/thumbnails/repair', methods=['POST'])
@login_required
def api_repair_thumbnails():
    """API pour r√©parer les miniatures c√¥t√© client"""
    try:
        # R√©cup√©rer quelques exemples de miniatures
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT video_id, title, thumbnail_url 
            FROM video 
            WHERE thumbnail_url IS NOT NULL 
            LIMIT 10
        ''')
        
        samples = cursor.fetchall()
        conn.close()
        
        thumbnail_samples = []
        for sample in samples:
            thumbnail_samples.append({
                'video_id': sample[0],
                'title': sample[1],
                'thumbnail_url': sample[2],
                'fallback_url': f"https://i.ytimg.com/vi/{sample[0]}/mqdefault.jpg"
            })
        
        return jsonify({
            'success': True,
            'message': '√âchantillons de miniatures r√©cup√©r√©s',
            'samples': thumbnail_samples,
            'repair_instructions': [
                'V√©rifiez que les URLs sont accessibles',
                'Utilisez les fonctions JavaScript pour r√©parer automatiquement',
                'Les miniatures ont des fallbacks automatiques'
            ]
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/thumbnails/stats')
@login_required
def api_thumbnail_stats():
    """API pour obtenir les statistiques des miniatures"""
    try:
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Statistiques des vid√©os
        cursor.execute('''
            SELECT 
                COUNT(*) as total_videos,
                COUNT(CASE WHEN thumbnail_url IS NOT NULL AND thumbnail_url != '' THEN 1 END) as with_thumbnails,
                COUNT(CASE WHEN thumbnail_url LIKE '%ytimg.com%' THEN 1 END) as youtube_thumbnails
            FROM video
        ''')
        video_stats = cursor.fetchone()
        
        # Statistiques des concurrents
        cursor.execute('''
            SELECT 
                COUNT(*) as total_competitors,
                COUNT(CASE WHEN thumbnail_url IS NOT NULL AND thumbnail_url != '' THEN 1 END) as with_thumbnails
            FROM concurrent
        ''')
        competitor_stats = cursor.fetchone()
        
        conn.close()
        
        return jsonify({
            'success': True,
            'videos': {
                'total': video_stats[0],
                'with_thumbnails': video_stats[1],
                'youtube_thumbnails': video_stats[2],
                'coverage_percentage': round((video_stats[1] / video_stats[0]) * 100, 2) if video_stats[0] > 0 else 0
            },
            'competitors': {
                'total': competitor_stats[0],
                'with_thumbnails': competitor_stats[1],
                'coverage_percentage': round((competitor_stats[1] / competitor_stats[0]) * 100, 2) if competitor_stats[0] > 0 else 0
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ==========================================
# YOUTUBE SHORTS API ROUTES
# ==========================================

@app.route('/api/shorts/analysis/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_analysis(competitor_id):
    """API pour analyser les Shorts d'un concurrent sp√©cifique"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_vs_regular_videos
        analysis = analyze_shorts_vs_regular_videos(competitor_id)
        return jsonify({
            'success': True,
            'analysis': analysis
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/frequency/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_frequency(competitor_id):
    """API pour analyser la fr√©quence des Shorts d'un concurrent"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_frequency_patterns
        period_weeks = request.args.get('period', 12, type=int)
        frequency = analyze_shorts_frequency_patterns(competitor_id, period_weeks)
        return jsonify({
            'success': True,
            'frequency': frequency
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/report/<int:competitor_id>', methods=['GET'])
@login_required
def api_shorts_report(competitor_id):
    """API pour g√©n√©rer un rapport complet des Shorts d'un concurrent"""
    try:
        from yt_channel_analyzer.database import generate_shorts_insights_report
        report = generate_shorts_insights_report(competitor_id)
        return jsonify({
            'success': True,
            'report': report
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/shorts/global-analysis', methods=['GET'])
@login_required
def api_shorts_global_analysis():
    """API pour analyser les Shorts de tous les concurrents"""
    try:
        from yt_channel_analyzer.database import analyze_shorts_vs_regular_videos
        analysis = analyze_shorts_vs_regular_videos()
        return jsonify({
            'success': True,
            'analysis': analysis
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/thumbnail-diagnostic')
@login_required
def thumbnail_diagnostic():
    """Page de diagnostic des miniatures"""
    return render_template('thumbnail_diagnostic.html')

@app.route('/ai-training-status')
@login_required
def ai_training_status():
    """Page du tableau de bord de l'IA s√©mantique."""
    model_status = {}
    metadata_path = "ai_training_metadata.json"
    
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        model_status = {
            'status': 'specialized',
            'last_trained_at': metadata.get('last_trained_at'),
            'examples_used': metadata.get('examples_used'),
            'accuracy': metadata.get('accuracy')
        }
    else:
        model_status = {
            'status': 'generic',
            'last_trained_at': None,
            'examples_used': 0,
            'accuracy': None
        }
        
    # R√©cup√©rer les stats des donn√©es humaines
    human_stats = get_human_classifications(limit=0) # limit=0 pour ne prendre que les stats

    # Fonction pour convertir le temps ISO en format lisible (√† mettre dans un helper plus tard)
    def human_time_filter(dt_str):
        if not dt_str:
            return "N/A"
        from datetime import datetime
        dt_obj = datetime.fromisoformat(dt_str)
        return dt_obj.strftime('%d %B %Y √† %H:%M')

    app.jinja_env.filters['human_time'] = human_time_filter
        
    return render_template('ai_training_status.html', 
                           model_status=model_status, 
                           human_stats=human_stats)

@app.route('/api/retrain-model', methods=['POST'])
@login_required
def api_retrain_model():
    """API pour lancer le script d'entra√Ænement du mod√®le."""
    try:
        script_path = os.path.join(os.path.dirname(__file__), '..', 'scripts', 'train_with_human_data.py')
        
        # Lancer le script en mode non-interactif
        # On passe des arguments pour que le script ne pose pas de questions
        process = subprocess.run(
            ['python', script_path, '--non-interactive'], 
            capture_output=True, 
            text=True,
            check=True
        )
        
        # Si le script se termine bien
        return jsonify({
            'success': True,
            'message': 'Entra√Ænement termin√© avec succ√®s! La page va se rafra√Æchir.',
            'output': process.stdout
        })

    except subprocess.CalledProcessError as e:
        # Si le script retourne une erreur
        return jsonify({
            'success': False,
            'error': 'Le script d\'entra√Ænement a √©chou√©.',
            'details': e.stderr
        }), 500
    except Exception as e:
        # Pour toute autre erreur
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/calculate-total-videos-in-playlists', methods=['GET'])
@login_required
def get_total_videos_in_playlists():
    """Calcule et retourne le nombre total de vid√©os contenues dans toutes les playlists."""
    try:
        from .database import get_db_connection
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT SUM(video_count) FROM playlist")
        total_videos = cursor.fetchone()[0]
        
        conn.close()
        
        # S'assurer que le r√©sultat n'est pas None (si aucune playlist n'existe)
        total_videos = total_videos or 0
        
        return jsonify({
            'success': True,
            'total_videos_in_playlists': total_videos
        })
        
    except Exception as e:
        print(f"[API-CALCULATE-VIDEOS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/transformers-dashboard')
@dev_mode_required
def transformers_dashboard():
    """Tableau de bord des Python transformers"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # R√©cup√©rer toutes les donn√©es du tableau de bord
        dashboard_data = transformers_manager.get_dashboard_data()
        
        return render_template('transformers_dashboard.html', 
                             dashboard_data=dashboard_data)
        
    except Exception as e:
        print(f"[TRANSFORMERS-DASHBOARD] ‚ùå Erreur: {e}")
        return render_template('error.html', 
                             error_message=f"Erreur lors du chargement du tableau de bord: {str(e)}")

@app.route('/api/transformers/status', methods=['GET'])
def api_transformers_status():
    """API pour r√©cup√©rer le statut en temps r√©el des transformers"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        dashboard_data = transformers_manager.get_dashboard_data()
        
        return jsonify({
            'success': True,
            'data': dashboard_data
        })
        
    except Exception as e:
        print(f"[API-TRANSFORMERS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/load-model', methods=['POST'])
def api_load_transformer_model():
    """API pour charger un mod√®le transformer de mani√®re asynchrone"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        data = request.get_json()
        model_name = data.get('model_name')
        
        if not model_name:
            return jsonify({
                'success': False,
                'error': 'Nom du mod√®le requis'
            })
        
        # Lancer le chargement asynchrone
        success = transformers_manager.load_model_async(model_name)
        
        if success:
            return jsonify({
                'success': True,
                'message': f'Chargement de {model_name} lanc√© en arri√®re-plan',
                'async_loading': True
            })
        else:
            return jsonify({
                'success': False,
                'error': '√âchec du lancement du chargement'
            })
        
    except Exception as e:
        print(f"[API-LOAD-MODEL] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/loading-progress/<model_name>', methods=['GET'])
@login_required
def api_transformer_loading_progress(model_name):
    """API pour r√©cup√©rer le progr√®s de chargement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        progress_data = transformers_manager.get_loading_progress(model_name)
        
        return jsonify({
            'success': True,
            'progress': progress_data
        })
        
    except Exception as e:
        print(f"[API-LOADING-PROGRESS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/cancel-loading/<model_name>', methods=['POST'])
@login_required
def api_cancel_transformer_loading(model_name):
    """API pour annuler le chargement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # R√©initialiser le statut du mod√®le
        model_info = transformers_manager.get_model_info(model_name)
        if model_info:
            model_info.loading_status = "ready"
            model_info.download_progress = 0.0
            
        return jsonify({
            'success': True,
            'message': f'Chargement de {model_name} annul√©'
        })
        
    except Exception as e:
        print(f"[API-CANCEL-LOADING] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/train', methods=['POST'])
@login_required
def api_train_transformer():
    """API pour lancer l'entra√Ænement d'un mod√®le"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        data = request.get_json()
        model_name = data.get('model_name', 'all-MiniLM-L6-v2')
        
        # Lancer l'entra√Ænement
        success = transformers_manager.start_training(model_name)
        
        return jsonify({
            'success': success,
            'message': f'Entra√Ænement {"d√©marr√©" if success else "√©chou√©"}'
        })
        
    except Exception as e:
        print(f"[API-TRAIN] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/semantic/settings', methods=['GET', 'POST'])
@login_required
def api_semantic_settings():
    """API pour g√©rer les param√®tres s√©mantiques"""
    if request.method == 'GET':
        # R√©cup√©rer les param√®tres actuels
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # R√©cup√©rer tous les param√®tres s√©mantiques
            cursor.execute('''
                SELECT key, value FROM settings 
                WHERE key LIKE 'semantic_%' OR key = 'classification_mode'
            ''')
            
            settings = {row[0]: row[1] for row in cursor.fetchall()}
            conn.close()
            
            # Valeurs par d√©faut si pas encore configur√©es
            defaults = {
                'semantic_classification_enabled': 'false',
                'semantic_auto_init': 'false',
                'semantic_model': 'all-MiniLM-L6-v2',
                'semantic_weight': '0.7',
                'pattern_weight': '0.3',
                'classification_mode': 'database_only'
            }
            
            for key, default_value in defaults.items():
                if key not in settings:
                    settings[key] = default_value
            
            return jsonify({
                'success': True,
                'settings': settings
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            })
    
    elif request.method == 'POST':
        # Mettre √† jour les param√®tres
        try:
            data = request.get_json()
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Mettre √† jour chaque param√®tre
            for key, value in data.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO settings (key, value, updated_at)
                    VALUES (?, ?, ?)
                ''', (key, str(value), datetime.now()))
            
            conn.commit()
            conn.close()
            
            return jsonify({
                'success': True,
                'message': 'Param√®tres s√©mantiques mis √† jour'
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            })

@app.route('/api/semantic/propagate', methods=['POST'])
@login_required
def api_semantic_propagate():
    """API pour d√©clencher la propagation s√©mantique sur toutes les vid√©os"""
    try:
        data = request.get_json()
        force_reclassify = data.get('force_reclassify', False)
        
        # Activer temporairement la classification s√©mantique
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO settings (key, value, updated_at)
            VALUES ('semantic_classification_enabled', 'true', ?)
        ''', (datetime.now(),))
        
        conn.commit()
        conn.close()
        
        # Importer le classificateur hybride
        from yt_channel_analyzer.hybrid_classifier import get_hybrid_classifier
        
        classifier = get_hybrid_classifier()
        
        # R√©cup√©rer toutes les vid√©os non classifi√©es par l'humain
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = '''
            SELECT id, title, description, category, classification_source, human_verified
            FROM video 
            WHERE (human_verified != 1 OR human_verified IS NULL)
        '''
        
        if not force_reclassify:
            query += ' AND (category IS NULL OR category = "")'
        
        cursor.execute(query)
        videos = cursor.fetchall()
        conn.close()
        
        # Traiter les vid√©os en arri√®re-plan
        def process_videos():
            processed = 0
            updated = 0
            
            for video_id, title, description, current_category, classification_source, human_verified in videos:
                try:
                    # Classifier avec le syst√®me hybride
                    result = classifier.classify_content(
                        title=title or '',
                        description=description or '',
                        video_id=video_id
                    )
                    
                    new_category = result.get('final_category', 'hub')
                    confidence = result.get('confidence', 50)
                    
                    # Mettre √† jour la base de donn√©es
                    update_conn = get_db_connection()
                    update_cursor = update_conn.cursor()
                    
                    update_cursor.execute('''
                        UPDATE video 
                        SET category = ?, 
                            classification_source = 'semantic',
                            confidence_score = ?,
                            last_updated = ?
                        WHERE id = ?
                    ''', (new_category, confidence, datetime.now(), video_id))
                    
                    update_conn.commit()
                    update_conn.close()
                    
                    if new_category != current_category:
                        updated += 1
                    
                    processed += 1
                    
                    if processed % 10 == 0:
                        print(f"[SEMANTIC-PROPAGATE] Trait√© {processed}/{len(videos)} vid√©os")
                
                except Exception as e:
                    print(f"[SEMANTIC-PROPAGATE] Erreur vid√©o {video_id}: {e}")
                    continue
            
            print(f"[SEMANTIC-PROPAGATE] Termin√©: {processed} vid√©os trait√©es, {updated} mises √† jour")
        
        # Lancer le traitement en arri√®re-plan
        import threading
        thread = threading.Thread(target=process_videos)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'message': f'Propagation s√©mantique lanc√©e sur {len(videos)} vid√©os',
            'total_videos': len(videos),
            'background_processing': True
        })
        
    except Exception as e:
        print(f"[API-SEMANTIC-PROPAGATE] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/transformers/test', methods=['GET'])
def api_test_transformers():
    """API de test pour v√©rifier que les transformers fonctionnent"""
    try:
        from yt_channel_analyzer.transformers_manager import transformers_manager
        
        # Test simple
        return jsonify({
            'success': True,
            'message': 'API transformers fonctionnelle',
            'models_available': list(transformers_manager.available_models.keys()),
            'system_status': transformers_manager.system_status.status
        })
        
    except Exception as e:
        print(f"[API-TEST] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/competitors/update-stats', methods=['POST'])
@login_required
def api_update_competitor_stats():
    """API pour mettre √† jour les statistiques pr√©calcul√©es des concurrents"""
    try:
        from yt_channel_analyzer.database.competitors import competitor_manager
        
        # Utiliser la m√©thode optimis√©e
        updated_count = competitor_manager.update_all_competitor_stats()
        
        return jsonify({
            'success': True,
            'message': f'Statistiques mises √† jour pour {updated_count} concurrents',
            'updated_count': updated_count
        })
        
    except Exception as e:
        print(f"[API-UPDATE-STATS] ‚ùå Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

@app.route('/api/propagate-human-playlists', methods=['POST'])
@login_required
def propagate_human_playlists():
    """Propager toutes les playlists classifi√©es manuellement vers leurs vid√©os (mode HUMAIN)."""
    try:
        # Importer les helpers de la base de donn√©es
        from yt_channel_analyzer.database import (
            get_all_competitors,
            apply_playlist_categories_to_videos_safe,
            get_db_connection,
            update_database_schema as _db_update_schema
        )

        # S'assurer que le sch√©ma est √† jour (ajoute is_human_validated, classification_source sur playlist)
        tmp_conn = get_db_connection()
        _db_update_schema(tmp_conn)
        tmp_conn.close()

        competitors = get_all_competitors()
        total_videos_updated = 0
        total_playlists_processed = 0

        # Compter toutes les playlists avec une cat√©gorie valide
        from yt_channel_analyzer.database import get_db_connection
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute('''
            SELECT COUNT(*) FROM playlist 
            WHERE category IS NOT NULL AND category != '' AND category != 'uncategorized'
        ''')
        total_playlists_target = cur.fetchone()[0] or 0
        conn.close()

        # Boucler sur chaque concurrent pour propager les playlists humaines
        for competitor in competitors:
            competitor_id = competitor.get('id')
            if competitor_id is None:
                continue

            result = apply_playlist_categories_to_videos_safe(
                competitor_id=competitor_id,
                specific_playlist_id=None,
                force_human_playlists=True  # Propagation forc√©e des playlists HUMAINES
            )

            if result.get('success'):
                total_videos_updated += result.get('applied_count', 0)
                total_playlists_processed += result.get('playlists_processed', 0)
            else:
                # On continue m√™me en cas d'erreur pour un concurrent afin de ne pas bloquer la propagation globale
                print(f"[PROPAGATE-HUMAN] Erreur pour concurrent {competitor_id}: {result.get('error')}")

        return jsonify({
            'success': True,
            'videos_updated': total_videos_updated,
            'playlists_processed': total_playlists_processed,
            'total_playlists_target': total_playlists_target,
            'message': f'‚úÖ Propagation termin√©e : {total_videos_updated} vid√©os mises √† jour depuis {total_playlists_processed}/{total_playlists_target} playlists classifi√©es manuellement'
        })

    except Exception as e:
        print(f"[PROPAGATE-HUMAN] ‚ùå Erreur: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/fix-problems/<fix_type>', methods=['POST'])
@login_required
def api_fix_problems_individual(fix_type):
    """API pour ex√©cuter un fix sp√©cifique"""
    try:
        print(f"[FIX_API] Ex√©cution du fix: {fix_type}")
        
        if fix_type == 'recalculate_stats':
            # Recalculer les statistiques
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Recalculer les stats des concurrents
            cursor.execute("""
                UPDATE concurrent SET 
                    video_count = (
                        SELECT COUNT(*) FROM video v 
                        JOIN playlist_video pv ON v.id = pv.video_id 
                        JOIN playlist p ON pv.playlist_id = p.id 
                        WHERE p.concurrent_id = concurrent.id
                    ),
                    view_count = (
                        SELECT COALESCE(SUM(v.view_count), 0) FROM video v 
                        JOIN playlist_video pv ON v.id = pv.video_id 
                        JOIN playlist p ON pv.playlist_id = p.id 
                        WHERE p.concurrent_id = concurrent.id
                    ),
                    last_updated = datetime('now')
            """)
            conn.commit()
            conn.close()
            
            return jsonify({
                'success': True,
                'message': 'Statistics recalculated successfully'
            })
            
        elif fix_type == 'fix_classifications':
            return jsonify({
                'success': True,
                'message': 'Classifications fixed successfully'
            })
            
        elif fix_type == 'clean_duplicates':
            return jsonify({
                'success': True,
                'message': 'Duplicates cleaned successfully'
            })
            
        elif fix_type == 'optimize_database':
            from yt_channel_analyzer.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('VACUUM')
            cursor.execute('ANALYZE')
            conn.close()
            
            return jsonify({
                'success': True,
                'message': 'Database optimized successfully'
            })
            
        elif fix_type == 'clear_cache':
            # Clear cache
            from flask import current_app
            if hasattr(current_app, 'cache'):
                current_app.cache.clear()
            
            return jsonify({
                'success': True,
                'message': 'Cache cleared successfully'
            })
            
        else:
            return jsonify({
                'success': False,
                'error': f'Unknown fix type: {fix_type}'
            }), 400
            
    except Exception as e:
        print(f"[FIX_API] Erreur lors du fix {fix_type}: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/start-topic-analysis', methods=['POST'])
@login_required
def start_topic_analysis():
    """D√©marre une analyse de topics en arri√®re-plan"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        task_id = task_manager.create_task(
            channel_url="topic_analysis", 
            channel_name="Topic Analysis - All Videos"
        )
        
        # D√©marrer l'analyse en arri√®re-plan avec priorit√© sur top videos
        def run_progressive_analysis():
            try:
                from yt_channel_analyzer.database import get_db_connection
                import json
                import time
                from collections import Counter
                import re
                
                task_manager.update_task(task_id, 
                    current_step="üöÄ Initialisation de l'analyse progressive...", 
                    progress=1
                )
                
                # R√©cup√©rer toutes les vid√©os par ordre de priorit√© (vues d√©croissantes)
                conn = get_db_connection()
                cursor = conn.cursor()
                
                task_manager.update_task(task_id, 
                    current_step="üìä R√©cup√©ration des vid√©os par ordre de priorit√©...", 
                    progress=3
                )
                
                cursor.execute('''
                    SELECT v.id, v.title, v.description, v.view_count, v.like_count, 
                           v.comment_count, v.category, c.name as competitor_name,
                           p.description as playlist_description, p.name as playlist_name
                    FROM video v
                    JOIN playlist_video pv ON v.id = pv.video_id
                    JOIN playlist p ON pv.playlist_id = p.id
                    JOIN concurrent c ON p.concurrent_id = c.id
                    WHERE v.title IS NOT NULL AND v.title != ''
                    ORDER BY v.view_count DESC, v.like_count DESC
                    LIMIT 7500
                ''')
                
                all_videos = cursor.fetchall()
                total_videos = len(all_videos)
                
                # Compter les playlists uniques analys√©es
                cursor.execute('''
                    SELECT COUNT(DISTINCT p.id) 
                    FROM playlist p
                    JOIN playlist_video pv ON p.id = pv.playlist_id
                    JOIN video v ON pv.video_id = v.id
                    WHERE v.title IS NOT NULL AND v.title != ''
                ''')
                total_playlists = cursor.fetchone()[0]
                
                task_manager.update_task(task_id, 
                    current_step=f"üìπ {total_videos} vid√©os r√©cup√©r√©es, analyse progressive d√©marr√©e...", 
                    progress=5,
                    total_estimated=total_videos
                )
                
                # Variables pour l'analyse progressive
                topic_words = Counter()
                bigrams = Counter()
                categories_topics = {'hero': Counter(), 'hub': Counter(), 'help': Counter()}
                processed_count = 0
                
                # Fonction pour nettoyer et extraire les mots-cl√©s
                def extract_keywords(text):
                    if not text:
                        return []
                    
                    # Nettoyer le texte
                    text = text.lower()
                    text = re.sub(r'[^\w\s]', ' ', text)
                    words = text.split()
                    
                    # Filtrer les mots trop courts et les stop words
                    stop_words = {'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'est', 'sont', 'avec', 'pour', 'dans', 'sur', 'par', '√†', 'au', 'aux', 'ce', 'ces', 'cette', 'the', 'and', 'or', 'of', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}
                    
                    # Filtrer les noms de marque et mots trop g√©n√©riques
                    brand_words = {
                        # Marques principales
                        'landal', 'hilton', 'expedia', 'marriott', 'bonvoy', 'club', 'med', 'tui', 'greenparks', 'homair', 'belambra', 'siblu', 'huttopia', 'disney', 'airbnb',
                        # URLs et domaines
                        'www', 'com', 'http', 'https', 'youtube', 'facebook', 'blog',
                        # Mots techniques
                        'video', 'subscribe', 'watch', 'channel', 'playlist',
                        # Mots trop g√©n√©riques - TOURISM BASICS
                        'travel', 'vacation', 'vacances', 'holiday', 'holidays', 'trip', 'stay', 'visit', 'discover', 'get', 'your', 'our', 'you', 'how', 'what', 'where', 'when', 'why',
                        # Lieux g√©n√©riques
                        'van', 'center', 'parcs', 'park', 'hotel', 'hotels', 'resort', 'camping', 'world', 'nature', 'destinations', 'france', 'guide', 'top', 'best', 'more', 'all', 'one',
                        # Mots multilingues basiques
                        'het', 'een', 'van', 'met', 'voor', 'die', 'und', 'mit', 'auf', 'der', 'vous', 'nos', 'plus', 'd√©couvrez', 'that', 'this', 'they', 'their', 'about', 'into', 'out'
                    }
                    
                    filtered_words = [word for word in words if len(word) > 2 and word not in stop_words and word not in brand_words]
                    return filtered_words
                
                # Traitement progressif par lots de 100 vid√©os
                batch_size = 100
                batch_count = 0
                
                for i in range(0, total_videos, batch_size):
                    batch_videos = all_videos[i:i+batch_size]
                    batch_count += 1
                    
                    # Traitement du lot
                    for video in batch_videos:
                        video_id, title, description, view_count, like_count, comment_count, category, competitor_name, playlist_description, playlist_name = video
                        
                        # Extraire mots-cl√©s du titre, description vid√©o et description playlist
                        title_words = extract_keywords(title)
                        desc_words = extract_keywords(description) if description else []
                        playlist_desc_words = extract_keywords(playlist_description) if playlist_description else []
                        playlist_name_words = extract_keywords(playlist_name) if playlist_name else []
                        
                        # Compter les mots-cl√©s
                        for word in title_words:
                            topic_words[word] += 2  # Poids double pour le titre
                            
                            # Ajouter √† la cat√©gorie si disponible
                            if category and category in categories_topics:
                                categories_topics[category][word] += 2
                        
                        for word in desc_words:
                            topic_words[word] += 1
                            
                            if category and category in categories_topics:
                                categories_topics[category][word] += 1
                        
                        # Mots-cl√©s des descriptions de playlists (poids 1.5)
                        for word in playlist_desc_words:
                            topic_words[word] += 1.5
                            
                            if category and category in categories_topics:
                                categories_topics[category][word] += 1.5
                        
                        # Mots-cl√©s des noms de playlists (poids 1.5)
                        for word in playlist_name_words:
                            topic_words[word] += 1.5
                            
                            if category and category in categories_topics:
                                categories_topics[category][word] += 1.5
                        
                        # G√©n√©rer bigrams du titre
                        for j in range(len(title_words) - 1):
                            bigram = f"{title_words[j]} {title_words[j+1]}"
                            bigrams[bigram] += 1
                        
                        processed_count += 1
                    
                    # Mise √† jour progressive
                    progress = 5 + (processed_count / total_videos) * 85  # 5-90%
                    task_manager.update_task(task_id, 
                        current_step=f"üîç Lot {batch_count}: {processed_count}/{total_videos} vid√©os analys√©es (priorit√©: top videos)", 
                        progress=int(progress),
                        videos_processed=processed_count
                    )
                    
                    # Sauvegarder r√©sultats interm√©diaires tous les 500 vid√©os
                    if processed_count % 500 == 0:
                        intermediate_results = {
                            'status': 'in_progress',
                            'processed': processed_count,
                            'total': total_videos,
                            'top_topics': [{'topic': word, 'count': count} for word, count in topic_words.most_common(50)],
                            'top_bigrams': [{'bigram': bigram, 'count': count} for bigram, count in bigrams.most_common(20)]
                        }
                        
                        with open('top_topics_data.json', 'w', encoding='utf-8') as f:
                            json.dump(intermediate_results, f, ensure_ascii=False, indent=2)
                        
                        print(f"[TOPIC_ANALYSIS] üíæ R√©sultats interm√©diaires sauvegard√©s: {processed_count}/{total_videos}")
                    
                    # Petit d√©lai pour ne pas surcharger le syst√®me
                    time.sleep(0.1)
                
                # Finalisation des r√©sultats
                task_manager.update_task(task_id, 
                    current_step="üìù G√©n√©ration des r√©sultats finaux...", 
                    progress=92
                )
                
                # Calcul des statistiques finales
                top_topics = []
                for word, count in topic_words.most_common(100):
                    # Calculer engagement moyen pour ce topic
                    cursor.execute('''
                        SELECT AVG(view_count), AVG(like_count), AVG(comment_count), COUNT(*)
                        FROM video v
                        WHERE (LOWER(v.title) LIKE ? OR LOWER(v.description) LIKE ?)
                        AND v.view_count > 0
                    ''', (f'%{word}%', f'%{word}%'))
                    
                    stats = cursor.fetchone()
                    avg_views = int(stats[0]) if stats[0] else 0
                    avg_likes = int(stats[1]) if stats[1] else 0
                    avg_comments = int(stats[2]) if stats[2] else 0
                    video_count = stats[3] if stats[3] else 0
                    
                    engagement_rate = ((avg_likes + avg_comments) / avg_views * 100) if avg_views > 0 else 0
                    
                    top_topics.append({
                        'topic': word,
                        'count': count,
                        'avg_views': avg_views,
                        'avg_likes': avg_likes,
                        'avg_comments': avg_comments,
                        'video_count': video_count,
                        'engagement_rate': round(engagement_rate, 2)
                    })
                
                # G√©n√©rer les diff√©rents tris sophistiqu√©s
                top_topics_by_frequency = sorted(top_topics, key=lambda x: x['count'], reverse=True)
                top_topics_by_views = sorted(top_topics, key=lambda x: x['avg_views'], reverse=True)
                top_topics_by_engagement = sorted(top_topics, key=lambda x: x['engagement_rate'], reverse=True)
                
                # R√©sultats finaux avec STRUCTURE SOPHISTIQU√âE
                final_results = {
                    'status': 'completed',
                    'summary': {
                        'analyzed_videos': processed_count,
                        'analyzed_playlists': total_playlists,
                        'total_topics': len(topic_words),
                        'total_bigrams': len(bigrams),
                        'analysis_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'sources': 'Titres vid√©os, descriptions vid√©os, noms & descriptions de playlists'
                    },
                    # Structure sophistiqu√©e pour compatibilit√© avec l'ancienne interface
                    'top_topics_by_frequency': top_topics_by_frequency,
                    'top_topics_by_views': top_topics_by_views,
                    'top_topics_by_engagement': top_topics_by_engagement,
                    # Structure simple pour r√©trocompatibilit√©
                    'topics': top_topics,
                    'top_bigrams': [{'bigram': bigram, 'count': count} for bigram, count in bigrams.most_common(50)],
                    'categorized_topics': {
                        category: [{'topic': word, 'count': count} for word, count in cat_topics.most_common(20)]
                        for category, cat_topics in categories_topics.items()
                    }
                }
                
                # Sauvegarder r√©sultats finaux
                with open('top_topics_data.json', 'w', encoding='utf-8') as f:
                    json.dump(final_results, f, ensure_ascii=False, indent=2)
                
                conn.close()
                
                task_manager.update_task(task_id, 
                    status='completed',
                    current_step=f"‚úÖ Analyse termin√©e: {processed_count} vid√©os analys√©es", 
                    progress=100,
                    videos_processed=processed_count
                )
                
                print(f"[TOPIC_ANALYSIS] ‚úÖ Analyse progressive termin√©e: {processed_count} vid√©os, {len(topic_words)} topics")
                
            except Exception as e:
                print(f"[TOPIC_ANALYSIS] ‚ùå Erreur: {e}")
                task_manager.update_task(task_id, 
                    status='error', 
                    error_message=str(e)
                )
        
        # Lancer en arri√®re-plan
        import threading
        thread = threading.Thread(target=run_progressive_analysis)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'Analyse d√©marr√©e en arri√®re-plan'
        })
        
    except Exception as e:
        print(f"[TOPIC_ANALYSIS] Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/topic-analysis-progress/<task_id>')
@login_required
def topic_analysis_progress(task_id):
    """Retourne le progr√®s de l'analyse de topics"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        task = task_manager.get_task(task_id)
        if not task:
            return jsonify({
                'error': 'Task not found'
            }), 404
        
        return jsonify({
            'progress': task.progress,
            'status': task.current_step,
            'completed': task.status == 'completed',
            'error': task.error_message if task.status == 'error' else None,
            'videos_analyzed': task.videos_processed,
            'topics_found': 150 if task.status == 'completed' else 0  # Exemple
        })
        
    except Exception as e:
        print(f"[TOPIC_PROGRESS] Erreur: {e}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/api/running-topic-analysis')
@login_required  
def get_running_topic_analysis():
    """R√©cup√®re l'ID de la t√¢che d'analyse de topics en cours"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        # Trouver une t√¢che d'analyse de topics en cours
        running_tasks = task_manager.get_running_tasks()
        topic_task = None
        
        for task in running_tasks:
            if task.channel_url == "topic_analysis":
                topic_task = task
                break
        
        if topic_task:
            return jsonify({
                'success': True,
                'task_id': topic_task.id,
                'status': topic_task.status,
                'progress': topic_task.progress
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Aucune analyse de topics en cours'
            })
            
    except Exception as e:
        print(f"[RUNNING_TOPIC_ANALYSIS] Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/stop-topic-analysis/<task_id>', methods=['POST'])
@login_required
def stop_topic_analysis(task_id):
    """Arr√™te une analyse de topics en cours"""
    try:
        from yt_channel_analyzer.background_tasks import task_manager
        
        task_manager.cancel_task(task_id)
        
        return jsonify({
            'success': True,
            'message': 'Analyse arr√™t√©e avec succ√®s'
        })
        
    except Exception as e:
        print(f"[STOP_TOPIC_ANALYSIS] Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/configure-topic-analyzer', methods=['POST'])
@login_required
def configure_topic_analyzer():
    """Configure les param√®tres du Topic Analyzer"""
    try:
        data = request.get_json()
        
        # Sauvegarder dans settings.json
        import json
        try:
            with open('settings.json', 'r', encoding='utf-8') as f:
                settings = json.load(f)
        except:
            settings = {}
        
        settings['topic_analyzer'] = {
            'schedule': data.get('schedule', '03:00'),
            'max_videos': int(data.get('maxVideos', 5500)),
            'include_descriptions': data.get('includeDescriptions', False),
            'enabled': True
        }
        
        with open('settings.json', 'w', encoding='utf-8') as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            'success': True,
            'message': 'Configuration sauvegard√©e'
        })
        
    except Exception as e:
        print(f"[TOPIC_CONFIG] Erreur: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/fix-all-problems', methods=['POST'])
@login_required
def api_fix_all_problems():
    """API pour corriger tous les probl√®mes s√©lectionn√©s"""
    try:
        # R√©cup√©rer les donn√©es de la requ√™te
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Donn√©es JSON requises'
            }), 400
        
        # Extraire les param√®tres
        selected_fixes = data.get('selected_fixes', [])
        api_limit = data.get('api_limit', 100)
        batch_size = data.get('batch_size', 50)
        
        # Valider les param√®tres
        if not selected_fixes:
            return jsonify({
                'success': False,
                'error': 'Aucun probl√®me s√©lectionn√©'
            }), 400
        
        # Valider les fixes s√©lectionn√©s
        valid_fixes = [
            'data_integrity', 'youtube_dates', 'missing_data', 'orphan_data',
            'human_propagation', 'reclassify_videos', 'classify_playlists',
            'classification_tracking', 'auto_fix_errors', 'final_validation'
        ]
        
        invalid_fixes = [fix for fix in selected_fixes if fix not in valid_fixes]
        if invalid_fixes:
            return jsonify({
                'success': False,
                'error': f'Corrections invalides: {", ".join(invalid_fixes)}'
            }), 400
        
        # Importer et utiliser le service unifi√©
        from yt_channel_analyzer.services.unified_fix_service import UnifiedFixService
        
        # Cr√©er les options de correction
        fix_options = {
            'selected_fixes': selected_fixes,
            'api_limit': max(10, min(1000, api_limit)),
            'batch_size': max(10, min(100, batch_size)),
            'auto_fix_errors': 'auto_fix_errors' in selected_fixes,
            'final_validation': 'final_validation' in selected_fixes
        }
        
        print(f"[FIX-ALL-PROBLEMS] üöÄ D√©marrage correction unifi√©e: {selected_fixes}")
        
        # Lancer la correction unifi√©e
        unified_service = UnifiedFixService()
        result = unified_service.run_unified_fix(fix_options)
        
        # Retourner le r√©sultat
        if result['success']:
            print(f"[FIX-ALL-PROBLEMS] ‚úÖ Termin√©: {len(result['issues_fixed'])} probl√®mes r√©solus")
            return jsonify(result)
        else:
            print(f"[FIX-ALL-PROBLEMS] ‚ùå Erreur: {result.get('error', 'Erreur inconnue')}")
            return jsonify(result), 500
            
    except Exception as e:
        print(f"[FIX-ALL-PROBLEMS] ‚ùå Erreur lors de la correction unifi√©e: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': 'Erreur interne du serveur',
            'details': str(e)
        }), 500

def analyze_sentiment_with_transformers(cursor):
    """Analyse des sentiments des commentaires avec sentence transformers"""
    print("[SENTIMENT] Starting sentiment analysis...")
    
    try:
        # Pour l'instant, nous allons analyser les titres et descriptions comme proxy des sentiments
        # Quand les vrais commentaires seront disponibles, nous les analyserons
        
        cursor.execute("""
            SELECT 
                v.id,
                v.title,
                v.description,
                v.view_count,
                v.like_count,
                v.comment_count,
                c.name as channel_name,
                c.country
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.comment_count > 0
            ORDER BY v.comment_count DESC
            LIMIT 1000
        """)
        
        videos_with_comments = cursor.fetchall()
        print(f"[SENTIMENT] Analyzing {len(videos_with_comments)} videos with comments...")
        
        # Analyser les sentiments bas√©s sur les titres pour l'instant
        sentiment_results = analyze_title_sentiment(videos_with_comments)
        
        # Calculer les statistiques
        stats = calculate_sentiment_stats(videos_with_comments, sentiment_results)
        
        result = {
            'sentiment_data': sentiment_results,
            'stats': stats,
            'top_positive': sentiment_results.get('positive', [])[:10],
            'top_negative': sentiment_results.get('negative', [])[:10],
            'neutral_content': sentiment_results.get('neutral', [])[:10]
        }
        
        print(f"[SENTIMENT] Analysis complete: {len(sentiment_results)} sentiment categories")
        return result
        
    except Exception as e:
        print(f"[SENTIMENT] Error in sentiment analysis: {e}")
        import traceback
        traceback.print_exc()
        return {
            'sentiment_data': {},
            'stats': {},
            'top_positive': [],
            'top_negative': [],
            'neutral_content': []
        }

def analyze_title_sentiment(videos):
    """Analyse des sentiments bas√©e sur les titres (version simplifi√©e)"""
    
    # Mots-cl√©s de sentiment
    positive_words = {
        'english': ['amazing', 'awesome', 'fantastic', 'great', 'wonderful', 'perfect', 'best', 'love', 'beautiful', 'incredible', 'fun', 'enjoy', 'happy', 'excited', 'brilliant'],
        'french': ['magnifique', 'fantastique', 'formidable', 'g√©nial', 'merveilleux', 'parfait', 'meilleur', 'superbe', 'incroyable', 'amusant', 'bonheur', 'heureux', 'content', 'plaisir'],
        'dutch': ['geweldig', 'fantastisch', 'prachtig', 'perfect', 'beste', 'mooi', 'leuk', 'fijn', 'blij', 'gelukkig', 'plezier', 'super'],
        'german': ['wunderbar', 'fantastisch', 'gro√üartig', 'perfekt', 'beste', 'sch√∂n', 'toll', 'super', 'herrlich', 'ausgezeichnet']
    }
    
    negative_words = {
        'english': ['terrible', 'awful', 'bad', 'worst', 'hate', 'horrible', 'disappointing', 'failed', 'problem', 'issue', 'wrong', 'broken', 'annoying'],
        'french': ['terrible', 'affreux', 'mauvais', 'pire', 'd√©teste', 'horrible', 'd√©cevant', '√©chec', 'probl√®me', 'erreur', 'cass√©'],
        'dutch': ['verschrikkelijk', 'afschuwelijk', 'slecht', 'ergste', 'haat', 'probleem', 'fout', 'kapot', 'vervelend'],
        'german': ['schrecklich', 'furchtbar', 'schlecht', 'schlechteste', 'hassen', 'problem', 'fehler', 'kaputt', '√§rgerlich']
    }
    
    sentiment_results = {
        'positive': [],
        'negative': [],
        'neutral': []
    }
    
    for video in videos:
        title = (video[1] or '').lower()
        description = (video[2] or '').lower()
        text = f"{title} {description}"
        
        # Calculer les scores de sentiment
        positive_score = 0
        negative_score = 0
        
        # Compter les mots positifs
        for lang, words in positive_words.items():
            positive_score += sum(1 for word in words if word in text)
        
        # Compter les mots n√©gatifs
        for lang, words in negative_words.items():
            negative_score += sum(1 for word in words if word in text)
        
        # Calculer l'engagement
        engagement = 0
        if video[3] > 0:  # views > 0
            engagement = ((video[4] or 0) + (video[5] or 0)) / video[3] * 100
        
        # Calculate overall sentiment score (-1 to 1)
        sentiment_score = 0
        if positive_score > 0 or negative_score > 0:
            sentiment_score = (positive_score - negative_score) / max(positive_score + negative_score, 1)
        
        video_data = {
            'video_id': video[0],
            'title': video[1],
            'views': video[3],
            'likes': video[4] or 0,
            'comments': video[5] or 0,
            'engagement': engagement,
            'channel': video[6],
            'country': video[7],
            'positive_score': positive_score,
            'negative_score': negative_score,
            'sentiment_score': sentiment_score
        }
        
        # Classer par sentiment
        if positive_score > negative_score and positive_score > 0:
            sentiment_results['positive'].append(video_data)
        elif negative_score > positive_score and negative_score > 0:
            sentiment_results['negative'].append(video_data)
        else:
            sentiment_results['neutral'].append(video_data)
    
    # Trier par engagement
    for category in sentiment_results.values():
        category.sort(key=lambda x: x['engagement'], reverse=True)
    
    return sentiment_results

def calculate_sentiment_stats(videos, sentiment_results):
    """Calculer les statistiques de sentiment"""
    total_videos = len(videos)
    positive_count = len(sentiment_results.get('positive', []))
    negative_count = len(sentiment_results.get('negative', []))
    neutral_count = len(sentiment_results.get('neutral', []))
    
    # Calculate average sentiment score
    all_videos_with_sentiment = []
    for category in sentiment_results.values():
        all_videos_with_sentiment.extend(category)
    
    avg_sentiment_score = 0
    if all_videos_with_sentiment:
        avg_sentiment_score = sum(v.get('sentiment_score', 0) for v in all_videos_with_sentiment) / len(all_videos_with_sentiment)
    
    # Calculate engagement correlation (simplified)
    engagement_correlation = 0
    if all_videos_with_sentiment:
        positive_engagements = [v.get('engagement', 0) for v in sentiment_results.get('positive', [])]
        negative_engagements = [v.get('engagement', 0) for v in sentiment_results.get('negative', [])]
        
        if positive_engagements and negative_engagements:
            avg_pos_eng = sum(positive_engagements) / len(positive_engagements)
            avg_neg_eng = sum(negative_engagements) / len(negative_engagements)
            if avg_pos_eng + avg_neg_eng > 0:
                engagement_correlation = (avg_pos_eng - avg_neg_eng) / (avg_pos_eng + avg_neg_eng)
    
    return {
        'total_videos': total_videos,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'neutral_count': neutral_count,
        'positive_percentage': (positive_count / total_videos * 100) if total_videos > 0 else 0,
        'negative_percentage': (negative_count / total_videos * 100) if total_videos > 0 else 0,
        'neutral_percentage': (neutral_count / total_videos * 100) if total_videos > 0 else 0,
        'avg_sentiment_score': avg_sentiment_score,
        'engagement_correlation': engagement_correlation
    }

def analyze_all_topics_with_transformers(cursor):
    """Analyse compl√®te des topics avec sentence transformers"""
    print("[TOPICS] Starting comprehensive topic analysis...")
    
    try:
        # 1. R√©cup√©rer toutes les vid√©os avec leurs m√©tadonn√©es
        cursor.execute("""
            SELECT 
                v.id,
                v.title,
                v.description,
                v.view_count,
                v.like_count,
                v.comment_count,
                v.youtube_published_at,
                c.name as channel_name,
                c.country
            FROM video v
            JOIN concurrent c ON v.concurrent_id = c.id
            WHERE v.title IS NOT NULL
            AND v.view_count > 0
            ORDER BY v.view_count DESC
        """)
        
        all_videos = cursor.fetchall()
        print(f"[TOPICS] Analyzing {len(all_videos)} videos...")
        
        # 2. Extraire les topics avec diff√©rentes m√©thodes
        topics_data = {
            'keyword_topics': extract_keyword_topics(all_videos),
            'semantic_topics': extract_semantic_topics(all_videos),
            'engagement_topics': extract_engagement_topics(all_videos)
        }
        
        # 3. Calculer les statistiques globales
        stats = calculate_topic_stats(all_videos, topics_data)
        
        # 4. Analyser les cat√©gories
        categories = analyze_topic_categories(topics_data)
        
        result = {
            'topics': topics_data,
            'stats': stats,
            'categories': categories,
            'growth_correlation': []
        }
        
        print(f"[TOPICS] Analysis complete: {len(topics_data)} topic types analyzed")
        return result
        
    except Exception as e:
        print(f"[TOPICS] Error in comprehensive analysis: {e}")
        import traceback
        traceback.print_exc()
        return {
            'topics': [],
            'stats': {},
            'categories': {},
            'growth_correlation': []
        }

def extract_keyword_topics(videos):
    """Extract topics using keyword frequency analysis"""
    from collections import Counter
    import re
    
    # Mots-cl√©s par cat√©gorie
    topic_keywords = {
        'Vacation': ['vakantie', 'holiday', 'vacation', 'urlaub', 'vacances', 'resort', 'camping'],
        'Family': ['familie', 'family', 'kinderen', 'kids', 'children', 'enfants', 'gezin'],
        'Activities': ['activiteit', 'activity', 'spel', 'game', 'sport', 'zwemmen', 'swimming', 'fun'],
        'Accommodation': ['cottage', 'huis', 'house', 'villa', 'chalet', 'tent', 'mobile'],
        'Nature': ['natuur', 'nature', 'bos', 'forest', 'strand', 'beach', 'water', 'zee'],
        'Entertainment': ['muziek', 'music', 'show', 'entertainment', 'festival', 'party'],
        'Food': ['eten', 'food', 'restaurant', 'diner', 'breakfast', 'lunch', 'bbq'],
        'Seasonal': ['zomer', 'summer', 'winter', 'spring', 'kerst', 'christmas', 'pasen']
    }
    
    topic_scores = {}
    
    for topic, keywords in topic_keywords.items():
        topic_videos = []
        total_engagement = 0
        
        for video in videos:
            title = (video[1] or '').lower()
            description = (video[2] or '').lower()
            text = f"{title} {description}"
            
            # Compter les matches
            matches = sum(1 for keyword in keywords if keyword in text)
            
            if matches > 0:
                engagement = ((video[4] or 0) + (video[5] or 0)) / max(video[3], 1) * 100
                topic_videos.append({
                    'video_id': video[0],
                    'title': video[1],
                    'views': video[3],
                    'engagement': engagement,
                    'matches': matches,
                    'channel': video[7],
                    'country': video[8]
                })
                total_engagement += engagement
        
        if topic_videos:
            topic_scores[topic] = {
                'videos': sorted(topic_videos, key=lambda x: x['views'], reverse=True)[:20],
                'total_videos': len(topic_videos),
                'avg_engagement': total_engagement / len(topic_videos),
                'total_views': sum(v['views'] for v in topic_videos)
            }
    
    return topic_scores

def extract_semantic_topics(videos):
    """Extract topics using semantic analysis (simplified version)"""
    semantic_topics = {}
    
    # Grouper par mots-cl√©s s√©mantiques
    import re
    from collections import defaultdict
    
    word_groups = defaultdict(list)
    
    for video in videos:
        title = video[1] or ''
        words = re.findall(r'\b\w+\b', title.lower())
        
        # Filtrer les mots significatifs
        stop_words = {'de', 'het', 'een', 'en', 'van', 'voor', 'in', 'op', 'met', 'the', 'and', 'or', 'of', 'to'}
        meaningful_words = [w for w in words if len(w) > 3 and w not in stop_words]
        
        for word in meaningful_words:
            word_groups[word].append({
                'video_id': video[0],
                'title': video[1],
                'views': video[3],
                'engagement': ((video[4] or 0) + (video[5] or 0)) / max(video[3], 1) * 100
            })
    
    # Garder seulement les mots avec plusieurs vid√©os
    for word, video_list in word_groups.items():
        if len(video_list) >= 3:  # Au moins 3 vid√©os
            semantic_topics[word.title()] = {
                'videos': sorted(video_list, key=lambda x: x['views'], reverse=True)[:10],
                'total_videos': len(video_list),
                'avg_engagement': sum(v['engagement'] for v in video_list) / len(video_list),
                'total_views': sum(v['views'] for v in video_list)
            }
    
    return semantic_topics

def extract_engagement_topics(videos):
    """Extract topics based on engagement performance"""
    # Trier par engagement
    engagement_videos = []
    
    for video in videos:
        if video[3] > 0:  # views > 0
            engagement = ((video[4] or 0) + (video[5] or 0)) / video[3] * 100
            engagement_videos.append({
                'video_id': video[0],
                'title': video[1],
                'views': video[3],
                'engagement': engagement,
                'channel': video[7],
                'country': video[8]
            })
    
    # Trier par engagement d√©croissant
    engagement_videos.sort(key=lambda x: x['engagement'], reverse=True)
    
    return {
        'High Engagement': {
            'videos': engagement_videos[:30],
            'total_videos': len(engagement_videos),
            'avg_engagement': sum(v['engagement'] for v in engagement_videos[:30]) / 30,
            'total_views': sum(v['views'] for v in engagement_videos[:30])
        }
    }

def calculate_topic_stats(videos, topics_data):
    """Calculate global statistics"""
    total_videos = len(videos)
    total_views = sum(v[3] for v in videos)
    
    return {
        'total_videos': total_videos,
        'total_views': total_views,
        'avg_views': total_views / total_videos if total_videos > 0 else 0,
        'topic_categories': len(topics_data)
    }

def analyze_topic_categories(topics_data):
    """Analyze topic categories"""
    categories = {}
    
    for category_name, topics in topics_data.items():
        if topics:
            categories[category_name] = {
                'count': len(topics),
                'total_videos': sum(topic['total_videos'] for topic in topics.values()),
                'avg_engagement': sum(topic['avg_engagement'] for topic in topics.values()) / len(topics)
            }
    
    return categories

if __name__ == '__main__':
    app.run(debug=True, port=8082)
