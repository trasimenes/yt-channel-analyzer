#!/usr/bin/env python3
"""
Script de consolidation des donnÃ©es YouTube pour analyse approfondie
Ã‰tape 1: RÃ©cupÃ©ration de TOUTES les donnÃ©es depuis SQLite
Ã‰tape 2: Consolidation avec mÃ©tadonnÃ©es (pays, concurrent)
Ã‰tape 3: PrÃ©paration pour l'analyse sÃ©mantique
"""

import json
import sqlite3
import os
from datetime import datetime
from collections import Counter
import pandas as pd

class YouTubeDataConsolidator:
    def __init__(self, db_path='instance/database.db'):
        """Initialise le consolidateur avec la base de donnÃ©es existante"""
        self.db_path = db_path
        self.consolidated_data = {
            'videos': [],
            'playlists': [],
            'competitors': [],
            'metadata': {}
        }
    
    def connect_db(self):
        """Connexion Ã  la base de donnÃ©es"""
        return sqlite3.connect(self.db_path)
    
    def step1_fetch_all_videos(self):
        """Ã‰tape 1: RÃ©cupÃ©ration de TOUS les titres et mÃ©tadonnÃ©es des vidÃ©os"""
        print("ğŸ“¹ Ã‰TAPE 1: RÃ©cupÃ©ration de tous les titres de vidÃ©os...")
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        # RequÃªte pour rÃ©cupÃ©rer TOUTES les vidÃ©os avec mÃ©tadonnÃ©es complÃ¨tes
        query = '''
        SELECT 
            v.id, v.video_id, v.title, v.published_at, v.view_count, 
            v.like_count, v.comment_count, v.duration_seconds, v.category,
            v.beauty_score, v.emotion_score, v.info_quality_score,
            c.id as competitor_id, c.name as competitor_name, c.country,
            c.subscriber_count, c.view_count as channel_views,
            p.id as playlist_id, p.name as playlist_name, p.category as playlist_category
        FROM video v
        JOIN concurrent c ON v.concurrent_id = c.id
        LEFT JOIN playlist_video pv ON v.id = pv.video_id
        LEFT JOIN playlist p ON pv.playlist_id = p.id
        ORDER BY v.view_count DESC
        '''
        
        cursor.execute(query)
        raw_data = cursor.fetchall()
        
        print(f"âœ… {len(raw_data)} enregistrements rÃ©cupÃ©rÃ©s depuis la base")
        
        # Organiser les donnÃ©es par vidÃ©o unique
        videos_dict = {}
        for row in raw_data:
            (vid_db_id, video_id, title, published_at, view_count, like_count, comment_count,
             duration_seconds, category, beauty_score, emotion_score, info_quality_score,
             competitor_id, competitor_name, country, subscriber_count, channel_views,
             playlist_id, playlist_name, playlist_category) = row
            
            if video_id not in videos_dict:
                videos_dict[video_id] = {
                    'db_id': vid_db_id,
                    'video_id': video_id,
                    'title': title or '',
                    'published_at': published_at,
                    'view_count': view_count or 0,
                    'like_count': like_count or 0,
                    'comment_count': comment_count or 0,
                    'duration_seconds': duration_seconds or 0,
                    'category': category,
                    'beauty_score': beauty_score,
                    'emotion_score': emotion_score,
                    'info_quality_score': info_quality_score,
                    'competitor_id': competitor_id,
                    'competitor_name': competitor_name,
                    'country': country,
                    'subscriber_count': subscriber_count,
                    'channel_views': channel_views,
                    'playlists': []
                }
            
            # Ajouter playlist si elle existe
            if playlist_id and playlist_name:
                videos_dict[video_id]['playlists'].append({
                    'playlist_id': playlist_id,
                    'playlist_name': playlist_name,
                    'playlist_category': playlist_category
                })
        
        self.consolidated_data['videos'] = list(videos_dict.values())
        conn.close()
        
        print(f"ğŸ¯ {len(self.consolidated_data['videos'])} vidÃ©os uniques consolidÃ©es")
        
        # Statistiques par pays
        country_stats = Counter(v['country'] for v in self.consolidated_data['videos'])
        print("ğŸ“Š RÃ©partition par pays:")
        for country, count in country_stats.most_common():
            print(f"  - {country}: {count:,} vidÃ©os")
        
        return len(self.consolidated_data['videos'])
    
    def step2_fetch_all_descriptions(self):
        """Ã‰tape 2: RÃ©cupÃ©ration de TOUTES les descriptions des vidÃ©os"""
        print("\nğŸ“ Ã‰TAPE 2: RÃ©cupÃ©ration de toutes les descriptions de vidÃ©os...")
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        # RÃ©cupÃ©rer les descriptions
        cursor.execute('SELECT video_id, description FROM video WHERE description IS NOT NULL')
        descriptions = cursor.fetchall()
        
        # Associer descriptions aux vidÃ©os
        description_dict = {video_id: desc for video_id, desc in descriptions}
        
        descriptions_added = 0
        for video in self.consolidated_data['videos']:
            video_id = video['video_id']
            if video_id in description_dict:
                video['description'] = description_dict[video_id]
                descriptions_added += 1
            else:
                video['description'] = ''
        
        conn.close()
        
        print(f"âœ… {descriptions_added:,} descriptions ajoutÃ©es aux vidÃ©os")
        print(f"ğŸ“Š {len(descriptions) - descriptions_added} descriptions non matchÃ©es")
        
        return descriptions_added
    
    def step3_fetch_all_playlists(self):
        """Ã‰tape 3: RÃ©cupÃ©ration de toutes les descriptions de playlists"""
        print("\nğŸµ Ã‰TAPE 3: RÃ©cupÃ©ration de toutes les playlists...")
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        query = '''
        SELECT 
            p.id, p.playlist_id, p.name, p.description, p.category,
            p.video_count, p.created_at, p.last_updated,
            p.classification_source, p.human_verified, p.is_human_validated,
            c.id as competitor_id, c.name as competitor_name, c.country
        FROM playlist p
        JOIN concurrent c ON p.concurrent_id = c.id
        ORDER BY p.video_count DESC
        '''
        
        cursor.execute(query)
        playlist_data = cursor.fetchall()
        
        for row in playlist_data:
            (pl_id, playlist_id, name, description, category, video_count,
             created_at, last_updated, classification_source, human_verified,
             is_human_validated, competitor_id, competitor_name, country) = row
            
            playlist_info = {
                'db_id': pl_id,
                'playlist_id': playlist_id,
                'name': name or '',
                'description': description or '',
                'category': category,
                'video_count': video_count or 0,
                'created_at': created_at,
                'last_updated': last_updated,
                'classification_source': classification_source,
                'human_verified': bool(human_verified),
                'is_human_validated': bool(is_human_validated),
                'competitor_id': competitor_id,
                'competitor_name': competitor_name,
                'country': country
            }
            
            self.consolidated_data['playlists'].append(playlist_info)
        
        conn.close()
        
        print(f"âœ… {len(self.consolidated_data['playlists'])} playlists rÃ©cupÃ©rÃ©es")
        
        # Statistiques playlists
        category_stats = Counter(p['category'] for p in self.consolidated_data['playlists'] if p['category'])
        print("ğŸ“Š RÃ©partition par catÃ©gorie:")
        for category, count in category_stats.most_common():
            print(f"  - {category.upper()}: {count} playlists")
        
        return len(self.consolidated_data['playlists'])
    
    def step4_fetch_competitors(self):
        """Ã‰tape 4: RÃ©cupÃ©ration des informations des concurrents"""
        print("\nğŸ¢ Ã‰TAPE 4: RÃ©cupÃ©ration des informations concurrents...")
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        query = '''
        SELECT 
            id, name, channel_id, country, subscriber_count, view_count,
            description, thumbnail_url, created_at, last_updated
        FROM concurrent
        ORDER BY subscriber_count DESC
        '''
        
        cursor.execute(query)
        competitor_data = cursor.fetchall()
        
        for row in competitor_data:
            (comp_id, name, channel_id, country, subscriber_count, view_count,
             description, thumbnail_url, created_at, last_updated) = row
            
            competitor_info = {
                'competitor_id': comp_id,
                'name': name,
                'channel_id': channel_id,
                'country': country,
                'subscriber_count': subscriber_count or 0,
                'view_count': view_count or 0,
                'description': description or '',
                'thumbnail_url': thumbnail_url,
                'created_at': created_at,
                'last_updated': last_updated
            }
            
            self.consolidated_data['competitors'].append(competitor_info)
        
        conn.close()
        
        print(f"âœ… {len(self.consolidated_data['competitors'])} concurrents rÃ©cupÃ©rÃ©s")
        
        # Statistiques concurrents
        country_stats = Counter(c['country'] for c in self.consolidated_data['competitors'])
        print("ğŸ“Š Concurrents par pays:")
        for country, count in country_stats.most_common():
            print(f"  - {country}: {count} concurrents")
        
        return len(self.consolidated_data['competitors'])
    
    def generate_metadata(self):
        """GÃ©nÃ¨re des mÃ©tadonnÃ©es globales sur le dataset consolidÃ©"""
        print("\nğŸ“‹ GÃ©nÃ©ration des mÃ©tadonnÃ©es globales...")
        
        videos = self.consolidated_data['videos']
        playlists = self.consolidated_data['playlists']
        competitors = self.consolidated_data['competitors']
        
        # Statistiques globales
        total_views = sum(v['view_count'] for v in videos)
        total_likes = sum(v['like_count'] for v in videos)
        total_comments = sum(v['comment_count'] for v in videos)
        
        # Statistiques par pays
        country_breakdown = {}
        for country in set(v['country'] for v in videos):
            country_videos = [v for v in videos if v['country'] == country]
            country_breakdown[country] = {
                'videos': len(country_videos),
                'total_views': sum(v['view_count'] for v in country_videos),
                'avg_views': sum(v['view_count'] for v in country_videos) / len(country_videos),
                'competitors': len([c for c in competitors if c['country'] == country])
            }
        
        # Statistiques temporelles
        years = Counter()
        for video in videos:
            if video['published_at']:
                year = video['published_at'][:4]
                years[year] += 1
        
        self.consolidated_data['metadata'] = {
            'consolidation_date': datetime.now().isoformat(),
            'total_videos': len(videos),
            'total_playlists': len(playlists),
            'total_competitors': len(competitors),
            'total_views': total_views,
            'total_likes': total_likes,
            'total_comments': total_comments,
            'avg_views_per_video': total_views / len(videos) if videos else 0,
            'engagement_rate': (total_likes + total_comments) / total_views if total_views > 0 else 0,
            'country_breakdown': country_breakdown,
            'temporal_distribution': dict(years.most_common()),
            'data_quality': {
                'videos_with_descriptions': len([v for v in videos if v.get('description', '').strip()]),
                'videos_with_categories': len([v for v in videos if v['category']]),
                'playlists_human_validated': len([p for p in playlists if p['is_human_validated']])
            }
        }
        
        print("âœ… MÃ©tadonnÃ©es gÃ©nÃ©rÃ©es")
        return self.consolidated_data['metadata']
    
    def save_consolidated_data(self, output_file='youtube_consolidated_data.json'):
        """Sauvegarde toutes les donnÃ©es consolidÃ©es"""
        print(f"\nğŸ’¾ Sauvegarde des donnÃ©es consolidÃ©es dans {output_file}...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.consolidated_data, f, ensure_ascii=False, indent=2)
        
        file_size_mb = os.path.getsize(output_file) / 1024 / 1024
        print(f"âœ… DonnÃ©es sauvegardÃ©es ({file_size_mb:.1f} MB)")
        
        # CrÃ©er aussi des fichiers sÃ©parÃ©s pour faciliter l'analyse
        with open('videos_only.json', 'w', encoding='utf-8') as f:
            json.dump(self.consolidated_data['videos'], f, ensure_ascii=False, indent=2)
        
        with open('playlists_only.json', 'w', encoding='utf-8') as f:
            json.dump(self.consolidated_data['playlists'], f, ensure_ascii=False, indent=2)
        
        # CSV pour analyse rapide
        df_videos = pd.DataFrame(self.consolidated_data['videos'])
        df_videos.to_csv('videos_consolidated.csv', index=False, encoding='utf-8')
        
        df_playlists = pd.DataFrame(self.consolidated_data['playlists'])
        df_playlists.to_csv('playlists_consolidated.csv', index=False, encoding='utf-8')
        
        print("âœ… Fichiers sÃ©parÃ©s crÃ©Ã©s: videos_only.json, playlists_only.json, *.csv")
        
        return output_file
    
    def print_summary(self):
        """Affiche un rÃ©sumÃ© dÃ©taillÃ© de la consolidation"""
        metadata = self.consolidated_data['metadata']
        
        print("\n" + "="*60)
        print("ğŸ¯ RÃ‰SUMÃ‰ DE LA CONSOLIDATION COMPLÃˆTE")
        print("="*60)
        
        print(f"ğŸ“¹ Total vidÃ©os: {metadata['total_videos']:,}")
        print(f"ğŸµ Total playlists: {metadata['total_playlists']:,}")
        print(f"ğŸ¢ Total concurrents: {metadata['total_competitors']:,}")
        print(f"ğŸ‘ï¸  Vues totales: {metadata['total_views']:,}")
        print(f"ğŸ‘ Likes totaux: {metadata['total_likes']:,}")
        print(f"ğŸ’¬ Commentaires totaux: {metadata['total_comments']:,}")
        print(f"ğŸ“Š Engagement rate: {metadata['engagement_rate']*100:.2f}%")
        
        print("\nğŸŒ RÃ‰PARTITION PAR PAYS:")
        for country, stats in metadata['country_breakdown'].items():
            print(f"  {country}: {stats['videos']:,} vidÃ©os, {stats['competitors']} concurrents, "
                  f"{stats['avg_views']:,.0f} vues/vidÃ©o en moyenne")
        
        print("\nğŸ“… RÃ‰PARTITION TEMPORELLE:")
        for year, count in sorted(metadata['temporal_distribution'].items()):
            print(f"  {year}: {count:,} vidÃ©os")
        
        print(f"\nâœ… QUALITÃ‰ DES DONNÃ‰ES:")
        dq = metadata['data_quality']
        print(f"  â€¢ VidÃ©os avec description: {dq['videos_with_descriptions']:,} "
              f"({dq['videos_with_descriptions']/metadata['total_videos']*100:.1f}%)")
        print(f"  â€¢ VidÃ©os catÃ©gorisÃ©es: {dq['videos_with_categories']:,} "
              f"({dq['videos_with_categories']/metadata['total_videos']*100:.1f}%)")
        print(f"  â€¢ Playlists validÃ©es humainement: {dq['playlists_human_validated']:,} "
              f"({dq['playlists_human_validated']/metadata['total_playlists']*100:.1f}%)")
        
        print("\nğŸš€ PRÃŠT POUR L'ANALYSE SÃ‰MANTIQUE!")
        print("="*60)

def main():
    """Script principal de consolidation"""
    print("ğŸš€ DÃ‰MARRAGE DE LA CONSOLIDATION YOUTUBE DATA")
    print("="*50)
    
    # Initialiser le consolidateur
    consolidator = YouTubeDataConsolidator()
    
    try:
        # Ã‰tape 1: RÃ©cupÃ©rer toutes les vidÃ©os
        consolidator.step1_fetch_all_videos()
        
        # Ã‰tape 2: RÃ©cupÃ©rer toutes les descriptions
        consolidator.step2_fetch_all_descriptions()
        
        # Ã‰tape 3: RÃ©cupÃ©rer toutes les playlists
        consolidator.step3_fetch_all_playlists()
        
        # Ã‰tape 4: RÃ©cupÃ©rer les concurrents
        consolidator.step4_fetch_competitors()
        
        # GÃ©nÃ©rer les mÃ©tadonnÃ©es
        consolidator.generate_metadata()
        
        # Sauvegarder tout
        output_file = consolidator.save_consolidated_data()
        
        # Afficher le rÃ©sumÃ©
        consolidator.print_summary()
        
        print(f"\nâœ… CONSOLIDATION TERMINÃ‰E!")
        print(f"ğŸ“ Fichier principal: {output_file}")
        print("ğŸ¯ Utilisez maintenant le script d'analyse sÃ©mantique sur ces donnÃ©es!")
        
    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        raise

if __name__ == "__main__":
    main()